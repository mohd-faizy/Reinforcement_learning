{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e052b5",
   "metadata": {},
   "source": [
    "# **‚ú®Monte Carlo Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457416b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [**Introduction to Reinforcement Learning Paradigms**](#1-introduction-to-reinforcement-learning-paradigms)\n",
    "   - [1.1 Model-Based Learning Recap](#11-model-based-learning-recap)\n",
    "   - [1.2 Model-Free Learning Fundamentals](#12-model-free-learning-fundamentals)\n",
    "   - [1.3 Model-Free $\\rightarrow$ On-Policy & Off-Policy](#13-model-free--on-policy--off-policy)\n",
    "\n",
    "1. [**Monte Carlo Methods in Reinforcement Learning**](#2-monte-carlo-methods-in-reinforcement-learning)\n",
    "   - [2.1 Core Concepts and Definitions](#21-core-concepts-and-definitions)\n",
    "   - [2.2 Episode Collection and Q-Value Estimation](#22-episode-collection-and-q-value-estimation)\n",
    "   - [2.3 Custom Grid World Environment Example](#23-custom-grid-world-environment-example)\n",
    "   - [2.4 First-Visit vs. Every-Visit Monte Carlo Methods](#24-first-visit-vs-every-visit-monte-carlo-methods)\n",
    "   - [2.5 Complete Code Implementation](#25-complete-code-implementation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d962139",
   "metadata": {},
   "source": [
    "# 1. ‚≠ê**Introduction to Reinforcement Learning Paradigms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0505ec",
   "metadata": {},
   "source": [
    "## 1.1 ‚úîÔ∏è**Model-Based** Reinforcement Learning(Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddba50",
   "metadata": {},
   "source": [
    "### üéØ **Core Concept**\n",
    "\n",
    "**Model-based learning** assumes you have **complete knowledge** of how the environment works ~ like having the rulebook for a game before you play.\n",
    "- **Environment dynamics are known**: You understand $P(s'|s,a)$ (transition probabilities) and $R(s,a)$ (reward function)\n",
    "- **No trial-and-error needed**: Can calculate optimal actions mathematically\n",
    "- **Planning-based approach**: Think first, act later\n",
    "\n",
    "\n",
    "### üßÆ **Mathematical Foundations**\n",
    "\n",
    "When you know the environment model, you can predict:\n",
    "- **Next state**: Given current state $s$ and action $a$, what's the probability of reaching state $s'$?\n",
    "- **Expected reward**: What reward do you get for taking action $a$ in state $s$?\n",
    "\n",
    "This knowledge enables **dynamic programming** techniques for finding optimal policies.\n",
    "\n",
    "\n",
    "### üîß **Core Algorithms**\n",
    "\n",
    "#### **üîñ Policy Iteration** \n",
    "*\"Improve the policy step by step\"*\n",
    "\n",
    "**Process**: Initialize policy ‚Üí Evaluate policy ‚Üí Improve policy ‚Üí Repeat until optimal\n",
    "\n",
    "##### **1. Policy Evaluation** (How good is my current policy?)\n",
    "\n",
    "**Full Formula**:\n",
    "$$V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)[R(s,a) + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "**Simplified Approach**:\n",
    "- **Iterative updates**: $V(s) \\leftarrow \\sum_a \\pi(a|s) [r + \\gamma V(s')]$\n",
    "- **Stop when values converge**\n",
    "\n",
    "##### **2. Policy Improvement** (Make the policy better)\n",
    "\n",
    "**Full Formula**:\n",
    "$$\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a)[R(s,a) + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "**Simplified Approach**:\n",
    "- **Act greedily**: Choose action with highest expected value\n",
    "- $\\pi'(s) = \\arg\\max_a Q^{\\pi}(s,a)$ where $Q^{\\pi}(s,a) = r + \\gamma V^{\\pi}(s')$\n",
    "\n",
    "\n",
    "\n",
    "#### **üîñ Value Iteration**\n",
    "*\"Find the best value for each state directly\"*\n",
    "\n",
    "**Process**: Initialize values ‚Üí Update values by selecting best actions ‚Üí Repeat until optimal\n",
    "\n",
    "##### **Full Formula**:\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a) + \\gamma V_k(s')]$$\n",
    "\n",
    "##### **Simplified Understanding**:\n",
    "- **One-step lookahead**: For each state, try all actions and pick the best\n",
    "- **Direct optimization**: No separate policy - values directly give you optimal actions\n",
    "- **Policy extraction**: $\\pi(s) = \\arg\\max_a Q(s,a)$\n",
    "\n",
    "\n",
    "\n",
    "### üìä **Key Variables Explained**\n",
    "\n",
    "| Symbol | Meaning | Simple Explanation |\n",
    "|--------|---------|-------------------|\n",
    "| $V^{\\pi}(s)$ | Value of state $s$ under policy $\\pi$ | \"How good is this state if I follow my current strategy?\" |\n",
    "| $\\pi(a\\|s)$ | Probability of taking action $a$ in state $s$ | \"How likely am I to choose this action here?\" |\n",
    "| $\\gamma$ | Discount factor (0 ‚â§ Œ≥ ‚â§ 1) | \"How much do I care about future rewards vs immediate ones?\" |\n",
    "| $P(s'\\|s,a)$ | Transition probability | \"If I do this action here, where will I end up?\" |\n",
    "| $R(s,a)$ | Reward function | \"What reward do I get for this action in this state?\" |\n",
    "\n",
    "\n",
    "\n",
    "### ‚ö° **Why Model-Based Learning Matters**\n",
    "\n",
    "#### **Advantages**:\n",
    "- **‚ö° Sample Efficiency**: No need for trial-and-error - can solve mathematically\n",
    "- **üéØ Computational Efficiency**: Planning is faster than learning through experience  \n",
    "- **üìà Theoretical Guarantees**: Provable convergence to optimal policies\n",
    "- **üîÑ Quick Adaptation**: Can immediately adjust to goal changes\n",
    "\n",
    "#### **Limitations**:\n",
    "- **ü§î Model Complexity**: Real environments are often too complex to model accurately\n",
    "- **‚ùì Unknown Dynamics**: Many real-world scenarios don't provide transition probabilities\n",
    "- **‚ö†Ô∏è Model Errors**: Wrong model leads to suboptimal policies\n",
    "- **üîÑ Non-Stationary**: Environments that change over time break the model\n",
    "\n",
    "\n",
    "\n",
    "### üéÆ **When to Use Model-Based vs Model-Free**\n",
    "\n",
    "| **Use Model-Based When** | **Use Model-Free When** |\n",
    "|------------------------------|---------------------------|\n",
    "| Environment rules are known | Environment is complex/unknown |\n",
    "| Sample efficiency is critical | Can afford many interactions |\n",
    "| Planning is computationally feasible | Environment changes frequently |\n",
    "| **Examples**: Chess, Grid worlds | **Examples**: Video games, Robotics |\n",
    "\n",
    "\n",
    "\n",
    "### üöÄ **Practical Takeaways**\n",
    "\n",
    "1. **Start Simple**: If you can model the environment, model-based is often faster.\n",
    "2. **Know Your Limits**: Complex real-world problems usually need model-free approaches.\n",
    "3. **Hybrid Approaches**: Many modern systems combine both methods.\n",
    "4. **Simplified Formulas**: Use iterative updates instead of complex summations for easier implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71771bd1",
   "metadata": {},
   "source": [
    "## 1.2 ‚úîÔ∏è**Model-Free** Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e33cf",
   "metadata": {},
   "source": [
    "### üéØ **Core Concept**\n",
    "\n",
    "**Model-free learning** is like learning to ride a bike by actually riding it - no instruction manual needed, just **trial and error**\n",
    "\n",
    "- **Experience-based**: Learn from sequences of $(s, a, r, s')$ tuples (`state`, `action`, `reward`, `next_state`)\n",
    "- **No environment model required**: Don't need to know $P(s'|s,a)$ or $R(s,a)$ in advance\n",
    "- **Direct interaction**: Agent learns by doing, not by thinking\n",
    "\n",
    "\n",
    "\n",
    "### üßÆ **Mathematical Foundations (Simplified)**\n",
    "\n",
    "Instead of complex environment models, model-free methods use **direct experience**:\n",
    "\n",
    "- **Experience tuple**: $(s_t, a_t, r_{t+1}, s_{t+1})$ - \"I was here, did this, got this reward, ended up there\"\n",
    "- **Value estimation**: Learn $V(s)$ or $Q(s,a)$ directly from observed rewards\n",
    "- **Policy learning**: Improve actions based on **actual outcomes**, not predictions\n",
    "\n",
    "\n",
    "\n",
    "### üîß **Core Algorithm Categories**\n",
    "\n",
    "#### **üîñ Value-Based Methods**\n",
    "*\"Learn how good each action is, then pick the best one\"*\n",
    "\n",
    "##### **‚≠êQ-Learning** (Most Popular):\n",
    "**Simplified Formula**:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "**In Plain English**:\n",
    "- **Update rule**: \"My estimate + learning_rate √ó (what_actually_happened - my_estimate)\"\n",
    "- **No model needed**: Just observe $(s, a, r, s')$ and update\n",
    "- **Policy**: Always choose $\\pi(s) = \\arg\\max_a Q(s,a)$\n",
    "\n",
    "##### **‚≠êDeep Q-Networks (DQN)**:\n",
    "- **Same Q-learning principle** but uses neural networks for complex state spaces\n",
    "- **Handles high-dimensional inputs** like images (Atari games)\n",
    "- **Experience replay**: Learn from past experiences multiple times\n",
    "\n",
    "\n",
    "\n",
    "#### **üîñ Policy-Based Methods**\n",
    "*\"Learn the strategy directly, skip the value estimation\"*\n",
    "\n",
    "##### **REINFORCE Algorithm**:\n",
    "**Simplified Formula**:\n",
    "$$\\nabla J(\\theta) = \\mathbb{E}[\\nabla \\log \\pi_\\theta(a|s) \\cdot R]$$\n",
    "\n",
    "**In Plain English**:\n",
    "- **Direct policy optimization**: Adjust policy parameters to maximize rewards\n",
    "- **Monte Carlo approach**: Use complete episode returns\n",
    "- **No value function needed**: Just improve policy based on episode outcomes\n",
    "\n",
    "\n",
    "\n",
    "#### **üîñ Actor-Critic Methods**\n",
    "*\"Best of both worlds: Learn values AND policy\"*\n",
    "\n",
    "**Two Components**:\n",
    "- **Actor**: Learns the `policy` $\\pi(a|s)$ (what to do)\n",
    "- **Critic**: Learns the `value function` $V(s)$ (how good is this state)\n",
    "\n",
    "**Popular Algorithms**: \n",
    "- **A2C** ‚Äì (`Advantage Actor-Critic`) A synchronous version of the actor-critic method that uses the advantage function to reduce variance in policy gradient updates. All agents collect experience in parallel and update the model together.\n",
    "\n",
    "- **A3C** ‚Äì (`Asynchronous Advantage Actor-Critic`) An asynchronous variant of A2C where multiple agents run in parallel but update the global model independently. This improves training stability and efficiency by decorrelating experiences.\n",
    "\n",
    "- **PPO** ‚Äì (`Proximal Policy Optimization`) A policy gradient method that uses a clipped objective to prevent large updates, making training more stable and sample-efficient. It‚Äôs widely used in modern RL applications due to its simplicity and robustness.\n",
    "\n",
    "\n",
    "\n",
    "### üìä **Key Characteristics & Advantages**\n",
    "\n",
    "#### **‚úÖ Why Model-Free Works**:\n",
    "\n",
    "| **Advantage** | **Explanation** |\n",
    "|---------------|----------------|\n",
    "| **üåç Real-World Ready** | No need to model complex environments (robotics, games) |\n",
    "| **üîÑ Adapts Naturally** | Handles changing environments without remodeling |\n",
    "| **üéØ Robust to Uncertainty** | Works even when environment dynamics are unknown |\n",
    "| **üìà Scalable** | Handles high-dimensional state spaces better |\n",
    "\n",
    "#### **‚ö†Ô∏è Challenges**:\n",
    "\n",
    "| **Challenge** | **Impact** |\n",
    "|---------------|------------|\n",
    "| **‚è∞ Sample Inefficiency** | Needs many interactions to learn well |\n",
    "| **üé≤ High Variance** | Learning can be unstable and noisy |\n",
    "| **‚öñÔ∏è Exploration-Exploitation** | Hard to balance trying new things vs using known good actions|\n",
    "| **üîß Hyperparameter Sensitivity** | Performance depends heavily on tuning|\n",
    "\n",
    "\n",
    "### üéÆ **Real-World Applications**\n",
    "\n",
    "#### **Perfect for Model-Free**:\n",
    "- **ü§ñ Autonomous Navigation**: Self-driving cars in traffic\n",
    "- **üéØ Game Playing**: Chess, Go, video games (AlphaGo, OpenAI Five)\n",
    "- **üí∞ Financial Trading**: Stock market strategies\n",
    "- **‚òÅÔ∏è Cloud Computing**: Resource allocation and load balancing\n",
    "- **üè≠ Robotics**: Manipulation tasks in unstructured environments\n",
    "\n",
    "***\n",
    "\n",
    "### üÜö **Model-Free vs Model-Based Comparison**\n",
    "\n",
    "| **Aspect** | **Model-Free** | **Model-Based** |\n",
    "|------------|----------------|-----------------|\n",
    "| **Learning Method** | Trial and error from experience | Planning with learned/given model |\n",
    "| **Environment Knowledge** | None required | Requires environment model (can be learned/approximate) |\n",
    "| **Sample Efficiency** | Low (needs many real samples) | High (leverages model simulations) |\n",
    "| **Real-World Suitability** | Good when samples are cheap | Better when real samples are expensive/risky |\n",
    "| **Computational Cost** | Low inference, high training | Low inference, high planning per step |\n",
    "| **Robustness** | Robust to model errors, sensitive to distribution shift | Sensitive to model errors, can handle uncertainty well |\n",
    "| **Convergence** | Guaranteed under certain conditions | Depends on model accuracy |\n",
    "| **Exploration** | Direct exploration in environment | Can explore safely in model |\n",
    "| **Interpretability** | Policy decisions less transparent | Planning process more interpretable |\n",
    "\n",
    "\n",
    "### üß† **Simplified Algorithm Comparison**\n",
    "\n",
    "#### **Q-Learning Example**:\n",
    "\n",
    "1. Start with random Q-values\n",
    "2. Take action, observe reward\n",
    "3. Update: $Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$\n",
    "4. Repeat until convergence\n",
    "\n",
    "\n",
    "#### **Policy Gradient Example**:\n",
    "\n",
    "1. Start with random policy\n",
    "2. Run episode, collect rewards\n",
    "3. Update: Increase probability of good actions\n",
    "4. Repeat until optimal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### üöÄ **Practical Takeaways**\n",
    "\n",
    "#### **Choose Model-Free When**:\n",
    "- ‚úÖ Environment is **complex or unknown**\n",
    "- ‚úÖ Environment **changes over time**\n",
    "- ‚úÖ You can afford **many interactions**\n",
    "- ‚úÖ **Safety** is more important than efficiency\n",
    "- ‚úÖ Working with **high-dimensional** problems\n",
    "\n",
    "#### **Implementation Tips**:\n",
    "1. **Start with Q-Learning** for discrete problems\n",
    "2. **Use DQN** for complex state spaces\n",
    "3. **Try Actor-Critic** for continuous actions\n",
    "4. **Focus on exploration strategies** early in learning\n",
    "5. **Monitor sample efficiency** - if too slow, consider model-based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef6f51",
   "metadata": {},
   "source": [
    "## 1.3 ‚ú®**Model-Free** $\\rightarrow$ `On-Policy` & `Off-Policy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c3963",
   "metadata": {},
   "source": [
    "### On-policy vs off-policy\n",
    "- **On-policy**: \n",
    "  - Learns and improves the same policy that collects the data (behavior policy = target policy).\n",
    "  - Agent learns by using the same strategy it's trying to improve. Like learning to drive by actually driving the car yourself.\n",
    "    - `SARSA`\n",
    "    - `A2C/A3C`\n",
    "    - `PPO`\n",
    "    - `Monte Carlo policy gradient` (REINFORCE).\n",
    "  - **On-policy analogy**: You improve your recipe by cooking it yourself each time\n",
    "  - **Real-world applications**: Training ChatGPT with PPO where fresh human feedback guides policy updates, robot learning where safety requires predictable policy improvements, and game AI where direct policy optimization works well.\n",
    "\n",
    "- **Off-policy**: \n",
    "  - Learns about a different target policy using data from a behavior policy (including replay of past data).\n",
    "  - Agent learns by observing different strategies or past experiences. Like learning to drive by watching driving videos or using a driving simulator with recorded data.\n",
    "    - `Q-learning`\n",
    "    - `DQN`\n",
    "    - `DDPG`\n",
    "    - `TD3`\n",
    "    - `SAC`\n",
    "  - **Off-policy analogy**: You improve your recipe by studying cooking videos, past attempts, and other chefs' techniques\n",
    "  - **Real-world applications**: Netflix recommendations trained on massive logged user interactions, autonomous driving using years of driving data, and trading algorithms learning from historical market data.\n",
    "\n",
    "### Practical Differences \n",
    "- **Stability vs Efficiency**  \n",
    "  - **On-policy** methods are more stable because they learn directly from the current behavior.  \n",
    "  - **Off-policy** methods are more efficient with data because they reuse old experiences (via replay buffers), but this can make learning less stable.\n",
    "\n",
    "- **Data Usage**  \n",
    "  - **On-policy** discards past experiences after updates and uses only fresh data.  \n",
    "  - **Off-policy** keeps and reuses past experiences, even from older or different policies, to learn better and faster.\n",
    "\n",
    "- **Corrections for Learning**  \n",
    "  - Off-policy methods often use techniques like importance sampling to adjust for differences between the behavior policy (that generated the data) and the target policy (being learned).\n",
    "\n",
    "\n",
    "### Quick comparison table\n",
    "\n",
    "| Algorithm | Method family | Policy type |\n",
    "|---|---|---|\n",
    "| `SARSA` | Value-based TD control | On-policy |\n",
    "| `Expected SARSA` | Value-based TD control | On-policy |\n",
    "| `PPO` | Policy-gradient actor-critic | On-policy  |\n",
    "| `REINFORCE` | Policy gradient | On-policy |\n",
    "| `A2C/A3C` | Actor-critic | On-policy |\n",
    "| `Q-learning` | Value-based TD control | Off-policy |\n",
    "| `DQN` (incl. Double DQN/Rainbow) | Deep value-based | Off-policy|\n",
    "| `DDPG` | Deterministic actor-critic | Off-policy  |\n",
    "| `TD3` | Deterministic actor-critic | Off-policy  |\n",
    "| `SAC` | Stochastic actor-critic (entropy) | Off-policy |\n",
    "\n",
    "### When to Use Each (Simple Explanation)\n",
    "\n",
    "- **Use On-policy methods (like PPO, A2C)** when:  \n",
    "  - You need **stable** and **predictable learning**.  \n",
    "  - The environment allows you to **reset or collect fresh data easily**.  \n",
    "  - Safety or reliable behavior is important (e.g., training robots, fine-tuning language models).\n",
    "\n",
    "- **Use Off-policy methods (like DQN, TD3, SAC)** when:  \n",
    "  - **Collecting new data is expensive or slow**.  \n",
    "  - You have access to **large amounts of past recorded data**.  \n",
    "  - You want to **make the most out of available data** by reusing it (e.g., recommendation systems, autonomous driving, financial trading).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e9525",
   "metadata": {},
   "source": [
    "# 2. ‚≠ê**Monte Carlo Methods in Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be47442",
   "metadata": {},
   "source": [
    "## 2.1 Core Concepts and Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cfbf8",
   "metadata": {},
   "source": [
    "### Monte Carlo Methods in Reinforcement Learning.\n",
    "\n",
    "- Monte Carlo (MC) methods are **model-free**, meaning they do not need to know how the environment works (no model of states or transitions).\n",
    "- They estimate the value of actions $Q(s,a)$ by **averaging the total rewards** (returns) collected over many complete episodes.\n",
    "\n",
    "\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "- The **return** $G_t$ at a time step $t$ is the total discounted reward received from that time onward:\n",
    "  $$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "  - $R_{t+k+1}$ is the reward at step $t+k+1$\n",
    "  - $\\gamma$ (between 0 and 1) is the discount factor, controlling how future rewards are valued\n",
    "\n",
    "- MC methods estimate the action-value function $Q(s,a)$ as the average $G_t$ observed whenever the agent visits state $s$ and takes action $a$:\n",
    "    $$Q(s,a) = \\mathbb{E}[G_t \\mid S_t = s, A_t = a]$$\n",
    "\n",
    "\n",
    "\n",
    "### Why Monte Carlo Methods Are Useful\n",
    "\n",
    "- They provide **unbiased estimates** of $Q$ values given enough episodes (they converge to the true values).\n",
    "- They are **simple to implement** because they update values by averaging actual observed outcomes.\n",
    "- They do **not rely on bootstrapping** (unlike Temporal Difference methods), so they don't update estimates using other estimated values.\n",
    "\n",
    "\n",
    "\n",
    "### Limitations and Practical Considerations\n",
    "\n",
    "- MC methods need **complete episodes** to compute returns ‚Äî they update values only after reaching an episode's end.\n",
    "- Therefore, they work best on **episodic tasks** with clear end points, like games or robot tasks with defined goals.\n",
    "- They can be less efficient and more memory-heavy for **long or continuing tasks** because they store full histories of rewards and actions for each episode.\n",
    "\n",
    "\n",
    "\n",
    "### Real-world Example\n",
    "\n",
    "Here are some other simple real-world examples for Monte Carlo methods in reinforcement learning:\n",
    "\n",
    "- **Robot Navigation**: A robot explores a room trying different paths until it reaches a goal. After each complete trip, the robot looks back at the total rewards (like how fast or obstacle-free the path was) to improve its route choices. Over many trips, it learns the best paths without needing to know the map in advance.\n",
    "\n",
    "- **Game Playing (e.g., Chess or Go)**: An AI plays many complete games. After each game ends, it reviews moves made and the final result (win/loss), updating how valuable each move was. This helps improve strategy over time by averaging results from many full games.\n",
    "\n",
    "- **Inventory Management**: A system manages stock levels in a warehouse. It runs through full cycles of ordering and sales (episodes) and updates decisions on how much stock to keep based on cumulative profit or loss measured at the end of each cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8af755",
   "metadata": {},
   "source": [
    "## 2.2 Episode Collection and Q-Value Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd574b87",
   "metadata": {},
   "source": [
    "### Core Concept  \n",
    "- Collect random episodes by interacting with the environment.  \n",
    "- Estimate Q-values $Q(s,a)$ using Monte Carlo (MC) by averaging returns after episodes.  \n",
    "- Update policy towards the optimal by improving Q-values over time.\n",
    "\n",
    "\n",
    "### Advanced Steps in Monte Carlo Reinforcement Learning\n",
    "\n",
    "#### Step 1: Episode Generation  \n",
    "- Start from a random initial state in the environment.  \n",
    "- Follow the current policy (usually starting random) until the episode finishes.  \n",
    "- Record the sequence of states, actions, and rewards:  \n",
    "  \n",
    "  $$(S_0, A_0, R_1), (S_1, A_1, R_2), \\ldots, (S_{T-1}, A_{T-1}, R_T)$$\n",
    "\n",
    "#### Step 2: Return Calculation  \n",
    "- For each time step $t$ in the episode, compute the return $G_t$ as the discounted sum of rewards from $t+1$ to the end $T$:  \n",
    "  \n",
    "  $$G_t = \\sum_{k=t+1}^{T} \\gamma^{k-(t+1)} R_k$$\n",
    "\n",
    "- Calculate returns by working backwards from the episode‚Äôs terminal state.\n",
    "\n",
    "#### Step 3: Q-Value Update  \n",
    "- For every visited state-action pair $(s, a)$, update the estimate $Q(s, a)$ by averaging all returns $G_i$ observed on the $N(s,a)$ visits so far:  \n",
    "\n",
    "  $$Q(s, a) \\gets \\frac{1}{N(s, a)} \\sum_{i=1}^{N(s, a)} G_i(s, a)$$\n",
    "\n",
    "- Here,  \n",
    "  - $N(s,a)$ is the count of times $(s,a)$ occurred.  \n",
    "  - $G_i(s,a)$ is the return observed at the $i^{th}$ visit to $(s,a)$.\n",
    "\n",
    "\n",
    "This procedure repeats over many episodes, gradually improving the estimates of $Q$-values and thus the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ef2aa",
   "metadata": {},
   "source": [
    "## 2.3 Custom Grid World Environment Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea219c9",
   "metadata": {},
   "source": [
    "### Core Concept\n",
    "- A custom grid world example shows states 0 to 5 with episode data displaying State, Action, Reward, and Return at each step.\n",
    "\n",
    "### Expanded Explanation\n",
    "\n",
    "#### Environment Setup  \n",
    "- The grid world has 6 states (0 to 5) arranged in a specific layout.  \n",
    "- Each state lets the agent choose among actions: Left, Down, Right, Up.  \n",
    "- Actions lead to immediate rewards (which can be positive or negative).  \n",
    "- Episodes end when the agent reaches certain goal states.\n",
    "\n",
    "\n",
    "\n",
    "#### Example Episodes  \n",
    "\n",
    "**Episode 1 Data:**\n",
    "\n",
    "| State | Action | Reward | Return |\n",
    "|-------|--------|--------|--------|\n",
    "| 3     | Right  | -2     | 5      |\n",
    "| 4     | Left   | -1     | 7      |\n",
    "| 3     | Right  | -2     | 8      |\n",
    "| 4     | Right  | 10     | 10     |\n",
    "\n",
    "**Episode 2 Data:**\n",
    "\n",
    "| State | Action | Reward | Return |\n",
    "|-------|--------|--------|--------|\n",
    "| 3     | Right  | -2     | 5      |\n",
    "| 4     | Up     | -1     | 7      |\n",
    "| 1     | Down   | -2     | 8      |\n",
    "| 4     | Right  | 10     | 10     |\n",
    "\n",
    "#### Return Calculation Process\n",
    "\n",
    "- The return is calculated **backwards from the episode‚Äôs end**, summing rewards step-by-step.  \n",
    "- Each return represents the cumulative sum of all future rewards from that time step onward.  \n",
    "- Keep track of every state-action pair $(s, a)$ visited and associate these with their returns for updating value estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8136348",
   "metadata": {},
   "source": [
    "## 2.4 `First-Visit` vs. `Every-Visit` Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0327b",
   "metadata": {},
   "source": [
    "- **Core Concept:**\n",
    "  - Q(3, right) - first-visit Monte Carlo\n",
    "    -  Average first visit to (s,a) within episodes\"\n",
    "  - Q(3, right) - every-visit Monte Carlo  \n",
    "    -  Average every visit to (s,a) within episodes\"\n",
    "\n",
    "### Expanded Explanation:\n",
    "\n",
    "#### First-Visit Monte Carlo:\n",
    "**Definition:** Only the **first occurrence** of each state-action pair $(s,a)$ within an episode contributes to the average return.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "- **Specific case 1:** If $(s,a)$ appears multiple times in episode, only use first return\n",
    "- **Specific case 2:** Each episode contributes at most one sample per $(s,a)$ pair\n",
    "\n",
    "- **Example:**\n",
    "    - For $Q(3, \\text{Right})$ with first-visit:\n",
    "        - Episode 1: First visit return = 5\n",
    "        - Episode 2: First visit return = 5  \n",
    "        - **Average: $(5 + 5)/2 = 5$**\n",
    "\n",
    "#### Every-Visit Monte Carlo:\n",
    "**Definition:** **Every occurrence** of state-action pair $(s,a)$ within episodes contributes to the average.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "- **Specific case 1:** Multiple visits within same episode all contribute\n",
    "- **Specific case 2:** More samples per episode, potentially faster convergence\n",
    "\n",
    "- **Example:**\n",
    "    - For $Q(3, \\text{Right})$ with every-visit:\n",
    "        - Episode 1: Returns = 5, 8 (two visits)\n",
    "        - Episode 2: Return = 5 (one visit)\n",
    "        - **Average: $(5 + 8 + 5)/3 = 6$**\n",
    "\n",
    "#### Why the Difference Matters:\n",
    "**First-Visit Characteristics:**\n",
    "- **Unbiased estimates** of true Q-values\n",
    "- **Lower variance** per episode\n",
    "- **Cleaner theoretical analysis**\n",
    "\n",
    "**Every-Visit Characteristics:**\n",
    "- **Biased estimates** (especially early in learning)\n",
    "- **Higher variance** but more samples\n",
    "- **Potentially faster convergence** in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22699b",
   "metadata": {},
   "source": [
    "## 2.5 Complete Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85aab0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(): \n",
    "    episode = [] \n",
    "    state, info = env.reset()  \n",
    "    terminated = False \n",
    "    while not terminated: \n",
    "        action = env.action_space.sample()  \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)  \n",
    "        episode.append((state, action, reward)) \n",
    "        state = next_state \n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760211af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def generate_episode(env, policy=None, max_steps=1000):\n",
    "    \"\"\"Generate a single episode using given policy or random actions.\"\"\"\n",
    "    episode = []\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    step_count = 0\n",
    "    \n",
    "    while not terminated and not truncated and step_count < max_steps:\n",
    "        # Use provided policy or random action selection\n",
    "        if policy is not None:\n",
    "            action = policy.get(state, env.action_space.sample())\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        step_count += 1\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def calculate_returns(episode, gamma=1.0):\n",
    "    \"\"\"Calculate discounted returns for each step in episode.\"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    # Work backwards through episode\n",
    "    for i in reversed(range(len(episode))):\n",
    "        _, _, reward = episode[i]\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c75512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(num_episodes): \n",
    "    Q = np.zeros((num_states, num_actions)) \n",
    "    returns_sum = np.zeros((num_states, num_actions)) \n",
    "    returns_count = np.zeros((num_states, num_actions))  \n",
    "    for i in range(num_episodes): \n",
    "        episode = generate_episode() \n",
    "        visited_states_actions = set()  \n",
    "        for j, (state, action, reward) in enumerate(episode):  \n",
    "            if (state, action) not in visited_states:  \n",
    "                returns_sum[state, action] += sum([x[2] for x in episode[j:]])  \n",
    "                returns_count[state, action] += 1 \n",
    "                visited_states_actions.add((state, action))  \n",
    "    nonzero_counts = returns_count != 0  \n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts] \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14cbcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(env, num_episodes, num_states, num_actions, gamma=1.0):\n",
    "    \"\"\"First-visit Monte Carlo for estimating Q-values.\"\"\"\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    returns_sum = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode\n",
    "        episode = generate_episode(env)\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Track first visits only\n",
    "        visited_state_actions = set()\n",
    "        \n",
    "        for step, ((state, action, reward), G) in enumerate(zip(episode, returns)):\n",
    "            if (state, action) not in visited_state_actions:\n",
    "                returns_sum[state, action] += G\n",
    "                returns_count[state, action] += 1\n",
    "                visited_state_actions.add((state, action))\n",
    "        \n",
    "        # Print progress periodically\n",
    "        if (episode_num + 1) % 100 == 0:\n",
    "            print(f\"Completed {episode_num + 1}/{num_episodes} episodes\")\n",
    "    \n",
    "    # Calculate final Q-values (avoid division by zero)\n",
    "    nonzero_counts = returns_count > 0\n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    \n",
    "    return Q\n",
    "\n",
    "def every_visit_mc(env, num_episodes, num_states, num_actions, gamma=1.0):\n",
    "    \"\"\"Every-visit Monte Carlo for estimating Q-values.\"\"\"\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    returns_sum = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        episode = generate_episode(env)\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Update for every visit (no set tracking needed)\n",
    "        for (state, action, reward), G in zip(episode, returns):\n",
    "            returns_sum[state, action] += G\n",
    "            returns_count[state, action] += 1\n",
    "        \n",
    "        if (episode_num + 1) % 100 == 0:\n",
    "            print(f\"Completed {episode_num + 1}/{num_episodes} episodes\")\n",
    "    \n",
    "    nonzero_counts = returns_count > 0\n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5797b2",
   "metadata": {},
   "source": [
    "#### Policy Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf2fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(): \n",
    "    policy = {state: np.argmax(Q[state]) for state in range(num_states)}     \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c8813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(Q, num_states):\n",
    "    \"\"\"Derive greedy policy from Q-values.\"\"\"\n",
    "    policy = {}\n",
    "    for state in range(num_states):\n",
    "        # Select action with highest Q-value\n",
    "        best_action = np.argmax(Q[state])\n",
    "        policy[state] = best_action\n",
    "    return policy\n",
    "\n",
    "def evaluate_policy(env, policy, num_eval_episodes=100):\n",
    "    \"\"\"Evaluate policy performance over multiple episodes.\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(num_eval_episodes):\n",
    "        episode_reward = 0\n",
    "        state, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not terminated and not truncated:\n",
    "            action = policy.get(state, env.action_space.sample())\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(total_rewards),\n",
    "        'std_reward': np.std(total_rewards),\n",
    "        'min_reward': np.min(total_rewards),\n",
    "        'max_reward': np.max(total_rewards)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c177f508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running First-Visit Monte Carlo...\n",
      "Completed 100/1000 episodes\n",
      "Completed 200/1000 episodes\n",
      "Completed 300/1000 episodes\n",
      "Completed 400/1000 episodes\n",
      "Completed 500/1000 episodes\n",
      "Completed 600/1000 episodes\n",
      "Completed 700/1000 episodes\n",
      "Completed 800/1000 episodes\n",
      "Completed 900/1000 episodes\n",
      "Completed 1000/1000 episodes\n",
      "Running Every-Visit Monte Carlo...\n",
      "Completed 100/1000 episodes\n",
      "Completed 200/1000 episodes\n",
      "Completed 300/1000 episodes\n",
      "Completed 400/1000 episodes\n",
      "Completed 500/1000 episodes\n",
      "Completed 600/1000 episodes\n",
      "Completed 700/1000 episodes\n",
      "Completed 800/1000 episodes\n",
      "Completed 900/1000 episodes\n",
      "Completed 1000/1000 episodes\n",
      "Evaluating First-Visit Policy...\n",
      "First-visit policy: {0: np.int64(1), 1: np.int64(2), 2: np.int64(1), 3: np.int64(0), 4: np.int64(1), 5: np.int64(0), 6: np.int64(1), 7: np.int64(0), 8: np.int64(2), 9: np.int64(1), 10: np.int64(1), 11: np.int64(0), 12: np.int64(0), 13: np.int64(2), 14: np.int64(2), 15: np.int64(0)}\n",
      "Performance: {'mean_reward': np.float64(1.0), 'std_reward': np.float64(0.0), 'min_reward': np.float64(1.0), 'max_reward': np.float64(1.0)}\n",
      "Evaluating Every-Visit Policy...\n",
      "Every-visit policy: {0: np.int64(0), 1: np.int64(2), 2: np.int64(1), 3: np.int64(0), 4: np.int64(1), 5: np.int64(0), 6: np.int64(1), 7: np.int64(0), 8: np.int64(2), 9: np.int64(1), 10: np.int64(1), 11: np.int64(0), 12: np.int64(0), 13: np.int64(2), 14: np.int64(2), 15: np.int64(0)}\n",
      "Performance: {'mean_reward': np.float64(0.0), 'std_reward': np.float64(0.0), 'min_reward': np.float64(0.0), 'max_reward': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Run Monte Carlo methods\n",
    "print(\"Running First-Visit Monte Carlo...\")\n",
    "Q_first = first_visit_mc(env, 1000, num_states, num_actions)\n",
    "policy_first = get_policy(Q_first, num_states)\n",
    "\n",
    "print(\"Running Every-Visit Monte Carlo...\")\n",
    "Q_every = every_visit_mc(env, 1000, num_states, num_actions)\n",
    "policy_every = get_policy(Q_every, num_states)\n",
    "\n",
    "# Evaluate policies\n",
    "print(\"Evaluating First-Visit Policy...\")\n",
    "eval_first = evaluate_policy(env, policy_first)\n",
    "print(f\"First-visit policy: {policy_first}\")\n",
    "print(f\"Performance: {eval_first}\")\n",
    "\n",
    "print(\"Evaluating Every-Visit Policy...\")\n",
    "eval_every = evaluate_policy(env, policy_every)\n",
    "print(f\"Every-visit policy: {policy_every}\")\n",
    "print(f\"Performance: {eval_every}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
