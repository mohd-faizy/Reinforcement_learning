{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e052b5",
   "metadata": {},
   "source": [
    "# **‚ú®Monte Carlo Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457416b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [**Introduction to Reinforcement Learning Paradigms**](#1-introduction-to-reinforcement-learning-paradigms)\n",
    "   - [1.1 Model-Based Learning Recap](#11-model-based-learning-recap)\n",
    "   - [1.2 Model-Free Learning Fundamentals](#12-model-free-learning-fundamentals)\n",
    "\n",
    "1. [**Monte Carlo Methods in Reinforcement Learning**](#2-monte-carlo-methods-in-reinforcement-learning)\n",
    "   - [2.1 Core Concepts and Definitions](#21-core-concepts-and-definitions)\n",
    "   - [2.2 Episode Collection and Q-Value Estimation](#22-episode-collection-and-q-value-estimation)\n",
    "   - [2.3 Custom Grid World Environment Example](#23-custom-grid-world-environment-example)\n",
    "   - [2.4 First-Visit vs. Every-Visit Monte Carlo Methods](#24-first-visit-vs-every-visit-monte-carlo-methods)\n",
    "   - [2.5 Complete Code Implementation](#25-complete-code-implementation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d962139",
   "metadata": {},
   "source": [
    "# 1. ‚≠ê**Introduction to Reinforcement Learning Paradigms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0505ec",
   "metadata": {},
   "source": [
    "## 1.1 ‚úîÔ∏èModel-Based Reinforcement Learning(Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddba50",
   "metadata": {},
   "source": [
    "### üéØ **Core Concept**\n",
    "\n",
    "**Model-based learning** assumes you have **complete knowledge** of how the environment works - like having the rulebook for a game before you play.[1][4]\n",
    "\n",
    "- **Environment dynamics are known**: You understand $P(s'|s,a)$ (transition probabilities) and $R(s,a)$ (reward function)\n",
    "- **No trial-and-error needed**: Can calculate optimal actions mathematically\n",
    "- **Planning-based approach**: Think first, act later[2]\n",
    "\n",
    "\n",
    "### üßÆ **Mathematical Foundations**\n",
    "\n",
    "When you know the environment model, you can predict:\n",
    "- **Next state**: Given current state $s$ and action $a$, what's the probability of reaching state $s'$?\n",
    "- **Expected reward**: What reward do you get for taking action $a$ in state $s$?\n",
    "\n",
    "This knowledge enables **dynamic programming** techniques for finding optimal policies.[4]\n",
    "\n",
    "\n",
    "\n",
    "### üîß **Core Algorithms**\n",
    "\n",
    "#### **üîñ Policy Iteration** \n",
    "*\"Improve the policy step by step\"*\n",
    "\n",
    "**Process**: Initialize policy ‚Üí Evaluate policy ‚Üí Improve policy ‚Üí Repeat until optimal\n",
    "\n",
    "##### **1. Policy Evaluation** (How good is my current policy?)\n",
    "\n",
    "**Full Formula**:\n",
    "$$V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)[R(s,a) + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "**Simplified Approach**:\n",
    "- **Iterative updates**: $V(s) \\leftarrow \\sum_a \\pi(a|s) [r + \\gamma V(s')]$\n",
    "- **No complex summations** - just update each state value repeatedly\n",
    "- **Stop when values converge**\n",
    "\n",
    "##### **2. Policy Improvement** (Make the policy better)\n",
    "\n",
    "**Full Formula**:\n",
    "$$\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a)[R(s,a) + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "**Simplified Approach**:\n",
    "- **Act greedily**: Choose action with highest expected value\n",
    "- $\\pi'(s) = \\arg\\max_a Q^{\\pi}(s,a)$ where $Q^{\\pi}(s,a) = r + \\gamma V^{\\pi}(s')$\n",
    "\n",
    "\n",
    "\n",
    "#### **üîñ Value Iteration**\n",
    "*\"Find the best value for each state directly\"*\n",
    "\n",
    "**Process**: Initialize values ‚Üí Update values by selecting best actions ‚Üí Repeat until optimal\n",
    "\n",
    "##### **Full Formula**:\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a) + \\gamma V_k(s')]$$\n",
    "\n",
    "##### **Simplified Understanding**:\n",
    "- **One-step lookahead**: For each state, try all actions and pick the best\n",
    "- **Direct optimization**: No separate policy - values directly give you optimal actions\n",
    "- **Policy extraction**: $\\pi(s) = \\arg\\max_a Q(s,a)$\n",
    "\n",
    "\n",
    "\n",
    "### üìä **Key Variables Explained**\n",
    "\n",
    "| Symbol | Meaning | Simple Explanation |\n",
    "|--------|---------|-------------------|\n",
    "| $V^{\\pi}(s)$ | Value of state $s$ under policy $\\pi$ | \"How good is this state if I follow my current strategy?\" |\n",
    "| $\\pi(a\\|s)$ | Probability of taking action $a$ in state $s$ | \"How likely am I to choose this action here?\" |\n",
    "| $\\gamma$ | Discount factor (0 ‚â§ Œ≥ ‚â§ 1) | \"How much do I care about future rewards vs immediate ones?\" |\n",
    "| $P(s'\\|s,a)$ | Transition probability | \"If I do this action here, where will I end up?\" |\n",
    "| $R(s,a)$ | Reward function | \"What reward do I get for this action in this state?\" |\n",
    "\n",
    "\n",
    "\n",
    "### ‚ö° **Why Model-Based Learning Matters**\n",
    "\n",
    "#### **Advantages**:\n",
    "- **‚ö° Sample Efficiency**: No need for trial-and-error - can solve mathematically\n",
    "- **üéØ Computational Efficiency**: Planning is faster than learning through experience  \n",
    "- **üìà Theoretical Guarantees**: Provable convergence to optimal policies\n",
    "- **üîÑ Quick Adaptation**: Can immediately adjust to goal changes\n",
    "\n",
    "#### **Limitations**:[1]\n",
    "- **ü§î Model Complexity**: Real environments are often too complex to model accurately\n",
    "- **‚ùì Unknown Dynamics**: Many real-world scenarios don't provide transition probabilities\n",
    "- **‚ö†Ô∏è Model Errors**: Wrong model leads to suboptimal policies\n",
    "- **üîÑ Non-Stationary**: Environments that change over time break the model\n",
    "\n",
    "\n",
    "\n",
    "### üéÆ **When to Use Model-Based vs Model-Free**\n",
    "\n",
    "| **Use Model-Based When**[4] | **Use Model-Free When** |\n",
    "|------------------------------|---------------------------|\n",
    "| Environment rules are known | Environment is complex/unknown |\n",
    "| Sample efficiency is critical | Can afford many interactions |\n",
    "| Planning is computationally feasible | Environment changes frequently |\n",
    "| **Examples**: Chess, Grid worlds | **Examples**: Video games, Robotics |\n",
    "\n",
    "\n",
    "\n",
    "### üöÄ **Practical Takeaways**\n",
    "\n",
    "1. **Start Simple**: If you can model the environment, model-based is often faster.\n",
    "2. **Know Your Limits**: Complex real-world problems usually need model-free approaches.\n",
    "3. **Hybrid Approaches**: Many modern systems combine both methods.\n",
    "4. **Simplified Formulas**: Use iterative updates instead of complex summations for easier implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71771bd1",
   "metadata": {},
   "source": [
    "## 1.2 ‚úîÔ∏èModel-Free Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e33cf",
   "metadata": {},
   "source": [
    "### üéØ **Core Concept**\n",
    "\n",
    "**Model-free learning** is like learning to ride a bike by actually riding it - no instruction manual needed, just **trial and error**\n",
    "\n",
    "- **Experience-based**: Learn from sequences of $(s, a, r, s')$ tuples (state, action, reward, next state)\n",
    "- **No environment model required**: Don't need to know $P(s'|s,a)$ or $R(s,a)$ in advance\n",
    "- **Direct interaction**: Agent learns by doing, not by thinking\n",
    "\n",
    "\n",
    "\n",
    "### üßÆ **Mathematical Foundations (Simplified)**\n",
    "\n",
    "Instead of complex environment models, model-free methods use **direct experience**:\n",
    "\n",
    "- **Experience tuple**: $(s_t, a_t, r_{t+1}, s_{t+1})$ - \"I was here, did this, got this reward, ended up there\"\n",
    "- **Value estimation**: Learn $V(s)$ or $Q(s,a)$ directly from observed rewards\n",
    "- **Policy learning**: Improve actions based on **actual outcomes**, not predictions\n",
    "\n",
    "\n",
    "\n",
    "### üîß **Core Algorithm Categories**\n",
    "\n",
    "#### **üîñ Value-Based Methods**\n",
    "*\"Learn how good each action is, then pick the best one\"*\n",
    "\n",
    "##### **Q-Learning** (Most Popular):\n",
    "**Simplified Formula**:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "**In Plain English**:\n",
    "- **Update rule**: \"My estimate + learning_rate √ó (what_actually_happened - my_estimate)\"\n",
    "- **No model needed**: Just observe $(s, a, r, s')$ and update\n",
    "- **Policy**: Always choose $\\pi(s) = \\arg\\max_a Q(s,a)$\n",
    "\n",
    "##### **Deep Q-Networks (DQN)**:\n",
    "- **Same Q-learning principle** but uses neural networks for complex state spaces\n",
    "- **Handles high-dimensional inputs** like images (Atari games)\n",
    "- **Experience replay**: Learn from past experiences multiple times\n",
    "\n",
    "\n",
    "\n",
    "#### **üîñ Policy-Based Methods**\n",
    "*\"Learn the strategy directly, skip the value estimation\"*\n",
    "\n",
    "##### **REINFORCE Algorithm**:\n",
    "**Simplified Formula**:\n",
    "$$\\nabla J(\\theta) = \\mathbb{E}[\\nabla \\log \\pi_\\theta(a|s) \\cdot R]$$\n",
    "\n",
    "**In Plain English**:\n",
    "- **Direct policy optimization**: Adjust policy parameters to maximize rewards\n",
    "- **Monte Carlo approach**: Use complete episode returns\n",
    "- **No value function needed**: Just improve policy based on episode outcomes\n",
    "\n",
    "\n",
    "\n",
    "#### **üîñ Actor-Critic Methods**\n",
    "*\"Best of both worlds: Learn values AND policy\"*\n",
    "\n",
    "**Two Components**:\n",
    "- **Actor**: Learns the policy $\\pi(a|s)$ (what to do)\n",
    "- **Critic**: Learns the value function $V(s)$ (how good is this state)\n",
    "\n",
    "**Popular Algorithms**: A2C, A3C, PPO\n",
    "\n",
    "\n",
    "\n",
    "### üìä **Key Characteristics & Advantages**\n",
    "\n",
    "#### **‚úÖ Why Model-Free Works**:\n",
    "\n",
    "| **Advantage** | **Explanation** |\n",
    "|---------------|----------------|\n",
    "| **üåç Real-World Ready** | No need to model complex environments (robotics, games) |\n",
    "| **üîÑ Adapts Naturally** | Handles changing environments without remodeling |\n",
    "| **üéØ Robust to Uncertainty** | Works even when environment dynamics are unknown |\n",
    "| **üìà Scalable** | Handles high-dimensional state spaces better |\n",
    "\n",
    "#### **‚ö†Ô∏è Challenges**:[3]\n",
    "\n",
    "| **Challenge** | **Impact** |\n",
    "|---------------|------------|\n",
    "| **‚è∞ Sample Inefficiency** | Needs many interactions to learn well |\n",
    "| **üé≤ High Variance** | Learning can be unstable and noisy |\n",
    "| **‚öñÔ∏è Exploration-Exploitation** | Hard to balance trying new things vs using known good actions|\n",
    "| **üîß Hyperparameter Sensitivity** | Performance depends heavily on tuning|\n",
    "\n",
    "\n",
    "### üéÆ **Real-World Applications**\n",
    "\n",
    "#### **Perfect for Model-Free**:\n",
    "- **ü§ñ Autonomous Navigation**: Self-driving cars in traffic\n",
    "- **üéØ Game Playing**: Chess, Go, video games (AlphaGo, OpenAI Five)\n",
    "- **üí∞ Financial Trading**: Stock market strategies\n",
    "- **‚òÅÔ∏è Cloud Computing**: Resource allocation and load balancing\n",
    "- **üè≠ Robotics**: Manipulation tasks in unstructured environments\n",
    "\n",
    "***\n",
    "\n",
    "### üÜö **Model-Free vs Model-Based Comparison**\n",
    "\n",
    "| **Aspect** | **Model-Free** | **Model-Based** |\n",
    "|------------|----------------|-----------------|\n",
    "| **Learning Method** | Trial and error | Mathematical planning |\n",
    "| **Environment Knowledge** | None required | Complete model needed |\n",
    "| **Sample Efficiency** | Low (needs many samples) | High (few samples needed) |\n",
    "| **Real-World Suitability** | Excellent | Limited to known environments |\n",
    "| **Computational Cost** | Low per step | High (planning cost) |\n",
    "| **Robustness** | High (adapts to changes) | Low (breaks with model errors) |\n",
    "\n",
    "\n",
    "\n",
    "### üß† **Simplified Algorithm Comparison**\n",
    "\n",
    "#### **Q-Learning Example**:\n",
    "```\n",
    "1. Start with random Q-values\n",
    "2. Take action, observe reward\n",
    "3. Update: Q(s,a) += Œ±[r + Œ≥√ómax(Q(s',a')) - Q(s,a)]\n",
    "4. Repeat until convergence\n",
    "```\n",
    "\n",
    "#### **Policy Gradient Example**:\n",
    "```\n",
    "1. Start with random policy\n",
    "2. Run episode, collect rewards\n",
    "3. Update: Increase probability of good actions\n",
    "4. Repeat until optimal\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### üöÄ **Practical Takeaways**\n",
    "\n",
    "#### **Choose Model-Free When**:\n",
    "- ‚úÖ Environment is **complex or unknown**\n",
    "- ‚úÖ Environment **changes over time**\n",
    "- ‚úÖ You can afford **many interactions**\n",
    "- ‚úÖ **Safety** is more important than efficiency\n",
    "- ‚úÖ Working with **high-dimensional** problems\n",
    "\n",
    "#### **Implementation Tips**:\n",
    "1. **Start with Q-Learning** for discrete problems\n",
    "2. **Use DQN** for complex state spaces\n",
    "3. **Try Actor-Critic** for continuous actions\n",
    "4. **Focus on exploration strategies** early in learning\n",
    "5. **Monitor sample efficiency** - if too slow, consider model-based approaches\n",
    "\n",
    "\n",
    "\n",
    "### üí° **The Big Picture**\n",
    "\n",
    "Model-free reinforcement learning is like **learning to drive in real traffic** rather than studying traffic rules in a classroom. It's messier, takes longer, but ultimately produces agents that can handle **real-world complexity and uncertainty**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e9525",
   "metadata": {},
   "source": [
    "# 2. ‚≠ê**Monte Carlo Methods in Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be47442",
   "metadata": {},
   "source": [
    "## 2.1 Core Concepts and Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cfbf8",
   "metadata": {},
   "source": [
    "**Core Concept:**\n",
    "- Monte Carlo methods\n",
    "    - Model-free techniques\n",
    "    - Estimate Q-values based on episodes\n",
    "\n",
    "### Expanded Explanation:\n",
    "\n",
    "**Monte Carlo (MC) methods** form a class of model-free reinforcement learning algorithms that estimate action-value functions $Q(s,a)$ by averaging returns observed across multiple episodes.\n",
    "\n",
    "#### Mathematical Foundations:\n",
    "\n",
    "**The Concept of Return:**\n",
    "The **return** (denoted as $G_t$) represents the cumulative discounted reward from time step $t$:\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "Where:\n",
    "- $R_{t+k+1}$ = Reward received at time step $t+k+1$\n",
    "- $\\gamma$ = Discount factor (0 ‚â§ Œ≥ ‚â§ 1)\n",
    "\n",
    "**Q-Value Estimation:**\n",
    "MC methods estimate $Q(s,a)$ as the expected return following visits to state-action pair $(s,a)$:\n",
    "$$Q(s,a) = \\mathbb{E}[G_t | S_t = s, A_t = a]$$\n",
    "\n",
    "#### Why Monte Carlo Methods Matter:\n",
    "- **Unbiased Estimates**: Converge to true Q-values with infinite episodes\n",
    "- **Simple Implementation**: Straightforward averaging of observed returns\n",
    "- **No Bootstrapping**: Don't rely on estimates of other values\n",
    "\n",
    "#### Long-term Consequences:\n",
    "MC methods require **complete episodes** to update values, making them:\n",
    "- Suitable for episodic tasks with clear terminal states\n",
    "- Less efficient for continuing tasks or very long episodes\n",
    "- Memory-intensive for storing complete episode histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8af755",
   "metadata": {},
   "source": [
    "## 2.2 Episode Collection and Q-Value Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd574b87",
   "metadata": {},
   "source": [
    "**Core Concept:** - *Collecting random episodes ‚Üí Estimate Q-values using MC ‚Üí Optimal policy*\n",
    "\n",
    "### Advanced Implementation Details:\n",
    "\n",
    "#### Core Algorithm:\n",
    "**Step 1: Episode Generation**\n",
    "- Initialize environment to random starting state\n",
    "- Follow policy (initially random) until episode termination\n",
    "- Record sequence: $(S_0, A_0, R_1), (S_1, A_1, R_2), \\ldots, (S_{T-1}, A_{T-1}, R_T)$\n",
    "\n",
    "**Step 2: Return Calculation**\n",
    "- For each time step $t$, compute: $G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k$\n",
    "- Calculate returns working backwards from terminal state\n",
    "\n",
    "**Step 3: Q-Value Updates**\n",
    "- For each $(s,a)$ pair visited, update running average:\n",
    "- $Q(s,a) \\leftarrow \\frac{1}{N(s,a)} \\sum_{i=1}^{N(s,a)} G_i(s,a)$\n",
    "\n",
    "Where:\n",
    "- $N(s,a)$ = Number of times $(s,a)$ has been visited\n",
    "- $G_i(s,a)$ = Return from $i$-th visit to $(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ef2aa",
   "metadata": {},
   "source": [
    "## 2.3 Custom Grid World Environment Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea219c9",
   "metadata": {},
   "source": [
    "**Core Concept from PDF:** - *Custom grid world [showing states 0-5] with episode data showing State, Action, Reward, Return.*\n",
    "\n",
    "### Expanded Explanation:\n",
    "\n",
    "#### Environment Setup:\n",
    "The custom grid world consists of 6 states (0-5) arranged in a specific layout where:\n",
    "- Each state allows multiple actions (Left, Down, Right, Up)\n",
    "- Actions result in immediate rewards (can be negative)\n",
    "- Episodes terminate when reaching specific goal states\n",
    "\n",
    "#### Example Episode Analysis:\n",
    "\n",
    "**Episode 1 Data:**\n",
    "| State | Action | Reward | Return |\n",
    "|-------|--------|--------|--------|\n",
    "| 3     | Right  | -2     | 5      |\n",
    "| 4     | Left   | -1     | 7      |\n",
    "| 3     | Right  | -2     | 8      |\n",
    "| 4     | Right  | 10     | 10     |\n",
    "\n",
    "**Episode 2 Data:**\n",
    "| State | Action | Reward | Return |\n",
    "|-------|--------|--------|--------|\n",
    "| 3     | Right  | -2     | 5      |\n",
    "| 4     | Up     | -1     | 7      |\n",
    "| 1     | Down   | -2     | 8      |\n",
    "| 4     | Right  | 10     | 10     |\n",
    "\n",
    "#### Return Calculation Process:\n",
    "- **Working Backwards**: Calculate returns from episode end to start\n",
    "- **Cumulative Rewards**: Each return includes all future rewards in episode\n",
    "- **State-Action Visits**: Track which $(s,a)$ pairs occur and their associated returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8136348",
   "metadata": {},
   "source": [
    "## 2.4 `First-Visit` vs. `Every-Visit` Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0327b",
   "metadata": {},
   "source": [
    "- **Core Concept:**\n",
    "  - Q(3, right) - first-visit Monte Carlo\n",
    "    -  Average first visit to (s,a) within episodes\"\n",
    "  - Q(3, right) - every-visit Monte Carlo  \n",
    "    -  Average every visit to (s,a) within episodes\"\n",
    "\n",
    "### Expanded Explanation:\n",
    "\n",
    "#### First-Visit Monte Carlo:\n",
    "**Definition:** Only the **first occurrence** of each state-action pair $(s,a)$ within an episode contributes to the average return.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "- **Specific case 1:** If $(s,a)$ appears multiple times in episode, only use first return\n",
    "- **Specific case 2:** Each episode contributes at most one sample per $(s,a)$ pair\n",
    "\n",
    "- **Example:**\n",
    "    - For $Q(3, \\text{Right})$ with first-visit:\n",
    "        - Episode 1: First visit return = 5\n",
    "        - Episode 2: First visit return = 5  \n",
    "        - **Average: $(5 + 5)/2 = 5$**\n",
    "\n",
    "#### Every-Visit Monte Carlo:\n",
    "**Definition:** **Every occurrence** of state-action pair $(s,a)$ within episodes contributes to the average.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "- **Specific case 1:** Multiple visits within same episode all contribute\n",
    "- **Specific case 2:** More samples per episode, potentially faster convergence\n",
    "\n",
    "- **Example:**\n",
    "    - For $Q(3, \\text{Right})$ with every-visit:\n",
    "        - Episode 1: Returns = 5, 8 (two visits)\n",
    "        - Episode 2: Return = 5 (one visit)\n",
    "        - **Average: $(5 + 8 + 5)/3 = 6$**\n",
    "\n",
    "#### Why the Difference Matters:\n",
    "**First-Visit Characteristics:**\n",
    "- **Unbiased estimates** of true Q-values\n",
    "- **Lower variance** per episode\n",
    "- **Cleaner theoretical analysis**\n",
    "\n",
    "**Every-Visit Characteristics:**\n",
    "- **Biased estimates** (especially early in learning)\n",
    "- **Higher variance** but more samples\n",
    "- **Potentially faster convergence** in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22699b",
   "metadata": {},
   "source": [
    "## 2.5 Complete Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85aab0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(): \n",
    "    episode = [] \n",
    "    state, info = env.reset()  \n",
    "    terminated = False \n",
    "    while not terminated: \n",
    "        action = env.action_space.sample()  \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)  \n",
    "        episode.append((state, action, reward)) \n",
    "        state = next_state \n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760211af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def generate_episode(env, policy=None, max_steps=1000):\n",
    "    \"\"\"Generate a single episode using given policy or random actions.\"\"\"\n",
    "    episode = []\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    step_count = 0\n",
    "    \n",
    "    while not terminated and not truncated and step_count < max_steps:\n",
    "        # Use provided policy or random action selection\n",
    "        if policy is not None:\n",
    "            action = policy.get(state, env.action_space.sample())\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        step_count += 1\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def calculate_returns(episode, gamma=1.0):\n",
    "    \"\"\"Calculate discounted returns for each step in episode.\"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    \n",
    "    # Work backwards through episode\n",
    "    for i in reversed(range(len(episode))):\n",
    "        _, _, reward = episode[i]\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c75512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(num_episodes): \n",
    "    Q = np.zeros((num_states, num_actions)) \n",
    "    returns_sum = np.zeros((num_states, num_actions)) \n",
    "    returns_count = np.zeros((num_states, num_actions))  \n",
    "    for i in range(num_episodes): \n",
    "        episode = generate_episode() \n",
    "        visited_states_actions = set()  \n",
    "        for j, (state, action, reward) in enumerate(episode):  \n",
    "            if (state, action) not in visited_states:  \n",
    "                returns_sum[state, action] += sum([x[2] for x in episode[j:]])  \n",
    "                returns_count[state, action] += 1 \n",
    "                visited_states_actions.add((state, action))  \n",
    "    nonzero_counts = returns_count != 0  \n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts] \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14cbcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(env, num_episodes, num_states, num_actions, gamma=1.0):\n",
    "    \"\"\"First-visit Monte Carlo for estimating Q-values.\"\"\"\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    returns_sum = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        # Generate episode\n",
    "        episode = generate_episode(env)\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Track first visits only\n",
    "        visited_state_actions = set()\n",
    "        \n",
    "        for step, ((state, action, reward), G) in enumerate(zip(episode, returns)):\n",
    "            if (state, action) not in visited_state_actions:\n",
    "                returns_sum[state, action] += G\n",
    "                returns_count[state, action] += 1\n",
    "                visited_state_actions.add((state, action))\n",
    "        \n",
    "        # Print progress periodically\n",
    "        if (episode_num + 1) % 100 == 0:\n",
    "            print(f\"Completed {episode_num + 1}/{num_episodes} episodes\")\n",
    "    \n",
    "    # Calculate final Q-values (avoid division by zero)\n",
    "    nonzero_counts = returns_count > 0\n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    \n",
    "    return Q\n",
    "\n",
    "def every_visit_mc(env, num_episodes, num_states, num_actions, gamma=1.0):\n",
    "    \"\"\"Every-visit Monte Carlo for estimating Q-values.\"\"\"\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    returns_sum = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        episode = generate_episode(env)\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        \n",
    "        # Update for every visit (no set tracking needed)\n",
    "        for (state, action, reward), G in zip(episode, returns):\n",
    "            returns_sum[state, action] += G\n",
    "            returns_count[state, action] += 1\n",
    "        \n",
    "        if (episode_num + 1) % 100 == 0:\n",
    "            print(f\"Completed {episode_num + 1}/{num_episodes} episodes\")\n",
    "    \n",
    "    nonzero_counts = returns_count > 0\n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5797b2",
   "metadata": {},
   "source": [
    "#### Policy Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf2fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(): \n",
    "    policy = {state: np.argmax(Q[state]) for state in range(num_states)}     \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c8813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(Q, num_states):\n",
    "    \"\"\"Derive greedy policy from Q-values.\"\"\"\n",
    "    policy = {}\n",
    "    for state in range(num_states):\n",
    "        # Select action with highest Q-value\n",
    "        best_action = np.argmax(Q[state])\n",
    "        policy[state] = best_action\n",
    "    return policy\n",
    "\n",
    "def evaluate_policy(env, policy, num_eval_episodes=100):\n",
    "    \"\"\"Evaluate policy performance over multiple episodes.\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(num_eval_episodes):\n",
    "        episode_reward = 0\n",
    "        state, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not terminated and not truncated:\n",
    "            action = policy.get(state, env.action_space.sample())\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(total_rewards),\n",
    "        'std_reward': np.std(total_rewards),\n",
    "        'min_reward': np.min(total_rewards),\n",
    "        'max_reward': np.max(total_rewards)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c177f508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running First-Visit Monte Carlo...\n",
      "Completed 100/1000 episodes\n",
      "Completed 200/1000 episodes\n",
      "Completed 300/1000 episodes\n",
      "Completed 400/1000 episodes\n",
      "Completed 500/1000 episodes\n",
      "Completed 600/1000 episodes\n",
      "Completed 700/1000 episodes\n",
      "Completed 800/1000 episodes\n",
      "Completed 900/1000 episodes\n",
      "Completed 1000/1000 episodes\n",
      "Running Every-Visit Monte Carlo...\n",
      "Completed 100/1000 episodes\n",
      "Completed 200/1000 episodes\n",
      "Completed 300/1000 episodes\n",
      "Completed 400/1000 episodes\n",
      "Completed 500/1000 episodes\n",
      "Completed 600/1000 episodes\n",
      "Completed 700/1000 episodes\n",
      "Completed 800/1000 episodes\n",
      "Completed 900/1000 episodes\n",
      "Completed 1000/1000 episodes\n",
      "Evaluating First-Visit Policy...\n",
      "First-visit policy: {0: np.int64(1), 1: np.int64(2), 2: np.int64(1), 3: np.int64(0), 4: np.int64(1), 5: np.int64(0), 6: np.int64(1), 7: np.int64(0), 8: np.int64(2), 9: np.int64(1), 10: np.int64(1), 11: np.int64(0), 12: np.int64(0), 13: np.int64(2), 14: np.int64(2), 15: np.int64(0)}\n",
      "Performance: {'mean_reward': np.float64(1.0), 'std_reward': np.float64(0.0), 'min_reward': np.float64(1.0), 'max_reward': np.float64(1.0)}\n",
      "Evaluating Every-Visit Policy...\n",
      "Every-visit policy: {0: np.int64(0), 1: np.int64(2), 2: np.int64(1), 3: np.int64(0), 4: np.int64(1), 5: np.int64(0), 6: np.int64(1), 7: np.int64(0), 8: np.int64(2), 9: np.int64(1), 10: np.int64(1), 11: np.int64(0), 12: np.int64(0), 13: np.int64(2), 14: np.int64(2), 15: np.int64(0)}\n",
      "Performance: {'mean_reward': np.float64(0.0), 'std_reward': np.float64(0.0), 'min_reward': np.float64(0.0), 'max_reward': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Run Monte Carlo methods\n",
    "print(\"Running First-Visit Monte Carlo...\")\n",
    "Q_first = first_visit_mc(env, 1000, num_states, num_actions)\n",
    "policy_first = get_policy(Q_first, num_states)\n",
    "\n",
    "print(\"Running Every-Visit Monte Carlo...\")\n",
    "Q_every = every_visit_mc(env, 1000, num_states, num_actions)\n",
    "policy_every = get_policy(Q_every, num_states)\n",
    "\n",
    "# Evaluate policies\n",
    "print(\"Evaluating First-Visit Policy...\")\n",
    "eval_first = evaluate_policy(env, policy_first)\n",
    "print(f\"First-visit policy: {policy_first}\")\n",
    "print(f\"Performance: {eval_first}\")\n",
    "\n",
    "print(\"Evaluating Every-Visit Policy...\")\n",
    "eval_every = evaluate_policy(env, policy_every)\n",
    "print(f\"Every-visit policy: {policy_every}\")\n",
    "print(f\"Performance: {eval_every}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
