{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d6f4be",
   "metadata": {},
   "source": [
    "# **Summary of `05` & `06` Notebooks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633cab3",
   "metadata": {},
   "source": [
    "## **üìëTable of Contents**\n",
    "\n",
    "1. [Part A: Monte Carlo Methods](#Part-A:-Monte-Carlo-Methods)\n",
    "2. [Part B: Temporal Difference Learning (SARSA)](#Part-B:-Temporal-Difference-Learning-(SARSA))\n",
    "3. [Part C: Q-Learning](#Part-C:-Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec0c0ae",
   "metadata": {},
   "source": [
    "## **Part A: Monte Carlo Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18675b1c",
   "metadata": {},
   "source": [
    "- Model-free learning technique\n",
    "- Estimates Q-values based on complete episodes\n",
    "- Two main approaches: first-visit MC and every-visit MC\n",
    "- Suitable for episodic tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5046bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# PART A: MONTE CARLO METHODS - EPISODE GENERATION\n",
    "# ==================================================================================================\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def generate_episode():\n",
    "    \"\"\"Generate a complete episode using random actions\"\"\"\n",
    "    episode = []\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = env.action_space.sample()  # Random action selection\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# Example episode structure: [(state, action, reward), ...]\n",
    "\n",
    "# ==================================================================================================\n",
    "# FIRST-VISIT MONTE CARLO IMPLEMENTATION\n",
    "# ==================================================================================================\n",
    "def first_visit_mc(num_episodes):\n",
    "    \"\"\"First-visit Monte Carlo method for Q-value estimation\"\"\"\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    returns_sum = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        episode = generate_episode()\n",
    "        visited_states_actions = set()  # Track first visits only\n",
    "        \n",
    "        for j, (state, action, reward) in enumerate(episode):\n",
    "            if (state, action) not in visited_states_actions:\n",
    "                # Calculate return from this point onwards\n",
    "                returns_sum[state, action] += sum([x[2] for x in episode[j:]])\n",
    "                returns_count[state, action] += 1\n",
    "                visited_states_actions.add((state, action))\n",
    "    \n",
    "    # Calculate Q-values as average returns\n",
    "    nonzero_counts = returns_count != 0\n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    return Q \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5e381",
   "metadata": {},
   "source": [
    "## **Part B: Temporal Difference Learning (SARSA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02a0b2",
   "metadata": {},
   "source": [
    "- TD learning updates Q-table at each step within episodes\n",
    "- SARSA is an on-policy method (learns from actions actually taken)\n",
    "- More suitable for long or indefinite episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43529c85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m         state, action = next_state, next_action\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Extract optimal policy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m policy_sarsa = \u001b[43mget_policy\u001b[49m(Q)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSARSA Policy:\u001b[39m\u001b[33m\"\u001b[39m, policy_sarsa)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_policy' is not defined"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================\n",
    "# PART B: SARSA IMPLEMENTATION\n",
    "# ==================================================================================================\n",
    "# Environment setup for SARSA\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# SARSA parameters\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "alpha = 0.1      # Learning rate\n",
    "gamma = 1.0      # Discount factor\n",
    "num_episodes = 1000\n",
    "\n",
    "def update_q_table_sarsa(state, action, reward, next_state, next_action):\n",
    "    \"\"\"SARSA update rule: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥Q(s',a') - Q(s,a)]\"\"\"\n",
    "    old_value = Q[state, action]\n",
    "    next_value = Q[next_state, next_action]\n",
    "    Q[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_value)\n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# SARSA TRAINING LOOP\n",
    "# ==================================================================================================\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    action = env.action_space.sample()  # Initial action selection\n",
    "    terminated = False\n",
    "    \n",
    "    while not terminated:\n",
    "        # Take action and observe next state and reward\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_action = env.action_space.sample()  # Next action selection\n",
    "        \n",
    "        # SARSA update\n",
    "        update_q_table_sarsa(state, action, reward, next_state, next_action)\n",
    "        \n",
    "        # Move to next state-action pair\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "# Extract optimal policy\n",
    "policy_sarsa = get_policy(Q)\n",
    "print(\"SARSA Policy:\", policy_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff340a6",
   "metadata": {},
   "source": [
    "## **Part C: Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43466a36",
   "metadata": {},
   "source": [
    "- Q-learning is an off-policy method (learns optimal policy regardless of actions taken)\n",
    "- Updates Q-values using maximum future reward\n",
    "- More robust for exploration strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6573aca5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     42\u001b[39m     reward_per_random_episode.append(episode_reward)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# ==================================================================================================\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# POLICY EVALUATION\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# ==================================================================================================\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Extract learned policy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m policy_qlearning = \u001b[43mget_policy\u001b[49m(Q)\n\u001b[32m     50\u001b[39m reward_per_learned_episode = []\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Test learned policy performance\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'get_policy' is not defined"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================\n",
    "# PART C: Q-LEARNING IMPLEMENTATION\n",
    "# ==================================================================================================\n",
    "# Environment setup for Q-learning\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "num_episodes = 1000\n",
    "alpha = 0.1\n",
    "gamma = 1.0\n",
    "\n",
    "num_states, num_actions = env.observation_space.n, env.action_space.n\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "reward_per_random_episode = []\n",
    "\n",
    "def update_q_table_qlearning(state, action, reward, new_state):\n",
    "    \"\"\"Q-learning update rule: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]\"\"\"\n",
    "    old_value = Q[state, action]\n",
    "    next_max = max(Q[new_state])  # Maximum Q-value for next state\n",
    "    Q[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# Q-LEARNING TRAINING LOOP\n",
    "# ==================================================================================================\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        # Random action selection for exploration\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Take action and observe new state and reward\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Q-learning update\n",
    "        update_q_table_qlearning(state, action, reward, new_state)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        state = new_state\n",
    "    \n",
    "    reward_per_random_episode.append(episode_reward)\n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# POLICY EVALUATION\n",
    "# ==================================================================================================\n",
    "# Extract learned policy\n",
    "policy_qlearning = get_policy(Q)\n",
    "reward_per_learned_episode = []\n",
    "\n",
    "# Test learned policy performance\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        # Select best action based on learned Q-table\n",
    "        action = policy_qlearning[state]\n",
    "        \n",
    "        # Take action and observe new state\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    reward_per_learned_episode.append(episode_reward)\n",
    "\n",
    "# Performance comparison\n",
    "avg_random_reward = np.mean(reward_per_random_episode)\n",
    "avg_learned_reward = np.mean(reward_per_learned_episode)\n",
    "\n",
    "print(f\"Average Random Policy Reward: {avg_random_reward:.3f}\")\n",
    "print(f\"Average Learned Policy Reward: {avg_learned_reward:.3f}\")\n",
    "print(f\"Performance Improvement: {(avg_learned_reward/avg_random_reward - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9914c",
   "metadata": {},
   "source": [
    "## **Key Differences Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb72ebc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Monte Carlo vs. TD Learning:**\n",
    "- **Monte Carlo**: Updates after complete episodes, requires episodic tasks\n",
    "- **TD Learning**: Updates at each step, works with continuing tasks\n",
    "\n",
    "**SARSA vs. Q-Learning:**\n",
    "- **SARSA**: On-policy, learns policy being followed\n",
    "- **Q-Learning**: Off-policy, learns optimal policy independent of behavior\n",
    "\n",
    "**Use Cases:**\n",
    "- **Monte Carlo**: Short episodic tasks with clear endpoints\n",
    "- **SARSA**: When you want to learn the policy you're actually following\n",
    "- **Q-Learning**: When you want to find the optimal policy regardless of exploration strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
