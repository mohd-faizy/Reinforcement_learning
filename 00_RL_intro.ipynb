{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607eeb4b",
   "metadata": {},
   "source": [
    "\n",
    "# **‚ú®Fundamentals of Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce77efd",
   "metadata": {},
   "source": [
    "## **üìëTable of Contents**\n",
    "\n",
    "1. [Introduction to Reinforcement Learning](#1-introduction-to-reinforcement-learning)\n",
    "   - 1.1 [Core Concepts and Trial-and-Error Learning](#11-core-concepts-and-trial-and-error-learning)\n",
    "   - 1.2 [Reward and Penalty System](#12-reward-and-penalty-system)\n",
    "   - 1.3 [RL vs. Other Machine Learning Types](#13-rl-vs-other-machine-learning-types)\n",
    "   - 1.4 [When to Use Reinforcement Learning](#14-when-to-use-reinforcement-learning)\n",
    "   - 1.5 [Real-World Applications](#15-real-world-applications)\n",
    "\n",
    "2. [Navigating the RL Framework](#2-navigating-the-rl-framework)\n",
    "   - 2.1 [Core Components](#21-core-components)\n",
    "   - 2.2 [The RL Interaction Loop](#22-the-rl-interaction-loop)\n",
    "   - 2.3 [Task Types: Episodic vs. Continuous](#23-task-types-episodic-vs-continuous)\n",
    "   - 2.4 [Return and Reward Optimization](#24-return-and-reward-optimization)\n",
    "   - 2.5 [Discounted Return and Time Value](#25-discounted-return-and-time-value)\n",
    "\n",
    "3. [Interacting with Gymnasium Environments](#3-interacting-with-gymnasium-environments)\n",
    "   - 3.1 [Introduction to Gymnasium](#31-introduction-to-gymnasium)\n",
    "   - 3.2 [Key Gymnasium Environments](#32-key-gymnasium-environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d267845",
   "metadata": {},
   "source": [
    "## **üîñ1. Introduction to Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e343fdf",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "By the end of this section, you should understand:\n",
    "\n",
    "- The fundamental concept of trial-and-error learning in RL\n",
    "- How RL differs from supervised and unsupervised learning\n",
    "- When RL is the appropriate machine learning approach\n",
    "- Real-world applications where RL excels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5e84d",
   "metadata": {},
   "source": [
    "### 1.1 Core Concepts and Trial-and-Error Learning\n",
    "\n",
    "> \"Reinforcement learning: Agent learns through trial and error\"\n",
    "\n",
    "**Enhanced Explanation:**\n",
    "\n",
    "**Reinforcement Learning (RL)** is a machine learning paradigm where an intelligent agent learns optimal behavior through direct interaction with an environment, using a trial-and-error approach combined with delayed rewards or penalties.\n",
    "\n",
    "**Core Principle:** Unlike traditional machine learning approaches that learn from pre-labeled datasets, RL agents discover optimal strategies by:\n",
    "1. **Trying different actions** in various situations\n",
    "2. **Observing the consequences** of those actions\n",
    "3. **Learning from feedback** to improve future decisions\n",
    "4. **Gradually developing expertise** through experience\n",
    "\n",
    "**Why Trial-and-Error Works:**\n",
    "- **Exploration:** Agents try new actions to discover potentially better strategies\n",
    "- **Exploitation:** Agents use known good actions to maximize rewards\n",
    "- **Balance:** Successful RL requires balancing exploration of new possibilities with exploitation of known good strategies\n",
    "\n",
    "**Real-World Analogy:** Consider learning to ride a bicycle. You don't study a manual and immediately know how to balance‚Äîyou get on the bike, try to pedal, fall down (negative feedback), adjust your balance, try again, gradually get better, and eventually master the skill through repeated trial and error.\n",
    "\n",
    "### 1.2 Reward and Penalty System\n",
    "\n",
    "> \"Agent receives: Rewards for good decisions, Penalties for bad decisions. Goal: maximize positive feedback over time\"\n",
    "\n",
    "**Enhanced Explanation:**\n",
    "\n",
    "The **reward system** is the fundamental mechanism that drives learning in reinforcement learning. It provides the agent with feedback about the quality of its decisions.\n",
    "\n",
    "**Reward Structure:**\n",
    "- **Positive Rewards (+):** Given for actions that move the agent closer to its goal\n",
    "- **Negative Rewards/Penalties (-):** Given for actions that are detrimental or move away from the goal\n",
    "- **Zero Rewards (0):** Given for neutral actions that neither help nor hinder progress\n",
    "\n",
    "**Goal Optimization:**\n",
    "The agent's primary objective is to **maximize cumulative reward over time**, not just immediate rewards. This creates the need for strategic thinking and long-term planning.\n",
    "\n",
    "**Key Characteristics:**\n",
    "1. **Delayed Gratification:** Sometimes immediate sacrifices lead to greater long-term rewards\n",
    "2. **Sparse Rewards:** In many environments, rewards are infrequent, making learning challenging\n",
    "3. **Dense Rewards:** Frequent feedback can speed up learning but may lead to suboptimal local strategies\n",
    "\n",
    "**Practical Example - Training a Pet Analogy:**\n",
    "When training a dog to sit:\n",
    "- **Positive Reward:** Treat when the dog sits on command\n",
    "- **Negative Feedback:** No treat or verbal \"no\" when the dog doesn't sit\n",
    "- **Learning Process:** Dog learns to associate sitting with positive outcomes\n",
    "- **Long-term Goal:** Reliable obedience and better behavior\n",
    "\n",
    "### 1.3 RL vs. Other Machine Learning Types\n",
    "\n",
    "> \"RL vs. other ML types\" (referenced in slides but details not fully provided in PDF)\n",
    "\n",
    "**Comprehensive Comparison:**\n",
    "\n",
    "| Aspect | Supervised Learning | Unsupervised Learning | Reinforcement Learning |\n",
    "|--------|-------------------|---------------------|----------------------|\n",
    "| **Data Type** | Labeled input-output pairs | Unlabeled data | Interactive environment |\n",
    "| **Learning Method** | Pattern recognition from examples | Hidden structure discovery | Trial-and-error with feedback |\n",
    "| **Feedback** | Immediate correct answers | No explicit feedback | Delayed rewards/penalties |\n",
    "| **Goal** | Predict outputs for new inputs | Find patterns or clusters | Maximize cumulative reward |\n",
    "| **Decision Making** | Single predictions | Data analysis insights | Sequential decision making |\n",
    "| **Time Dependency** | Usually independent samples | Usually independent samples | Time-dependent sequences |\n",
    "\n",
    "**When Each Approach Excels:**\n",
    "\n",
    "**Supervised Learning:** \n",
    "- Image classification\n",
    "- Spam email detection  \n",
    "- Medical diagnosis from symptoms\n",
    "\n",
    "**Unsupervised Learning:**\n",
    "- Customer segmentation\n",
    "- Anomaly detection\n",
    "- Data compression\n",
    "\n",
    "**Reinforcement Learning:**\n",
    "- Game playing\n",
    "- Robot control\n",
    "- Resource allocation over time\n",
    "\n",
    "**Key Distinguishing Features of RL:**\n",
    "1. **Sequential Decision Making:** Each action affects future states\n",
    "2. **No Direct Supervision:** No \"correct\" answers provided upfront\n",
    "3. **Exploration vs. Exploitation:** Must balance trying new things vs. using known good strategies\n",
    "4. **Temporal Credit Assignment:** Must determine which past actions led to current rewards\n",
    "\n",
    "### 1.4 When to Use Reinforcement Learning\n",
    "\n",
    "> \"When to use RL? Sequential decision-making, Decisions influence future observations, Learning through rewards and penalties, No direct supervision\"\n",
    "\n",
    "**Detailed Decision Framework:**\n",
    "\n",
    "**Use RL When You Have:**\n",
    "\n",
    "1. **Sequential Decision-Making Problems**\n",
    "   - Decisions are made over multiple time steps\n",
    "   - Current decisions affect future options\n",
    "   - **Example:** Chess moves affect board state and available future moves\n",
    "\n",
    "2. **Dynamic Environments**\n",
    "   - Environment changes based on agent actions\n",
    "   - Future observations depend on past decisions\n",
    "   - **Example:** Stock trading where each trade affects market conditions\n",
    "\n",
    "3. **Reward-Based Learning Scenarios**\n",
    "   - Clear objective function (maximize/minimize something)\n",
    "   - Feedback comes as rewards or penalties\n",
    "   - **Example:** Optimizing energy consumption in smart buildings\n",
    "\n",
    "4. **No Direct Supervision Available**\n",
    "   - No pre-labeled \"correct\" examples\n",
    "   - Must learn through interaction\n",
    "   - **Example:** Discovering new game strategies\n",
    "\n",
    "**Specific Examples from PDF:**\n",
    "\n",
    "**Appropriate for RL: Playing Video Games**\n",
    "- **Sequential decisions:** Each move affects game state\n",
    "- **Rewards/penalties:** Points gained/lost, lives lost\n",
    "- **No supervision:** Must discover winning strategies through play\n",
    "- **Dynamic environment:** Game responds to player actions\n",
    "\n",
    "**Inappropriate for RL: In-Game Object Recognition**\n",
    "- **No sequential decisions:** Single classification task\n",
    "- **No environment interaction:** Static image analysis\n",
    "- **Better suited for:** Supervised learning with labeled image datasets\n",
    "\n",
    "**Decision Checklist:**\n",
    "Use RL if you can answer \"yes\" to these questions:\n",
    "- [ ] Do actions have long-term consequences?\n",
    "- [ ] Is there a clear reward/penalty signal?\n",
    "- [ ] Must the system learn through interaction?\n",
    "- [ ] Are decisions made sequentially over time?\n",
    "- [ ] Does the environment respond to actions?\n",
    "\n",
    "### 1.5 Real-World Applications\n",
    "\n",
    "> \"RL applications: Robotics (Robot walking, Object manipulation), Finance (Optimizing trading and investment, Maximize profit), Autonomous Vehicles (Enhancing safety and efficiency, Minimizing accident risks), Chatbot development (Enhancing conversational skills, Improving user experiences)\"\n",
    "\n",
    "**Expanded Applications with Context:**\n",
    "\n",
    "#### Robotics\n",
    "**Applications:**\n",
    "- **Robot Walking:** Learning bipedal locomotion through trial and error\n",
    "- **Object Manipulation:** Grasping and manipulating objects with robotic arms\n",
    "\n",
    "**Why RL Works Here:**\n",
    "- **Complex Physics:** Difficult to model mathematically\n",
    "- **Real-time Feedback:** Immediate consequences (falling, dropping objects)\n",
    "- **Adaptability:** Must work in various environments\n",
    "- **Safety Learning:** Avoiding actions that damage equipment\n",
    "\n",
    "**Technical Challenges:**\n",
    "- Sample efficiency (learning quickly to avoid wear)\n",
    "- Safety constraints (avoiding dangerous actions)\n",
    "- Transfer learning (skills learned in simulation to real world)\n",
    "\n",
    "#### Finance\n",
    "**Applications:**\n",
    "- **Algorithmic Trading:** Optimizing buy/sell decisions over time\n",
    "- **Portfolio Management:** Balancing risk and return\n",
    "- **Risk Assessment:** Dynamic adjustment of investment strategies\n",
    "\n",
    "**Why RL Works Here:**\n",
    "- **Sequential Impact:** Each trade affects future market conditions\n",
    "- **Clear Objectives:** Maximize profit, minimize risk\n",
    "- **Dynamic Environment:** Markets constantly change\n",
    "- **Multi-objective Optimization:** Balance multiple goals simultaneously\n",
    "\n",
    "**Unique Considerations:**\n",
    "- **Non-stationarity:** Market conditions change over time\n",
    "- **Risk Management:** Must consider potential losses\n",
    "- **Regulatory Compliance:** Must operate within legal constraints\n",
    "\n",
    "#### Autonomous Vehicles\n",
    "**Applications:**\n",
    "- **Path Planning:** Navigating through traffic safely and efficiently\n",
    "- **Speed Control:** Optimizing fuel efficiency while meeting time constraints\n",
    "- **Emergency Response:** Learning appropriate reactions to unexpected situations\n",
    "\n",
    "**Why RL Works Here:**\n",
    "- **Safety Critical:** Must minimize accident risk\n",
    "- **Multi-objective:** Balance safety, efficiency, and passenger comfort\n",
    "- **Complex Environment:** Interactions with other vehicles and pedestrians\n",
    "- **Real-time Decisions:** Must act quickly in dynamic situations\n",
    "\n",
    "**Technical Requirements:**\n",
    "- **High Safety Standards:** Cannot learn through dangerous mistakes\n",
    "- **Simulation Training:** Most learning must happen in simulated environments\n",
    "- **Robustness:** Must handle edge cases and unusual scenarios\n",
    "\n",
    "#### Chatbot Development\n",
    "**Applications:**\n",
    "- **Dialogue Management:** Learning to maintain engaging conversations\n",
    "- **Personalization:** Adapting responses to individual users\n",
    "- **Goal Achievement:** Guiding conversations toward desired outcomes\n",
    "\n",
    "**Why RL Works Here:**\n",
    "- **Interactive Nature:** Each response affects conversation flow\n",
    "- **User Feedback:** Implicit feedback through engagement metrics\n",
    "- **Personalization:** Must adapt to different user preferences\n",
    "- **Long-term Engagement:** Building relationships over multiple conversations\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- **User Satisfaction:** Measured through ratings and continued usage\n",
    "- **Task Completion:** Successfully helping users achieve their goals\n",
    "- **Engagement Duration:** Length and frequency of conversations\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626cffe",
   "metadata": {},
   "source": [
    "## **üîñ2. Navigating the RL Framework**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a6248b",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "By the end of this section, you should understand:\n",
    "- The five core components of the RL framework\n",
    "- How agents and environments interact in the RL loop\n",
    "- The difference between episodic and continuous tasks\n",
    "- How return and discounting affect learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9b52f",
   "metadata": {},
   "source": [
    "### 2.1 Core Components\n",
    "\n",
    "> \"RL framework: Agent: learner, decision-maker; Environment: challenges to be solved; State: environment snapshot at given time; Action: agent's choice in response to state; Reward: feedback for agent action\"\n",
    "\n",
    "**Detailed Component Analysis:**\n",
    "\n",
    "#### The Agent\n",
    "**Definition:** The **agent** is the learner and decision-maker in the RL system.\n",
    "\n",
    "**Responsibilities:**\n",
    "- **Observe** the current state of the environment\n",
    "- **Select actions** based on its current policy/strategy\n",
    "- **Learn** from experience to improve future decisions\n",
    "- **Maintain** internal representations of value or policy\n",
    "\n",
    "**Types of Agents:**\n",
    "1. **Model-Free Agents:** Learn directly from experience without modeling environment\n",
    "2. **Model-Based Agents:** Build internal models of environment dynamics\n",
    "3. **Value-Based Agents:** Learn to estimate value of states or actions\n",
    "4. **Policy-Based Agents:** Directly learn optimal policies\n",
    "5. **Actor-Critic Agents:** Combine value estimation with policy learning\n",
    "\n",
    "**Internal Components:**\n",
    "- **Policy (œÄ):** Strategy for choosing actions\n",
    "- **Value Function (V):** Estimates of state or action values\n",
    "- **Model (optional):** Representation of environment dynamics\n",
    "\n",
    "#### The Environment\n",
    "**Definition:** The **environment** represents everything outside the agent that it must interact with.\n",
    "\n",
    "**Characteristics:**\n",
    "- **Provides states** to the agent\n",
    "- **Responds to actions** with new states and rewards\n",
    "- **Defines the rules** of the interaction\n",
    "- **May be stochastic** (random elements) or deterministic\n",
    "\n",
    "**Types of Environments:**\n",
    "1. **Fully Observable:** Agent sees complete state\n",
    "2. **Partially Observable:** Agent has limited state information\n",
    "3. **Deterministic:** Same action in same state always produces same result\n",
    "4. **Stochastic:** Randomness in transitions or rewards\n",
    "5. **Discrete:** Finite states and actions\n",
    "6. **Continuous:** Infinite states/actions (like real numbers)\n",
    "\n",
    "#### State\n",
    "**Definition:** A **state** is a complete snapshot of the environment at a specific time that contains all necessary information for decision-making.\n",
    "\n",
    "**State Properties:**\n",
    "- **Markov Property:** Current state contains all information needed for optimal decisions\n",
    "- **Completeness:** No relevant information is hidden\n",
    "- **Distinguishability:** Different states should lead to different optimal actions\n",
    "\n",
    "**State Representation Examples:**\n",
    "- **Chess:** Board position of all pieces\n",
    "- **Robot Navigation:** Position, orientation, sensor readings\n",
    "- **Trading:** Current prices, portfolio holdings, market indicators\n",
    "\n",
    "**State Space Considerations:**\n",
    "- **Size:** Number of possible states (affects learning complexity)\n",
    "- **Dimensionality:** Number of variables needed to represent state\n",
    "- **Continuous vs. Discrete:** Real-valued vs. finite set of states\n",
    "\n",
    "#### Action\n",
    "**Definition:** An **action** is the agent's choice in response to the current state.\n",
    "\n",
    "**Action Properties:**\n",
    "- **Available Actions:** May vary by state\n",
    "- **Consequences:** Actions change the environment state\n",
    "- **Irreversibility:** Some actions cannot be undone\n",
    "\n",
    "**Action Space Types:**\n",
    "1. **Discrete Actions:** Finite set of choices\n",
    "   - Example: {Move Left, Move Right, Jump, Shoot}\n",
    "2. **Continuous Actions:** Real-valued parameters\n",
    "   - Example: Steering angle in driving [-30¬∞, +30¬∞]\n",
    "3. **Multi-Discrete:** Multiple discrete choices simultaneously\n",
    "   - Example: {Direction} √ó {Speed} √ó {Special Action}\n",
    "\n",
    "#### Reward\n",
    "**Definition:** **Reward** is the scalar feedback signal that indicates how good the agent's action was.\n",
    "\n",
    "**Reward Design Principles:**\n",
    "1. **Alignment:** Rewards should align with desired behavior\n",
    "2. **Sparsity vs. Density:** Trade-off between information and noise\n",
    "3. **Scale:** Reward magnitude affects learning speed\n",
    "4. **Consistency:** Similar situations should give similar rewards\n",
    "\n",
    "**Common Reward Patterns:**\n",
    "- **Terminal Rewards:** Only given at end of episode (win/loss)\n",
    "- **Step Rewards:** Small rewards/penalties for each action\n",
    "- **Shaped Rewards:** Designed to guide learning toward goals\n",
    "- **Intrinsic Rewards:** Internal motivation (curiosity, exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e954fe",
   "metadata": {},
   "source": [
    "### 2.2 The RL Interaction Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2ccb8",
   "metadata": {},
   "source": [
    "```python\n",
    "env = create_environment() \n",
    "state = env.get_initial_state()  \n",
    " \n",
    "for i in range(n_iterations): \n",
    "    action = choose_action(state)  \n",
    "    state, reward = env.execute(action)  \n",
    "    update_knowledge(state, action, reward)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f54220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336 -0.20172954  0.03625453  0.32351476], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02323877 -0.00714208  0.04272482  0.04248186], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02309593 -0.20284982  0.04357446  0.34833285], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01903893 -0.00837385  0.05054112  0.06970263], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01887146 -0.2041826   0.05193517  0.37789345], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.0147878  -0.40000218  0.05949304  0.68648887], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.00678776 -0.5958973   0.07322282  0.9972924 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.00513019 -0.7919179   0.09316867  1.3120437 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.02096854 -0.98808783  0.11940954  1.6323743 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.0407303  -1.1843926   0.15205702  1.9597576 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.06441815 -1.3807645   0.19125217  2.2954495 ], Action: RIGHT, Reward: 1.0, Done: True\n",
      "Episode 1 finished. Total reward so far: 12.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336  0.18847767  0.03625453 -0.26141977], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.06945409  0.77262944 -0.02320687 -1.112592  ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.08490668  0.5778198  -0.0454587  -0.82727844], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.09646308  0.7735329  -0.06200428 -1.133905  ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.11193374  0.5792748  -0.08468238 -0.86129534], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.12351923  0.7754415  -0.10190828 -1.1793579 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.13902806  0.5817796  -0.12549543 -0.92028135], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.15066366  0.7783537  -0.14390106 -1.2496226 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.16623072  0.9749967  -0.16889352 -1.5836993 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.18573067  1.1716771  -0.20056751 -1.9239411 ], Action: RIGHT, Reward: 1.0, Done: True\n",
      "Episode 2 finished. Total reward so far: 27.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336 -0.20172954  0.03625453  0.32351476], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02323877 -0.00714208  0.04272482  0.04248186], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02309593  0.187342    0.04357446 -0.23642075], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02684277  0.3818152   0.03884605 -0.51504683], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03447907  0.18616833  0.02854511 -0.21038006], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03820244 -0.00934989  0.02433751  0.09116892], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03801544 -0.20481206  0.02616089  0.3914299 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.0339192  -0.40029535  0.03398949  0.6922449 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02591329 -0.595872    0.04783438  0.99543136], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01399586 -0.7915999   0.06774301  1.3027453 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.00183614 -0.98751265  0.09379791  1.6158417 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.0215864  -1.1836075   0.12611476  1.9362271 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.04525855 -1.3798318   0.1648393   2.2652054 ], Action: RIGHT, Reward: 1.0, Done: True\n",
      "Episode 3 finished. Total reward so far: 41.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336 -0.20172954  0.03625453  0.32351476], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02323877 -0.00714208  0.04272482  0.04248186], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02309593 -0.20284982  0.04357446  0.34833285], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01903893 -0.39856356  0.05054112  0.65443164], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01106766 -0.20418042  0.06362975  0.378082  ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.00698405 -0.01001712  0.07119139  0.10612098], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.00678371  0.18401624  0.07331381 -0.16327922], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01046404 -0.01207449  0.07004823  0.1516017 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01022255  0.18197817  0.07308026 -0.11818609], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01386211  0.37598106  0.07071654 -0.38694724], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02138173  0.17993027  0.06297759 -0.0728327 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02498034 -0.01603537  0.06152094  0.23903619], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02465963  0.17815623  0.06630167 -0.03362438], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02822275 -0.01785078  0.06562918  0.27921855], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02786574 -0.21384458  0.07121354  0.5918575 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02358885 -0.01978806  0.0830507   0.32242957], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02319308 -0.2159884   0.08949929  0.64010453], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01887332 -0.41223663  0.10230138  0.95957637], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01062858 -0.6085739   0.12149291  1.2825668 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-1.5428948e-03 -8.0501556e-01  1.4714424e-01  1.6106883e+00], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.01764321 -0.6119062   0.179358    1.3672589 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.02988133 -0.41942456  0.20670319  1.135617  ], Action: LEFT, Reward: 1.0, Done: True\n",
      "Episode 4 finished. Total reward so far: 64.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336 -0.20172954  0.03625453  0.32351476], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02323877 -0.00714208  0.04272482  0.04248186], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02309593  0.187342    0.04357446 -0.23642075], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02684277 -0.00837454  0.03884605  0.06968222], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02667528 -0.20403126  0.04023969  0.37436375], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02259465 -0.399701    0.04772697  0.6794581 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01460063 -0.20527339  0.06131613  0.40217513], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01049517 -0.401209    0.06935963  0.71354187], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.00247099 -0.20711237  0.08363046  0.44347236], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.00167126 -0.4033119   0.09249991  0.7613018 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.0097375  -0.20957771  0.10772595  0.49909905], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.01392905 -0.40604037  0.11770793  0.82369494], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.02204986 -0.60255885  0.13418183  1.1509591 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.03410104 -0.7991518   0.157201    1.482529  ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.05008407 -0.99580294  0.18685159  1.8198955 ], Action: LEFT, Reward: 1.0, Done: True\n",
      "Episode 5 finished. Total reward so far: 80.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336  0.18847767  0.03625453 -0.26141977], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03104291 -0.00714255  0.03102613  0.04247424], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01888104 -0.20369498  0.05171815  0.36711797], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01480714 -0.00934462  0.05906051  0.09118061], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01462025  0.18488325  0.06088412 -0.18229952], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01831791  0.37908354  0.05723813 -0.45517138], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02589958  0.57335144  0.0481347  -0.7292772 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03736661  0.37759838  0.03354916 -0.42184153], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.04491858  0.18201756  0.02511233 -0.1187738 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.04855893 -0.01345502  0.02273685  0.18172488], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.04828983 -0.2088948   0.02637135  0.48149285], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.04411193 -0.4043789   0.03600121  0.7823693 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.03602436 -0.20976976  0.05164859  0.5012268 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.03182896 -0.01541245  0.06167313  0.22525755], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.03152071  0.17877634  0.06617828 -0.04735143], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03509624 -0.01722907  0.06523126  0.26545534], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03475166 -0.21321847  0.07054036  0.57797855], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03048729 -0.4092545   0.08209993  0.89202297], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.0223022  -0.6053884   0.09994039  1.2093432 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01019443 -0.8016488   0.12412725  1.5315983 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.00583855 -0.6082224   0.15475923  1.2800908 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.01800299 -0.80494016  0.18036103  1.6169587 ], Action: RIGHT, Reward: 1.0, Done: True\n",
      "Episode 6 finished. Total reward so far: 105.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336 -0.20172954  0.03625453  0.32351476], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02323877 -0.00714208  0.04272482  0.04248186], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02309593  0.187342    0.04357446 -0.23642075], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02684277  0.3818152   0.03884605 -0.51504683], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.03447907  0.57636917  0.02854511 -0.79523975], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.04600646  0.38086733  0.01264031 -0.49371535], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.0536238   0.57580876  0.00276601 -0.782388  ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.06513998  0.77089256 -0.01288175 -1.0741993 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.08055783  0.57594323 -0.03436574 -0.7855867 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.0920767   0.77152    -0.05007747 -1.0888803 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.10750709  0.9672652  -0.07185508 -1.3968465 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.1268524   1.1632036  -0.09979201 -1.7111028 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.15011647  1.3593202  -0.13401407 -2.0331044 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.17730287  1.1658112  -0.17467615 -1.7847259 ], Action: RIGHT, Reward: 1.0, Done: True\n",
      "Episode 7 finished. Total reward so far: 120.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336 -0.20172954  0.03625453  0.32351476], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02323877 -0.00714208  0.04272482  0.04248186], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02309593  0.187342    0.04357446 -0.23642075], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02684277 -0.00837454  0.03884605  0.06968222], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02667528  0.18616958  0.04023969 -0.21049595], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.03039867  0.38069376  0.03602977 -0.49021873], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.03801255  0.5752894   0.0262254  -0.7713323 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.04951833  0.37981656  0.01079875 -0.4705145 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.05711466  0.18454377  0.00138846 -0.1744476 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.06080554  0.37964582 -0.00210049 -0.46669218], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.06839845  0.18455361 -0.01143434 -0.17467205], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.07208953  0.37983733 -0.01492778 -0.47094008], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.07968628  0.57516694 -0.02434658 -0.76829046], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.09118962  0.3803884  -0.03971239 -0.48336646], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.09879738  0.57604766 -0.04937972 -0.78829634], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.11031833  0.77181184 -0.06514564 -1.0960964 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.12575458  0.5776054  -0.08706757 -0.824544  ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.13730668  0.77380353 -0.10355845 -1.143292  ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.15278275  0.58017576 -0.12642428 -0.884798  ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.16438627  0.3869761  -0.14412025 -0.6343812 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.17212579  0.19412722 -0.15680787 -0.39033172], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.17600833  0.00153789 -0.16461451 -0.15090491], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.17603908  0.1985874  -0.16763261 -0.4906619 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.18001084  0.39562812 -0.17744584 -0.8311334 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.1879234   0.20331739 -0.1940685  -0.5990926 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.19198975  0.01136416 -0.20605037 -0.37326655], Action: RIGHT, Reward: 1.0, Done: True\n",
      "Episode 8 finished. Total reward so far: 147.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336 -0.20172954  0.03625453  0.32351476], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02323877 -0.39734846  0.04272482  0.62740684], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.0152918  -0.20284806  0.05527296  0.34847975], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01123484 -0.00855404  0.06224256  0.07372576], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01106376 -0.20451054  0.06371707  0.3853788 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.00697355 -0.40047646  0.07142465  0.69745135], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.00103598 -0.5965123   0.08537367  1.0117364 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.01296623 -0.4026268   0.1056084   0.74703676], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.02101876 -0.599035    0.12054913  1.0709989 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.03299946 -0.7955266   0.14196911  1.398953  ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.04891    -0.6024262   0.16994818  1.1538153 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.06095852 -0.79930705  0.19302447  1.4946065 ], Action: LEFT, Reward: 1.0, Done: True\n",
      "Episode 9 finished. Total reward so far: 160.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336  0.18847767  0.03625453 -0.26141977], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03104291 -0.00714255  0.03102613  0.04247424], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01888104 -0.593896    0.05171815  0.95197964], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.00700312 -0.39950672  0.07075775  0.67598397], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-9.870153e-04 -5.955368e-01  8.427742e-02  9.900788e-01], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.01289775 -0.79167956  0.104079    1.3079969 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.02873134 -0.987955    0.13023894  1.6313609 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.04849044 -1.1843442   0.16286615  1.9616318 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.07217733 -0.99127954  0.20209879  1.7235385 ], Action: LEFT, Reward: 1.0, Done: True\n",
      "Episode 10 finished. Total reward so far: 172.0\n",
      "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02727336 -0.20172954  0.03625453  0.32351476], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.02323877 -0.39734846  0.04272482  0.62740684], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.0152918  -0.20284806  0.05527296  0.34847975], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01123484 -0.00855404  0.06224256  0.07372576], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01106376  0.18562292  0.06371707 -0.19868816], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01477622 -0.01034977  0.05974331  0.11339449], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01456922  0.1838675   0.0620112  -0.15985757], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01824657 -0.01208489  0.05881405  0.15172568], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01800487 -0.20799753  0.06184856  0.4623679 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.01384492 -0.40393656  0.07109591  0.7738863 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.00576619 -0.5999608   0.08657365  1.0880646 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.00623302 -0.40608045  0.10833494  0.823754  ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.01435463 -0.21259399  0.12481002  0.567014  ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.01860651 -0.01942345  0.1361503   0.31611106], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.01899498  0.17352308  0.14247252  0.06927422], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.01552452 -0.02332353  0.143858    0.4032978 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [-0.01599099 -0.22016151  0.15192395  0.7376534 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.02039422 -0.02742765  0.16667703  0.49637654], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.02094277  0.16500026  0.17660455  0.26051128], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.01764277  0.35721928  0.18181477  0.0283197 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [-0.01049838  0.54933137  0.18238117 -0.2019391 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 4.8824539e-04  7.4144018e-01  1.7834239e-01 -4.3199965e-01], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.01531705  0.933648    0.1697024  -0.6635834 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.03399001  0.7366225   0.15643074 -0.32263714], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.04872246  0.9292112   0.14997798 -0.5621895 ], Action: RIGHT, Reward: 1.0, Done: False\n",
      "State: [ 0.06730668  1.1219456   0.13873419 -0.8041174 ], Action: LEFT, Reward: 1.0, Done: False\n",
      "State: [ 0.0897456   0.9252219   0.12265185 -0.47120997], Action: LEFT, Reward: 1.0, Done: False\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class RLInteractionLoop:\n",
    "    \"\"\"Simple RL interaction loop for CartPole environment.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = None\n",
    "        self.episode_count = 0\n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def create_environment(self):\n",
    "        \"\"\"Create CartPole environment.\"\"\"\n",
    "        self.env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "        return self.env\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        \"\"\"Reset environment and return initial state.\"\"\"\n",
    "        state, info = self.env.reset(seed=42)\n",
    "        return state\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose random action (replace with your policy later).\"\"\"\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def execute(self, action):\n",
    "        \"\"\"Execute action and return next state, reward, and done flag.\"\"\"\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.total_reward += reward  # Track total reward\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def update_knowledge(self, state, action, reward, done):\n",
    "        \"\"\"Update agent knowledge (replace with actual learning algorithm).\"\"\"\n",
    "        if reward > 0:\n",
    "            action_name = \"LEFT\" if action == 0 else \"RIGHT\"\n",
    "            print(f\"State: {state}, Action: {action_name}, Reward: {reward}, Done: {done}\")\n",
    "    \n",
    "    def episode_finished(self):\n",
    "        \"\"\"Called when episode ends.\"\"\"\n",
    "        self.episode_count += 1\n",
    "        print(f\"Episode {self.episode_count} finished. Total reward so far: {self.total_reward}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close environment.\"\"\"\n",
    "        if self.env:\n",
    "            self.env.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "rl_loop = RLInteractionLoop()\n",
    "\n",
    "# Setup\n",
    "env = rl_loop.create_environment()\n",
    "state = rl_loop.get_initial_state()\n",
    "\n",
    "# Training loop\n",
    "n_iterations = 200\n",
    "for i in range(n_iterations):\n",
    "    action = rl_loop.choose_action(state)\n",
    "    next_state, reward, done = rl_loop.execute(action)\n",
    "    rl_loop.update_knowledge(state, action, reward, done)\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    # Reset if episode ended\n",
    "    if done:\n",
    "        rl_loop.episode_finished()  # Simple progress update\n",
    "        state = rl_loop.get_initial_state()\n",
    "\n",
    "rl_loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1ebe0",
   "metadata": {},
   "source": [
    "**State Vector Breakdown**\n",
    "\n",
    "```python\n",
    "State: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
    "         ‚Üë           ‚Üë            ‚Üë           ‚Üë\n",
    "      [  0  ]    [    1   ]   [    2   ]  [   3   ]\n",
    "```\n",
    "\n",
    "| Index | Value | Meaning | Your Example |\n",
    "|-------|-------|---------|--------------|\n",
    "| **0** | Cart Position | Horizontal position of cart on track (meters) | `0.0273956` |\n",
    "| **1** | Cart Velocity | Speed of cart movement (m/s) | `-0.00611216` |\n",
    "| **2** | Pole Angle | Angle of pole from vertical (radians) | `0.03585979` |\n",
    "| **3** | Pole Angular Velocity | How fast pole is rotating (rad/s) | `0.0197368` |\n",
    "\n",
    "**What Your Values Mean:**\n",
    "\n",
    "- **Cart Position (0.027m)**: Cart is slightly right of center\n",
    "- **Cart Velocity (-0.006 m/s)**: Cart moving slowly leftward  \n",
    "- **Pole Angle (0.036 rad ‚âà 2¬∞)**: Pole tilted slightly to the right\n",
    "- **Pole Angular Velocity (0.020 rad/s)**: Pole rotating slowly clockwise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ebe40d",
   "metadata": {},
   "source": [
    "### 2.3 Task Types: Episodic vs. Continuous\n",
    "\n",
    "> Episodic tasks: Tasks segmented in episodes, Episode has beginning and end, Example: agent playing chess.\n",
    "\n",
    "> Continuous tasks: Continuous interaction, No distinct episodes, Example: Adjusting traffic lights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbec5ea",
   "metadata": {},
   "source": [
    "#### **‚≠êEpisodic Tasks**\n",
    "\n",
    "**Definition:** Tasks that have a natural beginning and end, with clear episode boundaries.\n",
    "\n",
    "**Characteristics:**\n",
    "- **Natural Termination:** Episodes end due to success, failure, or time limits\n",
    "- **Reset Mechanism:** Environment resets to initial or random state after each episode\n",
    "- **Independent Episodes:** Each episode is typically independent of previous ones\n",
    "- **Clear Evaluation:** Performance can be measured per episode\n",
    "\n",
    "**Common Examples:**\n",
    "1. **Board Games:** Chess, Go, checkers\n",
    "   - Start: Initial board position\n",
    "   - End: Checkmate, stalemate, or resignation\n",
    "   - Reset: New game with starting position\n",
    "\n",
    "2. **Video Games:** Mario, Pac-Man, racing games\n",
    "   - Start: Level beginning or spawn point\n",
    "   - End: Death, level completion, or time out\n",
    "   - Reset: Restart level or respawn\n",
    "\n",
    "3. **Robotic Tasks:** Pick and place, maze navigation\n",
    "   - Start: Robot at starting position\n",
    "   - End: Task completion or failure\n",
    "   - Reset: Robot returns to starting position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "947aa4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, State: [-0.17671879 -0.61768353  0.22881842  1.0667348 ], Total reward: 11.0 \n",
      "Episode: 2, State: [ 1.3186075e-01 -4.4112164e-04 -2.2313654e-01 -4.8384139e-01], Total reward: 18.0 \n",
      "Episode: 3, State: [ 0.15159626  0.9547194  -0.21405289 -1.6652651 ], Total reward: 11.0 \n",
      "Episode: 4, State: [ 0.24333732  1.0144546  -0.23884887 -1.5226989 ], Total reward: 35.0 \n",
      "Episode: 5, State: [-0.05319475 -0.6254332   0.22621156  1.3721168 ], Total reward: 19.0 \n",
      "Episode: 6, State: [-0.12074924 -0.81174415  0.21800552  1.5132087 ], Total reward: 10.0 \n",
      "Episode: 7, State: [-0.11288629 -0.16849484  0.21248294  0.67522573], Total reward: 13.0 \n",
      "Episode: 8, State: [-0.0907068 -0.7832927  0.2270286  1.519882 ], Total reward: 10.0 \n",
      "Episode: 9, State: [-0.1229967  -0.05857149  0.21505335  0.5714924 ], Total reward: 16.0 \n",
      "Episode: 10, State: [-0.1553902  -0.9386495   0.23611815  1.8174309 ], Total reward: 13.0 \n",
      "Episode: 11, State: [ 0.35596266 -0.02507479  0.22495376  1.3770994 ], Total reward: 62.0 \n",
      "Episode: 12, State: [-0.35516492 -2.4926999   0.21668987  2.8764455 ], Total reward: 33.0 \n",
      "Episode: 13, State: [-0.11801063 -0.5591551   0.21832201  1.3814242 ], Total reward: 23.0 \n",
      "Episode: 14, State: [ 0.2228941   1.4065907  -0.22918543 -2.113746  ], Total reward: 15.0 \n",
      "Episode: 15, State: [-0.17199458 -0.18306856  0.21149552  0.60935163], Total reward: 15.0 \n",
      "Episode: 16, State: [ 0.09396736  1.1524465  -0.24111575 -2.1043215 ], Total reward: 12.0 \n",
      "Episode: 17, State: [-0.05759249 -0.7845021   0.21763834  1.514331  ], Total reward: 26.0 \n",
      "Episode: 18, State: [-0.04594554  0.03891141 -0.22234274 -0.91389537], Total reward: 28.0 \n",
      "Episode: 19, State: [-0.12392589 -1.1616943   0.24218684  2.0725014 ], Total reward: 10.0 \n",
      "Episode: 20, State: [ 0.07312275  0.78103876 -0.21617882 -1.5581771 ], Total reward: 20.0 \n",
      "Episode: 21, State: [ 0.27930376  1.3998525  -0.23283729 -2.0004473 ], Total reward: 33.0 \n",
      "Episode: 22, State: [0.12423883 0.68356246 0.21163864 0.28114447], Total reward: 36.0 \n",
      "Episode: 23, State: [ 0.325616    1.3431647  -0.23373948 -1.5551308 ], Total reward: 33.0 \n",
      "Episode: 24, State: [ 0.11092132 -0.6541716   0.22275898  1.9258394 ], Total reward: 63.0 \n",
      "Episode: 25, State: [ 0.11914185  1.4135247  -0.22160523 -2.2195585 ], Total reward: 11.0 \n",
      "Episode: 26, State: [ 0.20431165  1.3589646  -0.24078563 -2.0933433 ], Total reward: 23.0 \n",
      "Episode: 27, State: [-0.01928605 -0.068992    0.22495179  0.8114401 ], Total reward: 50.0 \n",
      "Episode: 28, State: [-0.08988673 -0.45096245  0.223195    0.9580208 ], Total reward: 14.0 \n",
      "Episode: 29, State: [ 0.22423032 -0.1363017  -0.21042767 -0.0614439 ], Total reward: 51.0 \n",
      "Episode: 30, State: [-0.17986347 -0.6019279   0.23595229  1.2732662 ], Total reward: 13.0 \n",
      "Episode: 31, State: [ 0.14686596  1.2141846  -0.22755927 -2.1461833 ], Total reward: 12.0 \n",
      "Episode: 32, State: [ 0.21376151  1.1570498  -0.22954802 -1.9370066 ], Total reward: 12.0 \n",
      "Episode: 33, State: [-0.13996722 -1.1443875   0.2123756   2.0829792 ], Total reward: 16.0 \n",
      "Episode: 34, State: [-0.27806866 -1.0180116   0.22514081  1.5044793 ], Total reward: 37.0 \n",
      "Episode: 35, State: [-0.22405843 -1.4086026   0.24935745  2.1754034 ], Total reward: 27.0 \n",
      "Episode: 36, State: [-0.16108479 -0.19255424  0.22628987  0.693141  ], Total reward: 17.0 \n",
      "Episode: 37, State: [ 0.05570467  0.9455413  -0.21493164 -1.7117923 ], Total reward: 19.0 \n",
      "Episode: 38, State: [ 0.14152949  0.3552101  -0.22055042 -0.8946361 ], Total reward: 14.0 \n",
      "Episode: 39, State: [ 0.19103867  1.3390566  -0.24498434 -2.266563  ], Total reward: 11.0 \n",
      "Episode: 40, State: [-0.1346647  -0.94903135  0.23345196  1.6837493 ], Total reward: 13.0 \n",
      "Episode: 41, State: [-0.12654293 -1.1383609   0.2285348   2.1164565 ], Total reward: 14.0 \n",
      "Episode: 42, State: [-0.33306897 -1.9065458   0.21162665  1.9496742 ], Total reward: 40.0 \n",
      "Episode: 43, State: [-0.13954683 -0.38505113  0.22570232  0.9223709 ], Total reward: 42.0 \n",
      "Episode: 44, State: [ 0.02571795  0.57566506 -0.22140329 -1.4954257 ], Total reward: 27.0 \n",
      "Episode: 45, State: [ 0.17293353  0.8192887  -0.2110843  -1.4515109 ], Total reward: 14.0 \n",
      "Episode: 46, State: [ 0.02279467  0.6037539  -0.22412516 -1.5489222 ], Total reward: 23.0 \n",
      "Episode: 47, State: [-0.16121547 -1.1596905   0.239101    1.9513154 ], Total reward: 14.0 \n",
      "Episode: 48, State: [ 0.1380621   1.7745906  -0.24452302 -2.8124614 ], Total reward: 9.0 \n",
      "Episode: 49, State: [ 0.16441523  0.62150717 -0.22312866 -1.2228881 ], Total reward: 19.0 \n",
      "Episode: 50, State: [-0.17815292 -0.99239016  0.21499951  1.6604462 ], Total reward: 15.0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "What This Code Does\n",
    "- CartPole Environment: The agent tries to balance a pole on a cart by moving left or right\n",
    "- Random Actions: Since the agent chooses random actions, it won't learn or improve\n",
    "- Episode Structure: Each episode runs until the pole falls or time limit is reached\n",
    "- Placeholder Learning: The `update()` method is empty, so no actual learning occurs\n",
    "'''\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Simple Agent class (from PDF concepts)\n",
    "class SimpleAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose random action (In a real implementation, this would use a trained policy)\"\"\"\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Learn from experience (Currently empty [placeholder for actual learning algorithms like Q-learning])\"\"\"\n",
    "        pass\n",
    "\n",
    "'''\n",
    "For each episode:\n",
    "1. Reset environment - `env.reset()` starts a fresh episode\n",
    "2. Initialize tracking - Set total reward to 0 and done flag to False\n",
    "3. Run episode steps:\n",
    "    - Agent chooses an action based on current state\n",
    "    - Environment executes the action via `env.step(action)`\n",
    "    - Returns: next state, reward, termination flags, and info\n",
    "    - Agent learns from this experience (currently does nothing)\n",
    "    - Update state and accumulate rewards\n",
    "    - Continue until episode ends (`done = True`)\n",
    "4. Print results - Shows episode number, final state, and total reward\n",
    "\n",
    "'''\n",
    "# Your training function\n",
    "def episodic_training_loop(env, agent, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()  # Start new episode\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Learn from this experience\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        print(f\"Episode: {episode + 1}, State: {next_state}, Total reward: {total_reward} \")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create CartPole environment (from PDF)\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # Create simple agent\n",
    "    agent = SimpleAgent(env.action_space)\n",
    "    \n",
    "    # Run training\n",
    "    episodic_training_loop(env, agent, num_episodes=50)\n",
    "    \n",
    "    # Clean up\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88da643",
   "metadata": {},
   "source": [
    "#### **‚≠êContinuous Tasks**\n",
    "\n",
    "**Definition:** Tasks with ongoing interaction that doesn't naturally segment into distinct episodes.\n",
    "\n",
    "**Characteristics:**\n",
    "- **No Natural Termination:** Could theoretically run forever\n",
    "- **Ongoing State:** System state persists and evolves continuously\n",
    "- **Contextual Dependence:** Current state depends on entire history\n",
    "- **Long-term Optimization:** Must consider very long-term consequences\n",
    "\n",
    "**Common Examples:**\n",
    "1. **Traffic Light Control:**\n",
    "   - **Continuous Operation:** Lights must always be functioning\n",
    "   - **Evolving Conditions:** Traffic patterns change throughout day\n",
    "   - **No Reset:** System runs continuously 24/7\n",
    "\n",
    "2. **Stock Trading:**\n",
    "   - **Market Hours:** Continuous during trading sessions\n",
    "   - **Portfolio Persistence:** Holdings carry over between decisions\n",
    "   - **Long-term Strategy:** Decisions affect future market positions\n",
    "\n",
    "3. **Climate Control:**\n",
    "   - **Always Active:** HVAC systems run continuously\n",
    "   - **Environmental Changes:** Temperature, occupancy vary continuously\n",
    "   - **Efficiency Goals:** Long-term energy optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb8f12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting continuous training (no episodes)...\n",
      "Agent learns through continuous interaction!\n",
      "Step 0: Total reward = 1.0, Avg reward = 1.000\n",
      "Step 1000: Total reward = 1001.0, Avg reward = 1.000\n",
      "Step 2000: Total reward = 2001.0, Avg reward = 1.000\n",
      "Step 3000: Total reward = 3001.0, Avg reward = 1.000\n",
      "Step 4000: Total reward = 4001.0, Avg reward = 1.000\n",
      "\n",
      "Training completed!\n",
      "Step 5000: Total reward = 5000.0, Avg reward = 1.000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "What This Code Does\n",
    "- CartPole Environment: The agent tries to balance a pole on a cart by moving left or right\n",
    "- Continuous Learning: Unlike episodic learning, this runs for a fixed number of steps without episode boundaries\n",
    "- Random Actions: Since the agent chooses random actions, it won't learn or improve (just tracks performance)\n",
    "- Automatic Resets: When pole falls or time limit reached, environment resets automatically but training continues\n",
    "- Performance Tracking: Agent tracks total rewards and step count for continuous evaluation\n",
    "'''\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# Simple Agent class (from PDF concepts)\n",
    "class SimpleAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.total_reward = 0  # Track cumulative reward across all interactions\n",
    "        self.step_count = 0    # Count total steps taken for averaging\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose random action (In a real implementation, this would use a trained policy)\"\"\"\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Learn from experience continuously (Currently just tracks performance metrics)\"\"\"\n",
    "        self.total_reward += reward\n",
    "        self.step_count += 1\n",
    "\n",
    "\n",
    "# Evaluation function for continuous learning\n",
    "def evaluate_performance(agent, step):\n",
    "    \"\"\"Periodic evaluation without stopping training (calculates and displays average performance)\"\"\"\n",
    "    if agent.step_count > 0:\n",
    "        avg_reward = agent.total_reward / agent.step_count\n",
    "        print(f\"Step {step}: Total reward = {agent.total_reward:.1f}, Avg reward = {avg_reward:.3f}\")\n",
    "\n",
    "\n",
    "'''\n",
    "Continuous Training Process:\n",
    "1. Initialize environment once at the start\n",
    "2. Run for fixed number of steps (no episode boundaries):\n",
    "    - Agent chooses action based on current state\n",
    "    - Environment executes action and returns feedback\n",
    "    - Agent updates its performance tracking (no actual learning)\n",
    "    - If environment terminates/truncates, reset automatically but continue training\n",
    "    - Update state for next step\n",
    "    - Every 1000 steps, evaluate and print performance metrics\n",
    "3. Final evaluation after all steps completed\n",
    "\n",
    "Key Difference from Episodic:\n",
    "- No episode structure - continuous interaction\n",
    "- Environment resets handled internally\n",
    "- Performance measured across entire training period\n",
    "- Evaluation happens at regular intervals during training\n",
    "'''\n",
    "\n",
    "# Your continuous training function\n",
    "def continuous_training_loop(env, agent, total_steps):\n",
    "    state, _ = env.reset()  # Initialize environment once\n",
    "    \n",
    "    for step in range(total_steps):\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        # Learn continuously (currently just performance tracking)\n",
    "        agent.update(state, action, reward, next_state, False)\n",
    "        \n",
    "        # Handle environment resets (for simulation purposes) - training continues seamlessly\n",
    "        if terminated or truncated:\n",
    "            next_state, _ = env.reset()\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # Periodic evaluation without stopping (every 1000 steps)\n",
    "        if step % 1000 == 0:\n",
    "            evaluate_performance(agent, step)\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create CartPole environment (continuous interaction)\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # Create simple agent with performance tracking\n",
    "    agent = SimpleAgent(env.action_space)\n",
    "    \n",
    "    print(\"Starting continuous training (no episodes)...\")\n",
    "    print(\"Agent learns through continuous interaction!\")\n",
    "    \n",
    "    # Run continuous training for 5000 steps\n",
    "    continuous_training_loop(env, agent, total_steps=5000)\n",
    "    \n",
    "    # Final evaluation after training completion\n",
    "    print(f\"\\nTraining completed!\")\n",
    "    evaluate_performance(agent, 5000)\n",
    "    \n",
    "    # Clean up\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7d55f",
   "metadata": {},
   "source": [
    "**Key Differences Summary:**\n",
    "\n",
    "| Aspect | Episodic Tasks | Continuous Tasks |\n",
    "|--------|----------------|------------------|\n",
    "| **Duration** | Fixed or variable episodes | Indefinite |\n",
    "| **Reset** | Regular resets | No resets (or artificial ones) |\n",
    "| **Evaluation** | Per episode performance | Rolling window or periodic |\n",
    "| **Learning** | Episode-based updates | Continuous updates |\n",
    "| **Horizon** | Episode length | Infinite or very long |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd41971",
   "metadata": {},
   "source": [
    "### 2.4 Return and Reward Optimization\n",
    "\n",
    "- **‚≠êReturn:**\n",
    "  - Actions have long term consequences, \n",
    "  - Agent aims to maximize total reward over time. \n",
    "  - Return: sum of all expected rewards\n",
    "\n",
    "- **Mathematical Foundations**\n",
    "    - **The Concept of Return**\n",
    "      - **Definition:** The **return** (denoted as $G_t$) is the total `accumulation of rewards` from time step $t$ onward.\n",
    "      - **Mathematical Formulation:**\n",
    "        - Episodic tasks (finite horizon, ending at ùëá):\n",
    "          - $G_t = R_{t+1} + R_{t+2} + \\dots + R_T$\n",
    "\n",
    "        *Where:*\n",
    "        - $G_t$ = Return from time step `t`\n",
    "        - $R_{t+i}$ = Reward received at time step `t+i`\n",
    "        - $T$ = Terminal time step\n",
    "\n",
    "- Why Return Matters\n",
    "  - **Long-term Consequences:** Individual actions may have effects that extend far into the future.\n",
    "  - **Examples of Long-term Impact:**\n",
    "    - **Chess:** Sacrificing a piece now might lead to checkmate later\n",
    "    - **Investment:** Taking losses now might position for greater future gains  \n",
    "    - **Education:** Studying hard now enables better career opportunities later\n",
    "\n",
    ">**Strategic Thinking:** Agents must learn to make short-term sacrifices for long-term gains, similar to human decision-making in complex scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abfad342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [1, 6, 3]\n",
      "Discount factor (Œ≥): 0.9\n",
      "Discounts: [1.0, 0.9, 0.81]\n",
      "Discounted return: 8.83\n",
      "\n",
      "Returns for each time step:\n",
      "Return from step 0: 8.83\n",
      "Return from step 1: 8.70\n",
      "Return from step 2: 3.00\n"
     ]
    }
   ],
   "source": [
    "def calculate_returns(rewards, gamma):\n",
    "    \"\"\"Calculate returns for a sequence of rewards\"\"\"\n",
    "    returns = []\n",
    "    n = len(rewards)\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate return from time step t onward\n",
    "        G_t = 0\n",
    "        for k in range(t, n):\n",
    "            G_t += (gamma ** (k - t)) * rewards[k]\n",
    "        returns.append(G_t)\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# Sample Example \n",
    "rewards = [1, 6, 3]\n",
    "gamma = 0.9\n",
    "\n",
    "print(f\"Rewards: {rewards}\")\n",
    "print(f\"Discount factor (Œ≥): {gamma}\")\n",
    "\n",
    "# Calculate discounts as shown in PDF\n",
    "discounts = [gamma ** i for i in range(len(rewards))]\n",
    "print(f\"Discounts: {discounts}\")\n",
    "\n",
    "# Calculate discounted return (PDF method)\n",
    "discounted_return = sum(r * d for r, d in zip(rewards, discounts))\n",
    "print(f\"Discounted return: {discounted_return}\")\n",
    "\n",
    "print(\"\\nReturns for each time step:\")\n",
    "returns = calculate_returns(rewards, gamma)\n",
    "for t, G_t in enumerate(returns):\n",
    "    print(f\"Return from step {t}: {G_t:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de0251",
   "metadata": {},
   "source": [
    "### 2.5 Discounted Return and Time Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77908421",
   "metadata": {},
   "source": [
    "- **‚≠êDiscounted return:** \n",
    "  - Immediate rewards are more valuable than future ones\n",
    "  - Discounted return: gives more weight to nearer rewards\n",
    "  - Discount factor (Œ≥): discounts future rewards. Between zero and one, Balances immediate vs. long-term rewards \n",
    "  - **Lower value** ‚Üí *immediate gains*\n",
    "  - **Higher value** ‚Üí *long-term benefits*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d513190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discounts: [1. 0. 0.]\n",
      "The discounted return is 1.0\n",
      "------------------------------\n",
      "Discounts: [1.   0.5  0.25]\n",
      "The discounted return is 4.75\n",
      "------------------------------\n",
      "Discounts: [1.   0.9  0.81]\n",
      "The discounted return is 8.83\n",
      "------------------------------\n",
      "Discounts: [1.     0.99   0.9801]\n",
      "The discounted return is 9.880299999999998\n",
      "------------------------------\n",
      "Discounts: [1. 1. 1.]\n",
      "The discounted return is 10.0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "expected_rewards = np.array([1, 6, 3])  \n",
    "discount_factors = [0.0, 0.5, 0.9, 0.99, 1.0]   \n",
    "\n",
    "for discount_factor in discount_factors:\n",
    "    discounts = np.array([discount_factor ** i for i in range(len(expected_rewards))])  \n",
    "    print(f\"Discounts: {discounts}\") \n",
    "\n",
    "    discounted_return = np.sum(expected_rewards * discounts) \n",
    "    print(f\"The discounted return is {discounted_return}\")\n",
    "    print(\"-\"*30) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91174b",
   "metadata": {},
   "source": [
    "**Above Code demonstrates:**\n",
    "\n",
    "> How the discount factor ($\\gamma$) affects the calculation of the discounted return, which is the sum of future rewards weighted by how much future rewards are valued.\n",
    "\n",
    "- Each discount factor from 0 to 1 shows the agent‚Äôs preference for immediate vs. future rewards.\n",
    "- When the discount factor is 0, only the immediate reward counts (return = 1).\n",
    "- As the discount factor approaches 1, future rewards are valued more equally to immediate rewards, increasing the total discounted return.\n",
    "- This reflects the trade-off between short-term and long-term gains an agent considers in decision making.\n",
    "\n",
    "So, higher discount factors mean the agent cares more about future rewards, leading to a higher total expected return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d1cbc",
   "metadata": {},
   "source": [
    "#### Discounted Return Formula\n",
    "\n",
    "The **discounted return** is calculated as:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{T-t-1} R_T$$\n",
    "> we also include discounting (for infinite horizon or when future rewards are worth less)\n",
    "\n",
    "\n",
    "Or more generally (infinite horizon):\n",
    "\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "\n",
    "\n",
    "Where:  \n",
    "- $\\gamma$ (gamma) = discount factor, $0 \\leq \\gamma \\leq 1$\n",
    "- $\\gamma^k$ = discount applied to reward $k$ steps in the future  \n",
    "\n",
    "The range of $\\gamma$ $\\rightarrow$ [0, 1]\n",
    "- $\\gamma$ = 0 $\\rightarrow$ agent only cares about immediate reward.\n",
    "- $\\gamma$ = 1 $\\rightarrow$ agent values future rewards almost equally as present.\n",
    "\n",
    "#### ***Why Discounting Makes Sense***\n",
    "\n",
    "**Practical Reasons:**\n",
    "1. **Uncertainty:** Future rewards are less certain than immediate ones\n",
    "2. **Opportunity Cost:** Immediate rewards can be reinvested or utilized now\n",
    "3. **Computational Tractability:** Prevents infinite returns in continuous tasks\n",
    "4. **Psychological Realism:** Humans naturally discount future rewards\n",
    "\n",
    "> **Economic Parallel:** Similar to interest rates in finance - a dollar today is worth more than a dollar tomorrow due to investment opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67b9bbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_discounted_return(rewards, gamma):\n",
    "    \"\"\"Calculate discounted return.\"\"\"\n",
    "    rewards = np.array(rewards)\n",
    "    discounts = gamma ** np.arange(len(rewards))\n",
    "    return np.sum(rewards * discounts)\n",
    "\n",
    "# --- Example usage ---\n",
    "rewards = [1, 3, 6, 9, 12]\n",
    "gammas = np.linspace(0, 1, 21)\n",
    "\n",
    "# Calculate discounted returns\n",
    "returns = [calculate_discounted_return(rewards, g) for g in gammas]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(gammas, returns, \"bo-\", lw=2, markersize=4)\n",
    "plt.axhline(sum(rewards), color=\"r\", ls=\"--\", lw=2, label=f\"Maximum Return (Œ≥=1) = {sum(rewards)}\")\n",
    "\n",
    "\n",
    "plt.title(\"Effect of Œ≥ on Discounted Return\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Discount Factor (Œ≥)\")\n",
    "plt.ylabel(\"Discounted Return\")\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699463ab",
   "metadata": {},
   "source": [
    "**Key Insights from Discount Factor Analysis:**\n",
    "\n",
    "1. **Œ≥ = 0:** Only immediate reward matters (purely greedy)\n",
    "2. **Œ≥ = 0.5:** Strong preference for near-term rewards\n",
    "3. **Œ≥ = 0.9:** Balanced approach (common in practice)\n",
    "4. **Œ≥ = 0.99:** Nearly equal weighting of all future rewards\n",
    "5. **Œ≥ = 1.0:** All rewards weighted equally (undiscounted)\n",
    "\n",
    "**Practical Guidelines for Choosing Œ≥:**\n",
    "- **Short-term tasks:** Use lower `Œ≥` (0.5-0.8)\n",
    "- **Long-term planning:** Use higher `Œ≥` (0.9-0.99)\n",
    "- **Uncertain environments:** Use lower `Œ≥`\n",
    "- **Stable environments:** Use higher `Œ≥`\n",
    "\n",
    "\n",
    "-  **Real-world application** - Helps RL agents decide between immediate gains vs long-term benefits (like training a pet with treats)\n",
    "\n",
    "> The code demonstrates the fundamental RL concept that **immediate rewards are more valuable than future ones**, which is essential for agent decision-making.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece4f8c",
   "metadata": {},
   "source": [
    "## **üîñ3. Interacting with Gymnasium Environments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82574e9",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "By the end of this section, you should understand:\n",
    "- The purpose and capabilities of the Gymnasium library\n",
    "- How to `create`, `initialize`, and `interact` with RL environments\n",
    "- Methods for visualizing environment states\n",
    "- How to execute actions and interpret responses\n",
    "- Building complete training loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25403d0",
   "metadata": {},
   "source": [
    "### 3.1 Introduction to Gymnasium\n",
    "\n",
    "> \"Gymnasium: Standard library for RL tasks, Abstracts complexity of RL problems, Provides a plethora of RL environments\"\n",
    "\n",
    "**Comprehensive Library Overview:**\n",
    "\n",
    "#### What is Gymnasium?\n",
    "\n",
    "**Gymnasium** (formerly OpenAI Gym) is the de facto standard library for reinforcement learning research and development. It provides a unified interface for interacting with a wide variety of RL environments.\n",
    "\n",
    "**Key Benefits:**\n",
    "1. **Standardization:** Consistent API across all environments\n",
    "2. **Abstraction:** Hides complex environment implementation details\n",
    "3. **Variety:** Hundreds of pre-built environments\n",
    "4. **Community:** Large ecosystem of additional environments\n",
    "5. **Research Reproducibility:** Standard benchmarks for comparing algorithms\n",
    "\n",
    "#### Gymnasium Architecture\n",
    "\n",
    "**Core Components:**\n",
    "- **Environment Interface:** Standardized methods (reset, step, render, close)\n",
    "- **Action Spaces:** Define valid actions for each environment\n",
    "- **Observation Spaces:** Define structure of state observations\n",
    "- **Reward Specification:** Define reward ranges and meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "641eae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total environments: 63\n",
      "\n",
      "Acrobot: 1 envs | Examples: Acrobot-v1\n",
      "Ant: 4 envs | Examples: Ant-v2, Ant-v3, Ant-v4 ...\n",
      "BipedalWalker: 1 envs | Examples: BipedalWalker-v3\n",
      "BipedalWalkerHardcore: 1 envs | Examples: BipedalWalkerHardcore-v3\n",
      "Blackjack: 1 envs | Examples: Blackjack-v1\n",
      "CarRacing: 1 envs | Examples: CarRacing-v3\n",
      "CartPole: 2 envs | Examples: CartPole-v0, CartPole-v1\n",
      "CliffWalking: 1 envs | Examples: CliffWalking-v1\n",
      "CliffWalkingSlippery: 1 envs | Examples: CliffWalkingSlippery-v1\n",
      "FrozenLake: 1 envs | Examples: FrozenLake-v1\n",
      "FrozenLake8x8: 1 envs | Examples: FrozenLake8x8-v1\n",
      "GymV21Environment: 1 envs | Examples: GymV21Environment-v0\n",
      "GymV26Environment: 1 envs | Examples: GymV26Environment-v0\n",
      "HalfCheetah: 4 envs | Examples: HalfCheetah-v2, HalfCheetah-v3, HalfCheetah-v4 ...\n",
      "Hopper: 4 envs | Examples: Hopper-v2, Hopper-v3, Hopper-v4 ...\n",
      "Humanoid: 4 envs | Examples: Humanoid-v2, Humanoid-v3, Humanoid-v4 ...\n",
      "HumanoidStandup: 3 envs | Examples: HumanoidStandup-v2, HumanoidStandup-v4, HumanoidStandup-v5\n",
      "InvertedDoublePendulum: 3 envs | Examples: InvertedDoublePendulum-v2, InvertedDoublePendulum-v4, InvertedDoublePendulum-v5\n",
      "InvertedPendulum: 3 envs | Examples: InvertedPendulum-v2, InvertedPendulum-v4, InvertedPendulum-v5\n",
      "LunarLander: 1 envs | Examples: LunarLander-v3\n",
      "LunarLanderContinuous: 1 envs | Examples: LunarLanderContinuous-v3\n",
      "MountainCar: 1 envs | Examples: MountainCar-v0\n",
      "MountainCarContinuous: 1 envs | Examples: MountainCarContinuous-v0\n",
      "Pendulum: 1 envs | Examples: Pendulum-v1\n",
      "Pusher: 3 envs | Examples: Pusher-v2, Pusher-v4, Pusher-v5\n",
      "Reacher: 3 envs | Examples: Reacher-v2, Reacher-v4, Reacher-v5\n",
      "Swimmer: 4 envs | Examples: Swimmer-v2, Swimmer-v3, Swimmer-v4 ...\n",
      "Taxi: 1 envs | Examples: Taxi-v3\n",
      "Walker2d: 4 envs | Examples: Walker2d-v2, Walker2d-v3, Walker2d-v4 ...\n",
      "phys2d/CartPole: 2 envs | Examples: phys2d/CartPole-v0, phys2d/CartPole-v1\n",
      "phys2d/Pendulum: 1 envs | Examples: phys2d/Pendulum-v0\n",
      "tabular/Blackjack: 1 envs | Examples: tabular/Blackjack-v0\n",
      "tabular/CliffWalking: 1 envs | Examples: tabular/CliffWalking-v0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "def explore_gymnasium():\n",
    "    \"\"\"List Gymnasium environments grouped by category.\"\"\"\n",
    "    env_names = list(gym.envs.registry.keys())\n",
    "    categories = defaultdict(list)\n",
    "\n",
    "    for name in env_names:\n",
    "        category = name.split('-')[0] if '-' in name else name.split('/')[0]\n",
    "        categories[category].append(name)\n",
    "\n",
    "    print(f\"Total environments: {len(env_names)}\\n\")\n",
    "    for cat, envs in sorted(categories.items()):\n",
    "        preview = \", \".join(envs[:3]) + (\" ...\" if len(envs) > 3 else \"\")\n",
    "        print(f\"{cat}: {len(envs)} envs | Examples: {preview}\")\n",
    "\n",
    "    return categories\n",
    "\n",
    "# Run\n",
    "if __name__ == \"__main__\":\n",
    "    explore_gymnasium()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6d700",
   "metadata": {},
   "source": [
    "### 3.2 Key Gymnasium Environments\n",
    "\n",
    "**Original Content:**\n",
    "> \"Key Gymnasium environments: CartPole: Agent must balance a pole on moving cart, MountainCar: Agent must drive a car up a steep hill, FrozenLake: Agent must navigate a frozen lake with holes, Taxi: Picking up and dropping off passengers\"\n",
    "\n",
    "**Detailed Environment Analysis:**\n",
    "\n",
    "#### `CartPole-v1`\n",
    "**Original Description:** \"Agent must balance a pole on moving cart\"\n",
    "\n",
    "**Detailed Specifications:**\n",
    "- **Objective:** Balance a pole on a movable cart for as long as possible\n",
    "- **State Space:** 4-dimensional continuous\n",
    "  - Cart position: [-4.8, 4.8]\n",
    "  - Cart velocity: [-‚àû, ‚àû]  \n",
    "  - Pole angle: [-0.418, 0.418] radians (~24¬∞)\n",
    "  - Pole angular velocity: [-‚àû, ‚àû]\n",
    "- **Action Space:** Discrete (2 actions)\n",
    "  - 0: Push cart to the left\n",
    "  - 1: Push cart to the right\n",
    "- **Rewards:** +1 for every timestep pole remains upright\n",
    "- **Episode End:** Pole angle > 15¬∞, cart position > 2.4, or 500 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a5e055f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
      "State:  [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward:  1.0\n",
      "Terminated:  False\n",
      "Step 1 - State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Step 1 - Reward: 1.0\n",
      "Step 1 - Terminated: False\n",
      "Step 2 - State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Step 2 - Reward: 1.0\n",
      "Step 2 - Terminated: False\n",
      "Step 3 - State: [ 0.05025892  0.7725766   0.00367314 -1.111468  ]\n",
      "Step 3 - Reward: 1.0\n",
      "Step 3 - Terminated: False\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Creating and initializing the environment\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "state, info = env.reset(seed=42)\n",
    "print(state)\n",
    "\n",
    "# Performing actions\n",
    "action = 1  # Move to the right\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(\"State: \", state)\n",
    "print(\"Reward: \", reward)\n",
    "print(\"Terminated: \", terminated)\n",
    "\n",
    "# Simple interaction loop for a few steps\n",
    "for i in range(3):\n",
    "    if not terminated or truncated:\n",
    "        action = 1  # Move to the right\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        print(f\"Step {i+1} - State: {state}\")\n",
    "        print(f\"Step {i+1} - Reward: {reward}\")\n",
    "        print(f\"Step {i+1} - Terminated: {terminated}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40971a",
   "metadata": {},
   "source": [
    "### `MountainCar-v0`\n",
    "**Original Description:** \"Agent must drive a car up a steep hill\"\n",
    "\n",
    "**Detailed Specifications:**\n",
    "- **Objective:** Drive an underpowered car up a steep hill\n",
    "- **Challenge:** Car's engine isn't strong enough to drive straight up\n",
    "- **Strategy Required:** Build momentum by oscillating back and forth\n",
    "- **State Space:** 2-dimensional continuous\n",
    "  - Car position: [-1.2, 0.6] (goal at 0.5)\n",
    "  - Car velocity: [-0.07, 0.07]\n",
    "- **Action Space:** Discrete (3 actions)\n",
    "  - 0: Accelerate left\n",
    "  - 1: Don't accelerate  \n",
    "  - 2: Accelerate right\n",
    "- **Rewards:** -1 for every timestep until goal reached\n",
    "- **Episode End:** Reach goal (position ‚â• 0.5) or 200 steps\n",
    "\n",
    "### `FrozenLake-v1`\n",
    "**Original Description:** \"Agent must navigate a frozen lake with holes\"\n",
    "\n",
    "**Detailed Specifications:**\n",
    "- **Objective:** Navigate from start (S) to goal (G) avoiding holes (H)\n",
    "- **Environment:** 4√ó4 or 8√ó8 grid world\n",
    "- **Challenges:** \n",
    "  - Slippery surface (stochastic movement)\n",
    "  - Falling in holes ends episode\n",
    "- **State Space:** Discrete (16 positions for 4√ó4 version)\n",
    "- **Action Space:** Discrete (4 actions)\n",
    "  - 0: Left\n",
    "  - 1: Down  \n",
    "  - 2: Right\n",
    "  - 3: Up\n",
    "- **Rewards:** +1 for reaching goal, 0 otherwise\n",
    "- **Stochasticity:** Intended action succeeds 1/3 time, perpendicular actions 1/3 each\n",
    "\n",
    "### `Taxi-v3`\n",
    "**Original Description:** \"Picking up and dropping off passengers\"\n",
    "\n",
    "**Detailed Specifications:**\n",
    "- **Objective:** Navigate taxi to pick up passenger and deliver to destination\n",
    "- **Environment:** 5√ó5 grid with designated pickup/dropoff locations\n",
    "- **State Space:** Discrete (500 states)\n",
    "  - Taxi position: 25 locations\n",
    "  - Passenger location: 5 possibilities (4 locations + in taxi)  \n",
    "  - Destination: 4 locations\n",
    "- **Action Space:** Discrete (6 actions)\n",
    "  - 0-3: Move (south, north, east, west)\n",
    "  - 4: Pickup passenger\n",
    "  - 5: Dropoff passenger\n",
    "- **Rewards:** \n",
    "  - +20 for successful dropoff\n",
    "  - -10 for illegal pickup/dropoff\n",
    "  - -1 for each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e65472",
   "metadata": {},
   "source": [
    "**Key Features of Complete Interaction Loops:**\n",
    "\n",
    "1. **Policy Flexibility:** Support for different action selection strategies\n",
    "2. **Comprehensive Logging:** Detailed tracking of episodes and steps\n",
    "3. **Callback System:** Extensible monitoring and intervention points\n",
    "4. **Performance Analysis:** Built-in statistics and visualization\n",
    "5. **Error Handling:** Robust error management and recovery\n",
    "6. **Resource Management:** Proper cleanup and resource handling\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RL_Fundamentals_with_Gymnasium.ipynb"
  },
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
