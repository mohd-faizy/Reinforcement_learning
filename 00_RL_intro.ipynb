{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607eeb4b",
   "metadata": {},
   "source": [
    "\n",
    "# **‚≠êFundamentals of Reinforcement Learning ‚Äî with Gymnasium in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d9cf3",
   "metadata": {},
   "source": [
    "*************************************************\n",
    "## **1: Introduction to Reinforcement Learning**\n",
    "*************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c37a9",
   "metadata": {},
   "source": [
    "### 1. What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a **machine learning paradigm where an agent learns by interacting with an environment** through **trial and error**.\n",
    "\n",
    "* **Learning Process**: Agent explores and discovers optimal behavior via interaction.\n",
    "* **Feedback Mechanism**: Rewards for good decisions, penalties for bad ones.\n",
    "* **Objective**: Maximize long-term cumulative reward.\n",
    "\n",
    "\n",
    "\n",
    "### 2. RL as Training a Pet üê∂\n",
    "\n",
    "Analogy to make it intuitive:\n",
    "\n",
    "* **Agent = Pet** ‚Üí tries different behaviors (actions).\n",
    "* **Reward = Treat** ‚Üí given for good actions.\n",
    "* **Penalty = Scolding** ‚Üí given for bad actions.\n",
    "* Over time, the **pet learns which behaviors yield treats** (optimal policy).\n",
    "\n",
    "\n",
    "\n",
    "### 3. RL vs. Other Machine Learning Types\n",
    "\n",
    "#### üîπ Supervised Learning\n",
    "\n",
    "* Has labeled training data with correct answers.\n",
    "* Learns **from examples**.\n",
    "* Example: Predicting house prices given features.\n",
    "\n",
    "#### üîπ Unsupervised Learning\n",
    "\n",
    "* Finds **hidden patterns** in unlabeled data.\n",
    "* Example: Clustering customers based on spending habits.\n",
    "\n",
    "#### üîπ Reinforcement Learning\n",
    "\n",
    "* Learns by **acting and receiving rewards/penalties**.\n",
    "* Focuses on **sequential decision-making**.\n",
    "* No correct answer provided ‚Üí agent must discover it.\n",
    "\n",
    "‚úÖ **Key Features of RL:**\n",
    "\n",
    "* Sequential decision-making\n",
    "* Actions influence future states\n",
    "* Learns through rewards/penalties\n",
    "* No direct supervision\n",
    "\n",
    "\n",
    "\n",
    "### 4. When to Use RL?\n",
    "\n",
    "#### ‚úÖ Suitable Scenarios\n",
    "\n",
    "* **Sequential decision-making** (actions affect future states).\n",
    "* **Environmental interaction** (dynamic changes).\n",
    "* **Reward-based learning** (clear reward signal).\n",
    "* **No direct supervision** available.\n",
    "\n",
    "**Example ‚Äî Video Games üéÆ**\n",
    "\n",
    "* Agent (player) makes sequential moves.\n",
    "* Actions affect game state.\n",
    "* Rewards = points, Penalties = losing lives.\n",
    "* No teacher showing correct moves.\n",
    "\n",
    "```python\n",
    "\n",
    "# Simplified RL Game Example\n",
    "state = game.get_state()\n",
    "action = agent.choose_action(state)\n",
    "reward, next_state = game.step(action)\n",
    "agent.learn(state, action, reward, next_state)\n",
    "```\n",
    "\n",
    "#### ‚ùå Not Suitable\n",
    "\n",
    "**Example ‚Äî Object Recognition üñºÔ∏è**\n",
    "\n",
    "* No sequential decision-making.\n",
    "* Static classification problem.\n",
    "* Supervised learning is more efficient.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 5. RL Applications üöÄ\n",
    "\n",
    "#### 1. **Robotics** ü§ñ\n",
    "\n",
    "* **Robot Walking**: Learning locomotion via trial and error.\n",
    "* **Object Manipulation**: Picking and placing objects.\n",
    "\n",
    "#### 2. **Finance** üíπ\n",
    "\n",
    "* **Trading Optimization**: Deciding when to buy/sell stocks.\n",
    "* **Investment Strategies**: Maximizing long-term returns.\n",
    "* **Risk Management**: Balancing reward vs. risk.\n",
    "\n",
    "#### 3. **Autonomous Vehicles** üöó\n",
    "\n",
    "* **Safety Enhancement**: Learning safe driving behaviors.\n",
    "* **Efficiency Optimization**: Fuel-efficient route planning.\n",
    "* **Risk Minimization**: Avoiding hazards and accidents.\n",
    "\n",
    "#### 4. **Chatbots** üí¨\n",
    "\n",
    "* **Conversational Skills**: Responding meaningfully.\n",
    "* **User Experience**: Improving dialogue flow.\n",
    "* **Personalization**: Adapting to user preferences.\n",
    "\n",
    "\n",
    "\n",
    "‚ö° Quick Visualization of RL Loop:\n",
    "\n",
    "```\n",
    "Agent  ‚Üí  takes Action  ‚Üí  Environment\n",
    " ‚Üë                             ‚Üì\n",
    " Reward  ‚Üê  Feedback  ‚Üê  State Changes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e96ac5",
   "metadata": {},
   "source": [
    "**************************************\n",
    "## **2: Navigating the RL Framework**\n",
    "**************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc1d91",
   "metadata": {},
   "source": [
    "### 1. Core Components\n",
    "\n",
    "#### üîπ Agent\n",
    "\n",
    "* The **learner and decision-maker**.\n",
    "* Observes the environment and **takes actions**.\n",
    "* Goal ‚Üí Learn an **optimal policy** to maximize rewards.\n",
    "\n",
    "#### üîπ Environment üåç\n",
    "\n",
    "* The **world/problem** the agent interacts with.\n",
    "* Provides **challenges** to solve.\n",
    "* Responds to agent‚Äôs actions with **new states** and **rewards**.\n",
    "\n",
    "#### üîπ State üßæ\n",
    "\n",
    "* A **snapshot of the environment** at a given moment.\n",
    "* Contains all relevant info needed for decision-making.\n",
    "* Represents the **current situation** the agent is in.\n",
    "\n",
    "#### üîπ Action üéØ\n",
    "\n",
    "* Agent‚Äôs **choice/response** to the current state.\n",
    "* Set of possible actions depends on the environment.\n",
    "* Selected using the **agent‚Äôs policy**.\n",
    "\n",
    "#### üîπ Reward ‚≠ê\n",
    "\n",
    "* Immediate **feedback** for an agent‚Äôs action.\n",
    "* A **numerical signal** indicating action quality.\n",
    "* Drives the learning process:\n",
    "\n",
    "  * Positive ‚Üí Good choice\n",
    "  * Negative ‚Üí Bad choice\n",
    "\n",
    "\n",
    "\n",
    "### 2. The RL Interaction Loop üîÑ\n",
    "\n",
    "```python\n",
    "env = create_environment()\n",
    "state = env.get_initial_state()\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    action = choose_action(state)                # 1. Agent decides\n",
    "    state, reward = env.execute(action)          # 2. Env responds\n",
    "    update_knowledge(state, action, reward)      # 3. Agent learns\n",
    "```\n",
    "\n",
    "**Loop Breakdown:**\n",
    "\n",
    "1. Initialize environment and get starting state.\n",
    "2. Agent observes **current state**.\n",
    "3. Agent selects **action** (based on policy).\n",
    "4. Environment returns **new state + reward**.\n",
    "5. Agent **updates policy** using experience.\n",
    "6. Repeat until task ends (or indefinitely for continuous tasks).\n",
    "\n",
    "üìä **Visual:**\n",
    "\n",
    "```\n",
    "   State (S) ‚îÄ‚îÄ‚ñ∫ Agent ‚îÄ‚îÄ‚ñ∫ Action (A)\n",
    "      ‚ñ≤                          ‚îÇ\n",
    "      ‚îÇ                          ‚ñº\n",
    "   Reward (R) ‚óÑ‚îÄ‚îÄ Environment ‚óÑ‚îÄ‚îÄ New State (S‚Äô)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 3. Task Types\n",
    "\n",
    "#### üïπÔ∏è Episodic Tasks\n",
    "\n",
    "* Structure: Tasks divided into **episodes**.\n",
    "* Characteristics: Clear **start** and **end**.\n",
    "* Example: **Chess** ‚Üí Begins with setup, ends with checkmate/draw.\n",
    "* Learning: Agent improves **across multiple episodes**.\n",
    "\n",
    "#### üîÑ Continuous Tasks\n",
    "\n",
    "* Structure: **Ongoing interaction** without end.\n",
    "* Characteristics: No distinct breaks.\n",
    "* Example: **Traffic light control system** ‚Üí Runs continuously.\n",
    "* Learning: Agent adapts while **operating indefinitely**.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Return and Long-term Consequences\n",
    "\n",
    "#### üí° Return Concept\n",
    "\n",
    "* **Return** = Sum of all expected **future rewards**.\n",
    "* Agent‚Äôs **true goal**: Maximize **long-term return**, not just immediate reward.\n",
    "* Must balance **short-term vs. long-term** benefits.\n",
    "\n",
    "**Why Return Matters?**\n",
    "\n",
    "* Immediate reward might be small but ‚Üí could lead to **larger future gain**.\n",
    "* Encourages **strategic planning**, not greedy actions.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Discounted Return\n",
    "\n",
    "#### ‚ö†Ô∏è Problem with Simple Return\n",
    "\n",
    "* Future rewards are **uncertain**.\n",
    "* Immediate rewards are **more valuable** than far-future rewards.\n",
    "\n",
    "#### ‚úÖ Discounted Return Solution\n",
    "\n",
    "* **Formula**:\n",
    "\n",
    "  $$\n",
    "  G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\dots\n",
    "  $$\n",
    "\n",
    "* **Œ≥ (gamma)** ‚Üí Discount Factor (0 ‚â§ Œ≥ ‚â§ 1).\n",
    "\n",
    "* Interprets **how much we value future rewards**:\n",
    "\n",
    "  * Œ≥ = 0 ‚Üí Only immediate reward matters (**short-sighted**).\n",
    "  * Œ≥ = 1 ‚Üí Future rewards fully considered (**far-sighted**).\n",
    "  * Œ≥ ‚âà 0.9 ‚Üí Common balance between immediate & future.\n",
    "\n",
    "\n",
    "\n",
    "#### üî¢ Example: Discounted Return Calculation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Expected rewards for next 3 steps\n",
    "expected_rewards = np.array([1, 6, 3])\n",
    "gamma = 0.9\n",
    "\n",
    "# Compute discount multipliers\n",
    "discounts = np.array([gamma ** i for i in range(len(expected_rewards))])\n",
    "print(\"Discounts:\", discounts)\n",
    "# Output: [1.0, 0.9, 0.81]\n",
    "\n",
    "# Discounted return\n",
    "discounted_return = np.sum(expected_rewards * discounts)\n",
    "print(\"Discounted Return:\", discounted_return)\n",
    "# Output: 8.83\n",
    "```\n",
    "\n",
    "**Calculation Breakdown:**\n",
    "\n",
    "* Immediate reward (t=0): 1 √ó 1.0 = **1.0**\n",
    "* Next reward (t=1): 6 √ó 0.9 = **5.4**\n",
    "* Future reward (t=2): 3 √ó 0.81 = **2.43**\n",
    "* **Total Discounted Return** = 1.0 + 5.4 + 2.43 = **8.83**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4eeb2e",
   "metadata": {},
   "source": [
    "**************************************************\n",
    "## **3: Interacting with Gymnasium Environments**\n",
    "**************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f038ca1f",
   "metadata": {},
   "source": [
    "### 1. What is Gymnasium?\n",
    "\n",
    "- Gymnasium is a toolkit for developing and testing reinforcement learning (RL) algorithms. Think of it as a playground of ready-made environments where you can plug in your RL agent and test how well it learns.\n",
    "  - üîπ Abstracts Complexity: Instead of writing physics simulations or custom rules for environments from scratch, Gymnasium gives you ready-to-use environments like games, robotic simulations, and navigation problems.\n",
    "  - üîπ Benchmarking: Because everyone uses the same environments (e.g., CartPole, MountainCar), results are comparable across papers, tutorials, and experiments.\n",
    "  - üîπ Unified API: Every environment has the same way of interacting with your agent (`reset()`, `step()`, `render()`), so your RL code works across different problems with little modification.\n",
    "  - üîπ Evolution from OpenAI Gym: Gymnasium is the actively maintained successor to OpenAI Gym, with bug fixes, new environments, and compatibility with modern RL frameworks.\n",
    "\n",
    "> üëâ Why it matters: Without Gymnasium, RL research would be messy ‚Äî everyone would have different setups, making comparison and reproducibility nearly impossible.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Key Gymnasium Environments üåç\n",
    "\n",
    "- Gymnasium environments\n",
    "  - `CartPole`\n",
    "  - `MountainCar`\n",
    "  - `FrozenLake`\n",
    "  - `Taxi`\n",
    "\n",
    "- Key components:\n",
    "  - **Parameters / State (observations)**\n",
    "  - **Action space (what the agent can do)**\n",
    "  - **Environment dynamics (how it reacts)**\n",
    "  - **Agent‚Äôs role**\n",
    "\n",
    "\n",
    "\n",
    "#### üîπ 1. CartPole\n",
    "\n",
    "- **Environment**:\n",
    "  - A cart moves along a track with a pole attached.\n",
    "  - The task is to keep the pole from falling over.\n",
    "\n",
    "- **State (Observation Space)**:  \n",
    "  A vector of **4 continuous values**:\n",
    "  1. **Cart Position** ‚Üí (meters) distance from center (negative = left, positive = right).\n",
    "  2. **Cart Velocity** ‚Üí (m/s) speed of cart left/right.\n",
    "  3. **Pole Angle** ‚Üí (radians) tilt of pole from vertical.\n",
    "  4. **Pole Angular Velocity** ‚Üí (rad/s) rate of pole falling.\n",
    "\n",
    "- **Action Space (Discrete)**:\n",
    "  - `0`: Push cart **left**\n",
    "  - `1`: Push cart **right**\n",
    "\n",
    "- **Reward Signal**:\n",
    "  - +1 for every timestep the pole is balanced (until it falls or time ends).\n",
    "\n",
    "- **Agent‚Äôs Job**:\n",
    "  - Observe the **state vector**.\n",
    "  - Choose **left/right** actions that maximize the time the pole stays upright.\n",
    "\n",
    "\n",
    "\n",
    "#### üîπ 2. MountainCar\n",
    "\n",
    "- **Environment**:\n",
    "  - A small underpowered car is stuck in a valley between two hills.\n",
    "  - Goal: Reach the **flag at the top of the right hill**.\n",
    "  - Problem: The engine is too weak to drive up directly ‚Üí agent must build **momentum**.\n",
    "\n",
    "- **State (Observation Space)**:  \n",
    "  A vector of **2 continuous values**:\n",
    "  1. **Car Position** ‚Üí horizontal location on track (range: `[-1.2, 0.6]`).\n",
    "  2. **Car Velocity** ‚Üí speed of the car (range: `[-0.07, 0.07]`).\n",
    "\n",
    "- **Action Space (Discrete)**:\n",
    "  - `0`: Accelerate **left**\n",
    "  - `1`: Do **nothing** (coast)\n",
    "  - `2`: Accelerate **right**\n",
    "\n",
    "- **Reward Signal**:\n",
    "  - -1 for each timestep until the car reaches the flag.\n",
    "  - Encourages solving the task quickly.\n",
    "\n",
    "- **Agent‚Äôs Job**:\n",
    "  - Learn to **rock back and forth** to build momentum.\n",
    "  - Escape the valley using long-term planning (not greedy immediate actions).\n",
    "\n",
    "\n",
    "\n",
    "#### üîπ 3. FrozenLake\n",
    "\n",
    "- **Environment**:\n",
    "  - A **grid world** (default 4x4, but can be larger).\n",
    "  - Agent starts at `S` and must reach `G` (goal).\n",
    "  - Some tiles are `H` (holes) ‚Üí if agent falls, episode ends.\n",
    "  - Slippery ice: Actions may not always go in intended direction.\n",
    "\n",
    "- **State (Observation Space)**:\n",
    "  - Single integer representing agent‚Äôs **grid position** (0 to N¬≤‚àí1).\n",
    "  - Example (4x4):\n",
    "    - `0` = Top-left cell (Start `S`).\n",
    "    - `15` = Bottom-right cell (Goal `G`).\n",
    "\n",
    "- **Action Space (Discrete)**:\n",
    "  - `0`: Move **Left**\n",
    "  - `1`: Move **Down**\n",
    "  - `2`: Move **Right**\n",
    "  - `3`: Move **Up**\n",
    "\n",
    "- **Reward Signal**:\n",
    "  - `+1` if the agent reaches the goal.\n",
    "  - `0` for falling into holes or moving on frozen tiles.\n",
    "\n",
    "- **Agent‚Äôs Job**:\n",
    "  - Learn safe paths across the frozen grid.\n",
    "  - Balance **exploration vs. exploitation**, since slipping introduces randomness.\n",
    "\n",
    "\n",
    "\n",
    "#### üîπ 4. Taxi\n",
    "\n",
    "- **Environment**:\n",
    "  - A **5x5 grid world**.\n",
    "  - A taxi must pick up a passenger at one location and drop them off at the correct destination.\n",
    "\n",
    "- **State (Observation Space)**:\n",
    "  - Single integer encoding **(taxi_row, taxi_col, passenger_location, destination)**.\n",
    "  - **Taxi Position** ‚Üí row & column on grid.\n",
    "  - **Passenger Location** ‚Üí one of 5 (four fixed spots + inside taxi).\n",
    "  - **Destination** ‚Üí one of 4 fixed spots.\n",
    "  - üëâ This gives **500 discrete states** in total (25 taxi positions √ó 5 passenger states √ó 4 destinations).\n",
    "\n",
    "- **Action Space (Discrete)**:\n",
    "  - `0`: Move **South**\n",
    "  - `1`: Move **North**\n",
    "  - `2`: Move **East**\n",
    "  - `3`: Move **West**\n",
    "  - `4`: **Pickup** passenger\n",
    "  - `5`: **Dropoff** passenger\n",
    "\n",
    "- **Reward Signal**:\n",
    "  - `+20` for successful drop-off.\n",
    "  - `-1` per timestep (encourages efficiency).\n",
    "  - `-10` for illegal pickup/dropoff.\n",
    "\n",
    "- **Agent‚Äôs Job**:\n",
    "  - Learn **navigation** (move efficiently on grid).\n",
    "  - Learn **task completion** (pickup + dropoff).\n",
    "  - Optimize strategy to minimize penalties & maximize rewards.\n",
    "\n",
    "\n",
    "\n",
    "#### üìä Quick Comparison Table\n",
    "\n",
    "| Environment     | State (Observation)                                    | Actions                      | Reward Scheme                     | Agent‚Äôs Goal                  |\n",
    "| --------------- | ------------------------------------------------------ | ---------------------------- | --------------------------------- | ----------------------------- |\n",
    "| **CartPole**    | 4 floats (position, velocity, angle, angular velocity) | Left / Right                 | +1 per step until failure         | Balance the pole              |\n",
    "| **MountainCar** | 2 floats (position, velocity)                          | Left, Coast, Right           | -1 per step until flag            | Reach hilltop using momentum  |\n",
    "| **FrozenLake**  | Discrete grid position (0‚ÄìN¬≤‚àí1)                        | Up, Down, Left, Right        | +1 for goal, 0 otherwise          | Reach goal, avoid holes       |\n",
    "| **Taxi**        | Encoded state (taxi pos, passenger loc, destination)   | Move 4 dirs, Pickup, Dropoff | +20 success, -1 step, -10 illegal | Deliver passenger efficiently |\n",
    "\n",
    "<br>\n",
    "\n",
    "> üëâ This way, you see **who the agent is, what the environment looks like, what information the agent observes, what moves it can make, and how it gets rewarded/punished**.\n",
    "\n",
    "\n",
    "\n",
    "### üîπ MountainCar\n",
    "\n",
    "- **Objective**: Drive an underpowered car up a steep hill  \n",
    "- **Challenge**: Car cannot go straight up ‚Üí must **build momentum**  \n",
    "- **Actions**: Accelerate left (0), coast (1), accelerate right (2)  \n",
    "\n",
    "\n",
    "\n",
    "### üîπ FrozenLake\n",
    "\n",
    "- **Objective**: Navigate frozen lake grid to goal  \n",
    "- **Challenge**: Some tiles are **holes** ‚Üí falling ends episode  \n",
    "- **Actions**: Up, Down, Left, Right  \n",
    "- **Environment**: Slippery ‚Üí agent may not move as intended  \n",
    "\n",
    "\n",
    "\n",
    "### üîπ Taxi\n",
    "\n",
    "- **Objective**: Pick up passenger & drop off at destination  \n",
    "- **Challenge**: Navigate grid efficiently  \n",
    "- **Actions**: Move in 4 directions, pickup, dropoff  \n",
    "- **Complexity**: Multiple objectives (navigation + delivery)  \n",
    "\n",
    "\n",
    "\n",
    "### 3. Gymnasium Interface (Unified API)\n",
    "\n",
    "Every environment follows a **standard structure**:\n",
    "\n",
    "- `env = gym.make(\"EnvName\")` ‚Üí create environment\n",
    "- `env.reset()` ‚Üí start/reset environment\n",
    "- `env.step(action)` ‚Üí take an action\n",
    "- `env.render()` ‚Üí visualize state\n",
    "- Handles **state, reward, termination, truncation, info** consistently\n",
    "\n",
    "\n",
    "\n",
    "### 4. Working with Gymnasium ‚Äî Code Examples\n",
    "\n",
    "#### üîπ 1. Creating and Initializing Environment\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset environment to initial state\n",
    "state, info = env.reset(seed=42)\n",
    "print(\"Initial State:\", state)\n",
    "````\n",
    "\n",
    "**CartPole State Example:**\n",
    "`[-0.044, 0.024, -0.043, -0.017]`\n",
    "\n",
    "* Cart position = -0.044\n",
    "* Cart velocity = 0.024\n",
    "* Pole angle = -0.043\n",
    "* Pole angular velocity = -0.017\n",
    "\n",
    "\n",
    "\n",
    "### üîπ 2. Visualizing the Environment\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Render current state as image\n",
    "state_image = env.render()\n",
    "plt.imshow(state_image)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Reusable function:\n",
    "\n",
    "```python\n",
    "def render():\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "\n",
    "render()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### üîπ 3. Performing Actions\n",
    "\n",
    "```python\n",
    "# Actions: 0 = Move Left, 1 = Move Right\n",
    "action = 1\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(\"State:\", state)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Terminated:\", terminated)\n",
    "```\n",
    "\n",
    "**Possible Output:**\n",
    "\n",
    "```\n",
    "State: [-0.0435  0.2200 -0.0441 -0.3238]\n",
    "Reward: 1.0\n",
    "Terminated: False\n",
    "```\n",
    "\n",
    "**Return values of `env.step()`:**\n",
    "\n",
    "* **state** ‚Üí next environment state\n",
    "* **reward** ‚Üí immediate reward (usually 1 per step for CartPole)\n",
    "* **terminated** ‚Üí True if episode ends naturally (win/lose)\n",
    "* **truncated** ‚Üí True if time limit exceeded\n",
    "* **info** ‚Üí diagnostic info (optional)\n",
    "\n",
    "\n",
    "\n",
    "#### üîπ 4. Basic Interaction Loop\n",
    "\n",
    "```python\n",
    "terminated, truncated = False, False\n",
    "state, info = env.reset()\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action = 1  # Always move right\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    render()  # Visualize each step\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 5. Key Concepts Summary\n",
    "\n",
    "**üîÅ Environment Lifecycle:**\n",
    "\n",
    "1. **Creation** ‚Üí `gym.make()`\n",
    "2. **Initialization** ‚Üí `env.reset()`\n",
    "3. **Interaction** ‚Üí `env.step(action)`\n",
    "4. **Observation** ‚Üí state, reward, termination flags\n",
    "5. **Visualization** ‚Üí `env.render()`\n",
    "\n",
    "**‚ö†Ô∏è Important Notes:**\n",
    "\n",
    "* Always `reset()` before a new episode\n",
    "* Handle **both `terminated` and `truncated`**\n",
    "* Use **seeds** for reproducibility\n",
    "* `render_mode` determines visualization style\n",
    "* Action/state spaces differ per environment\n",
    "\n",
    "\n",
    "> ‚ö° By learning Gymnasium, you can **prototype and test RL algorithms quickly** on standard benchmarks before applying them to custom real-world problems."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RL_Fundamentals_with_Gymnasium.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexers": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
