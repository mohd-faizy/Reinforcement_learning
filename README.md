<div align="center">

# ğŸ§  Reinforcement Learning

*A simple repository to learn Reinforcement Learning with theory, algorithms, and practice.*

[**RL Theory**](#rl-theory) |
[**RL Algorithms**](#rl-algorithms) |
[**Implementations**](#implementations)  
[**RL Projects**](#rl-projects) |
[**RL Notebooks**](#rl-notebooks) |
[**Experiments & Results**](#experiments--results)


![Author](https://img.shields.io/badge/Author-mohd--faizy-blue?style=for-the-badge&logo=github&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.9%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.8.0-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.20.0-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![Gymnasium](https://img.shields.io/badge/Gymnasium-1.1.1-008000?style=for-the-badge&logo=openai&logoColor=white)
![Stable-Baselines3](https://img.shields.io/badge/Stable--Baselines3-RL%20Library-00BFFF?style=for-the-badge&logo=python&logoColor=white)
![RLlib](https://img.shields.io/badge/Ray-RLlib-FFCA28?style=for-the-badge&logo=ray&logoColor=black)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebooks-F37626?style=for-the-badge&logo=jupyter&logoColor=white)
![Weights & Biases](https://img.shields.io/badge/Weights%20%26%20Biases-Experiments-FFB000?style=for-the-badge&logo=weightsandbiases&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-Visualization-004C99?style=for-the-badge&logo=plotly&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)

</div>


---

## ğŸ“š Table of Contents

- [ğŸ§  Reinforcement Learning](#-reinforcement-learning)
  - [ğŸ“š Table of Contents](#-table-of-contents)
  - [ğŸ›£ï¸ Roadmap](#ï¸-roadmap)
  - [ğŸ§¬ What is Reinforcement Learning?](#-what-is-reinforcement-learning)
    - [ğŸŒŸ Real-World Applications](#-real-world-applications)
    - [ğŸ› ï¸ Core Components](#ï¸-core-components)
    - [ğŸ§  RL vs Supervised Learning](#-rl-vs-supervised-learning)
  - [ğŸ“ RL Curriculum](#-rl-curriculum)
    - [ğŸŸ¢ Beginner Level](#-beginner-level)
    - [ğŸŸ¡ Intermediate Level](#-intermediate-level)
    - [ğŸ”´ Advanced Level](#-advanced-level)
  - [âš™ï¸ Installation](#ï¸-installation)
    - [ğŸ“‹ Prerequisites](#-prerequisites)
    - [ğŸš€ Quick Setup](#-quick-setup)
    - [ğŸ“¦ Core Dependencies](#-core-dependencies)
  - [ğŸš€ Examples \& Demos](#-examples--demos)
    - [ğŸ“Š Algorithm Implementations](#-algorithm-implementations)
    - [ğŸ¥ Training Visualizations](#-training-visualizations)
    - [ğŸ”¬ Jupyter Notebooks](#-jupyter-notebooks)
  - [ğŸ¤ Contributing](#-contributing)
    - [ğŸ› ï¸ How to Contribute](#ï¸-how-to-contribute)
    - [ğŸ“ Contribution Guidelines](#-contribution-guidelines)
    - [ğŸ› Found a Bug?](#-found-a-bug)
  - [âš–ï¸ License](#ï¸-license)
  - [â¤ï¸ Support](#ï¸-support)
  - [ğŸª™ Credits \& Inspiration](#-credits--inspiration)
    - [ğŸ“š Foundational Resources](#-foundational-resources)
    - [ğŸ› ï¸ Open Source Libraries](#ï¸-open-source-libraries)
    - [ğŸ“ Educational Inspiration](#-educational-inspiration)
  - [ğŸ”— Connect with me](#-connect-with-me)

---

## ğŸ›£ï¸ Roadmap

```mermaid
graph TD
    A[ğŸ¯ Start Here] --> B[ğŸ“Š Foundations]
    B --> C[ğŸ° Multi-Armed Bandits]
    B --> D[ğŸ”„ Markov Decision Processes]
    
    C --> E[âš¡ Dynamic Programming]
    D --> E
    
    E --> F[ğŸ² Monte Carlo Methods]
    E --> G[â° Temporal Difference Learning]
    
    F --> H[ğŸ¯ Q-Learning & SARSA]
    G --> H
    
    H --> I[ğŸ“ˆ Function Approximation]
    I --> J[ğŸ§  Deep Q-Networks DQN]
    
    J --> K[ğŸ­ Policy Gradient Methods]
    K --> L[ğŸª Actor-Critic Methods]
    
    L --> M[ğŸš€ Advanced Deep RL]
    M --> N[ğŸ¤ Multi-Agent RL]
    M --> O[ğŸ—ï¸ Hierarchical RL]
    M --> P[ğŸ›¡ï¸ Safe RL]
    
    style A fill:#ff9999
    style B fill:#66b3ff
    style M fill:#99ff99
    style N fill:#ffcc99
    style O fill:#ffcc99
    style P fill:#ffcc99
```

---

<div align="center">
  <img src="_img\rl-map.png" alt="rl-map"/>
</div>

---

## ğŸ§¬ What is Reinforcement Learning?

Reinforcement Learning (RL) is a branch of machine learning where an **agent** learns to make decisions by interacting with an **environment**. Unlike supervised learning, RL doesn't rely on labeled data. Instead, the agent learns through trial and error, receiving **rewards** or **penalties** for its actions.

### ğŸŒŸ Real-World Applications

- ğŸ¤– **Robotics**: Robot navigation, manipulation, and control
- ğŸ® **Gaming**: AlphaGo, OpenAI Five, StarCraft II agents
- ğŸš— **Autonomous Vehicles**: Path planning and decision making
- ğŸ’° **Finance**: Algorithmic trading and portfolio management
- ğŸ¯ **Recommendation Systems**: Personalized content delivery
- âš¡ **Energy**: Smart grid optimization and resource allocation

---

### ğŸ› ï¸ Core Components

| Component | Description |
|-----------|-------------|
| ğŸ¤– **Agent** | The decision-maker that learns and takes actions |
| ğŸŒ **Environment** | The world the agent interacts with (MDPs, Gym environments) |
| ğŸ **Reward Function** | Feedback signal that guides learning (positive/negative) |
| ğŸ¯ **Policy** | The agent's strategy for choosing actions (deterministic/stochastic) |
| ğŸ’ **Value Function** | Estimates expected future rewards from states/actions |
| ğŸ” **Exploration vs Exploitation** | Balance between trying new actions and using known good ones |
| ğŸ§  **Training Algorithms** | Methods to improve the policy (Q-learning, policy gradients, etc.) |

---

### ğŸ§  RL vs Supervised Learning

| Aspect | Reinforcement Learning | Supervised Learning |
|--------|----------------------|-------------------|
| ğŸ“Š **Feedback Type** | Delayed rewards/penalties | Immediate labels |
| ğŸ“ˆ **Data Requirements** | Sequential interaction data | Static labeled datasets |
| ğŸ¯ **Training Objective** | Maximize cumulative reward | Minimize prediction error |
| ğŸ“¤ **Output** | Policies (action strategies) | Predictions/classifications |
| ğŸ”„ **Learning Style** | Trial and error | Pattern recognition |
| â° **Temporal Aspect** | Sequential decision making | Independent predictions |

---


## ğŸ“ RL Curriculum

### ğŸŸ¢ Beginner Level
- [ ] ğŸ° Multi-Armed Bandits
- [ ] ğŸ”„ Markov Decision Processes (MDPs)
- [ ] âš¡ Dynamic Programming (Value Iteration, Policy Iteration)
- [ ] ğŸ² Monte Carlo Methods
- [ ] â° Temporal Difference Learning (TD(0))

### ğŸŸ¡ Intermediate Level
- [ ] ğŸ¯ Q-Learning & SARSA
- [ ] ğŸª Expected SARSA & Double Q-Learning
- [ ] ğŸ“ˆ Function Approximation
- [ ] ğŸ§  Deep Q-Networks (DQN)
- [ ] ğŸ”§ DQN Variants (Double DQN, Dueling DQN, Prioritized Replay)

### ğŸ”´ Advanced Level
- [ ] ğŸ­ Policy Gradient Methods (REINFORCE)
- [ ] ğŸª Actor-Critic Methods (A2C, A3C)
- [ ] ğŸš€ Proximal Policy Optimization (PPO)
- [ ] ğŸŒŸ Deep Deterministic Policy Gradient (DDPG)
- [ ] ğŸ¯ Soft Actor-Critic (SAC)
- [ ] ğŸ¤ Multi-Agent Reinforcement Learning (MARL)
- [ ] ğŸ—ï¸ Hierarchical Reinforcement Learning
- [ ] ğŸ›¡ï¸ Safe Reinforcement Learning

---

## âš™ï¸ Installation

### ğŸ“‹ Prerequisites

- Python **3.9+**
- [Git](https://git-scm.com/) installed
- [UV](https://docs.astral.sh/uv/getting-started/installation/) package manager installed
- (Optional) CUDA-compatible GPU for deep RL training

---

### ğŸš€ Quick Setup

```bash
# Clone the repository
git clone https://github.com/mohd-faizy/Reinforcement_learning.git
cd reinforcement-learning

# Create and activate a virtual environment using UV
uv venv rl_env
source rl_env/bin/activate   # macOS/Linux
.\rl_env\Scripts\activate    # Windows

# Install dependencies
uv add -r requirements.txt

# For GPU support (optional, example: CUDA 12.1)
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### ğŸ“¦ Core Dependencies

```txt
gymnasium>=0.29.0
torch>=2.0.0
tensorflow>=2.13.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
jupyter>=1.0.0
wandb>=0.15.0
stable-baselines3>=2.0.0
```

---

## ğŸš€ Examples & Demos

### ğŸ“Š Algorithm Implementations

| Algorithm | Notebook/Script | Environment | Status |
|-----------|----------------|-------------|---------|
| ğŸ° Multi-Armed Bandit | `bandits/epsilon_greedy.ipynb` | Custom Bandits | âœ… |
| ğŸ¯ Q-Learning | `tabular/q_learning_frozen_lake.ipynb` | FrozenLake-v1 | âœ… |
| ğŸª SARSA | `tabular/sarsa_cliff_walking.ipynb` | CliffWalking-v0 | âœ… |
| ğŸ§  DQN | `deep_rl/dqn_cartpole.py` | CartPole-v1 | âœ… |
| ğŸ® DQN Atari | `deep_rl/dqn_atari_breakout.py` | ALE/Breakout-v5 | âœ… |
| ğŸ­ REINFORCE | `policy_gradient/reinforce_cartpole.py` | CartPole-v1 | âœ… |
| ğŸª A2C | `actor_critic/a2c_lunar_lander.py` | LunarLander-v2 | âœ… |
| ğŸš€ PPO | `advanced/ppo_continuous_control.py` | BipedalWalker-v3 | ğŸš§ |

### ğŸ¥ Training Visualizations

<details>
<summary>ğŸ® Click to see training demos</summary>

| Environment | Algorithm | Demo |
|-------------|-----------|------|
| CartPole-v1 | DQN | ![CartPole Demo](assets/cartpole_dqn.gif) |
| LunarLander-v2 | A2C | ![LunarLander Demo](assets/lunar_lander_a2c.gif) |
| Breakout | DQN | ![Breakout Demo](assets/breakout_dqn.gif) |

</details>

### ğŸ”¬ Jupyter Notebooks

Start exploring with our interactive notebooks:

```bash
# Launch Jupyter Lab
jupyter lab

# Navigate to notebooks/
# Start with: 01_introduction_to_rl.ipynb
```

---

## ğŸ¤ Contributing

We welcome contributions from the RL community! Here's how you can help:

### ğŸ› ï¸ How to Contribute

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingAlgorithm`)
3. **Commit** your changes (`git commit -m 'Add some AmazingAlgorithm'`)
4. **Push** to the branch (`git push origin feature/AmazingAlgorithm`)
5. **Open** a Pull Request

### ğŸ“ Contribution Guidelines

- âœ… Add tests for new algorithms
- âœ… Include docstrings and comments
- âœ… Update README if needed
- âœ… Follow PEP 8 style guidelines
- âœ… Add example usage in notebooks

### ğŸ› Found a Bug?

Please [open an issue](https://github.com/mohd-faizy/reinforcement-learning/issues) with:
- Description of the problem
- Steps to reproduce
- Expected vs actual behavior
- Environment details (OS, Python version, etc.)

---

## âš–ï¸ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2025 MOHD FAIZY

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction...
```

---

## â¤ï¸ Support

If this repository helped you learn RL, please consider:

- â­ **Starring** this repository
- ğŸ´ **Forking** for your own experiments
- ğŸ“¢ **Sharing** with fellow ML enthusiasts
- ğŸ› **Contributing** improvements and bug fixes


---

## ğŸª™ Credits & Inspiration

This repository builds upon the incredible work of the RL community:

### ğŸ“š Foundational Resources
- ğŸ“– **Sutton & Barto**: *Reinforcement Learning: An Introduction* (The RL Bible)
- ğŸ§  **DeepMind**: Pioneering DQN, AlphaGo, and agent architectures
- ğŸš€ **OpenAI**: GPT, PPO, and democratizing RL research

### ğŸ› ï¸ Open Source Libraries
- ğŸ‹ï¸ **Stable Baselines3**: High-quality RL implementations
- ğŸ¯ **Spinning Up (OpenAI)**: Educational RL resource
- ğŸ® **Gymnasium**: Maintained successor to OpenAI Gym
- âš¡ **Ray RLlib**: Scalable RL library
- ğŸ§ª **TensorFlow Agents**: TF-based RL library

### ğŸ“ Educational Inspiration
- ğŸ“º **David Silver's RL Course** (DeepMind/UCL)
- ğŸ¥ **Stanford CS234**: Reinforcement Learning
- ğŸ“± **Berkeley CS 285**: Deep Reinforcement Learning

---

## ğŸ”— Connect with me  

â¤ If you have questions or feedback, feel free to reach out!!!  

[<img align="left" src="https://cdn4.iconfinder.com/data/icons/social-media-icons-the-circle-set/48/twitter_circle-512.png" width="32px"/>][twitter]
[<img align="left" src="https://cdn-icons-png.flaticon.com/512/145/145807.png" width="32px"/>][linkedin]
[<img align="left" src="https://cdn-icons-png.flaticon.com/512/2626/2626299.png" width="32px"/>][Portfolio]  

[twitter]: https://twitter.com/F4izy  
[linkedin]: https://www.linkedin.com/in/mohd-faizy/  
[Portfolio]: https://ai.stackexchange.com/users/36737/faizy?tab=profile  

---

<div align="center">

  <img src="https://github-readme-stats.vercel.app/api?username=mohd-faizy&show_icons=true" width="380px" height="200px" />
  <img src="https://github-readme-stats.vercel.app/api/top-langs/?username=mohd-faizy&layout=compact" width="300px" height="200px" />

</div>



---

<div align="center">
  
**â­ Star this repo if you found it helpful! â­**

</div>