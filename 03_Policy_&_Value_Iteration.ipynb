{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9013b754",
   "metadata": {},
   "source": [
    "# **âœ¨Policy & Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127651d",
   "metadata": {},
   "source": [
    "## **ðŸ“‘Table of Contents**\n",
    "\n",
    "1. [Overview](#1-overview)\n",
    "\n",
    "2. [Policy Iteration](#2-policy-iteration-pi)\n",
    "   - 2.1 [Algorithm Overview and Definition](#21-algorithm-overview-and-definition)\n",
    "   - 2.2 [Algorithm Steps](#22-algorithm-steps)\n",
    "   - 2.3 [Implementation](#23-implementation)\n",
    "     - 2.3.1 [Environment Setup](#231-environment-setup)\n",
    "     - 2.3.2 [Policy Evaluation Phase](#232-policy-evaluation-phase)\n",
    "     - 2.3.3 [Policy Improvement Phase](#233-policy-improvement-phase)\n",
    "     - 2.3.4 [Complete Policy Iteration Algorithm](#234-complete-policy-iteration-algorithm)\n",
    "     - 2.3.5 [Visualization and Results](#235-visualization-and-results)\n",
    "   - 2.4 [Policy Iteration Properties](#24-policy-iteration-properties)\n",
    "     - 2.4.1 [Convergence Guarantees](#241-convergence-guarantees)\n",
    "     - 2.4.2 [Policy Improvement Theorem](#242-policy-improvement-theorem)\n",
    "     - 2.4.3 [Computational Complexity](#243-computational-complexity)\n",
    "   - 2.5 [Advantages and Disadvantages](#25-advantages-and-disadvantages)\n",
    "   - 2.6 [Example Results](#26-example-results)\n",
    "\n",
    "3. [Value Iteration](#3-value-iteration)\n",
    "   - 3.1 [Algorithm Overview and Definition](#31-algorithm-overview-and-definition)\n",
    "   - 3.2 [Mathematical Foundation](#32-mathematical-foundation)\n",
    "   - 3.3 [Algorithm Steps](#33-algorithm-steps)\n",
    "   - 3.4 [Implementation](#34-implementation)\n",
    "     - 3.4.1 [Environment Setup](#341-environment-setup)\n",
    "     - 3.4.2 [Core Value Iteration Functions](#342-core-value-iteration-functions)\n",
    "     - 3.4.3 [Complete Value Iteration Algorithm](#343-complete-value-iteration-algorithm)\n",
    "     - 3.4.4 [Policy Extraction](#344-policy-extraction)\n",
    "   - 3.5 [Convergence Analysis and Properties](#35-convergence-analysis-and-properties)\n",
    "     - 3.5.1 [Convergence Properties](#351-convergence-properties)\n",
    "     - 3.5.2 [Convergence Analysis Implementation](#352-convergence-analysis-implementation)\n",
    "     - 3.5.3 [Computational Complexity](#353-computational-complexity)\n",
    "   - 3.6 [Value Iteration vs Policy Iteration](#36-value-iteration-vs-policy-iteration)\n",
    "     - 3.6.1 [Detailed Comparison](#361-detailed-comparison)\n",
    "     - 3.6.2 [Performance Comparison Implementation](#362-performance-comparison-implementation)\n",
    "     - 3.6.3 [Intuitive Understanding](#363-intuitive-understanding)\n",
    "     - 3.6.4 [When to Use Each](#364-when-to-use-each)\n",
    "   - 3.7 [Advanced Topics](#37-advanced-topics)\n",
    "     - 3.7.1 [Modified Policy Iteration (Hybrid Approach)](#371-modified-policy-iteration-hybrid-approach)\n",
    "     - 3.7.2 [Visualization and Results](#372-visualization-and-results)\n",
    "   - 3.8 [Key Advantages and Disadvantages](#38-key-advantages-and-disadvantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2bc52",
   "metadata": {},
   "source": [
    "## **ðŸ”–1 Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86877bfb",
   "metadata": {},
   "source": [
    "* **Goal**: Find the **optimal policy** (Ï€\\*) that maximizes expected return in a Markov Decision Process (MDP).\n",
    "* Two key algorithms:\n",
    "\n",
    "  * **Policy Iteration (PI)** â†’ Iterative evaluation + improvement of a policy.\n",
    "  * **Value Iteration (VI)** â†’ A faster version that combines evaluation & improvement in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc936a5",
   "metadata": {},
   "source": [
    "# **ðŸ”–2 Policy Iteration ($PI$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bc1ec",
   "metadata": {},
   "source": [
    "## 2.1 **Algorithm Overview and Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e43da",
   "metadata": {},
   "source": [
    "**Policy Iteration** is a dynamic programming algorithm that finds the optimal policy by alternating between policy evaluation and policy improvement until convergence.\n",
    "\n",
    "> **Definition:** Policy Iteration finds the optimal policy through an iterative process of policy evaluation and improvement phases, guaranteed to converge to the optimal policy Ï€* in finite steps.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The algorithm is based on the **Policy Improvement Theorem**: If $Q^{\\pi}(s, \\pi'(s)) \\geq V^{\\pi}(s)$ for all states $s$, then policy $\\pi'$ is at least as good as policy $\\pi$.\n",
    "\n",
    "**Process Flow**: Initialize policy â†’ Evaluate policy â†’ Improve policy â†’ Until policy stops changing â†’ Optimal policy\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41245c0f",
   "metadata": {},
   "source": [
    "## 2.2 **Algorithm Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759895f",
   "metadata": {},
   "source": [
    "### Step 1: Initialize\n",
    "- Start with an arbitrary policy $\\pi_0$ (can be random)\n",
    "\n",
    "### Step 2: Policy Evaluation\n",
    "- Compute the **state-value function V(s)** under current policy Ï€:\n",
    "  $$V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} P(s'|s,a)[r + \\gamma V^{\\pi}(s')]$$\n",
    "- Iterate until values converge\n",
    "\n",
    "### Step 3: Policy Improvement  \n",
    "- For each state, pick the action that maximizes the Q-value:\n",
    "  $$\\pi'(s) = \\arg\\max_a Q^{\\pi}(s,a)$$\n",
    "- Update policy by acting greedily with respect to $V^{\\pi}$\n",
    "\n",
    "### Step 4: Convergence Check\n",
    "- If $\\pi' = \\pi$, stop (optimal policy found)\n",
    "- Otherwise, set $\\pi = \\pi'$ and repeat from Step 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e46b7",
   "metadata": {},
   "source": [
    "## 2.3 **Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec867c89",
   "metadata": {},
   "source": [
    "### 2.3.1 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "gamma = 0.9  # discount factor\n",
    "theta = 1e-6  # convergence threshold\n",
    "\n",
    "terminal_state = num_states - 1\n",
    "\n",
    "# Access transition probabilities\n",
    "P = env.unwrapped.P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8eb6a1",
   "metadata": {},
   "source": [
    "### 2.3.2 Policy Evaluation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821cbad",
   "metadata": {},
   "source": [
    "#### Basic State Value Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97ba8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_value(state, policy, V):\n",
    "    \"\"\"Computes the value of a state under the given policy.\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    value = 0.0\n",
    "    for prob, next_state, reward, terminated in P[state][action]:\n",
    "        value += prob * (reward + gamma * V[next_state])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cfc1fc",
   "metadata": {},
   "source": [
    "#### Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c89bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_iterative(policy, env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a policy by iteratively solving Bellman equation\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_V = np.zeros_like(V)\n",
    "        \n",
    "        # Update value for each non-terminal state\n",
    "        for state in range(env.observation_space.n - 1):\n",
    "            if state in policy:\n",
    "                action = policy[state]\n",
    "                expected_value = 0\n",
    "                \n",
    "                # Compute expected value over all possible transitions\n",
    "                transitions = env.unwrapped.P[state][action]\n",
    "                for prob, next_state, reward, is_terminal in transitions:\n",
    "                    if is_terminal:\n",
    "                        expected_value += prob * reward\n",
    "                    else:\n",
    "                        expected_value += prob * (reward + gamma * V[next_state])\n",
    "                \n",
    "                new_V[state] = expected_value\n",
    "        \n",
    "        # Check for convergence\n",
    "        delta = np.max(np.abs(new_V - V))\n",
    "        V = new_V\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b46c0c",
   "metadata": {},
   "source": [
    "#### Exact Policy Evaluation (Linear Algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "931444b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_exact(policy, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Solve policy evaluation exactly using linear algebra\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n - 1  # Exclude terminal state\n",
    "    \n",
    "    # Build system of linear equations: V = R + Î³PV\n",
    "    # Rearrange to: (I - Î³P)V = R\n",
    "    \n",
    "    I = np.eye(n_states)\n",
    "    P_matrix = np.zeros((n_states, n_states))\n",
    "    R = np.zeros(n_states)\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        if state in policy:\n",
    "            action = policy[state]\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if not is_terminal and next_state < n_states:\n",
    "                    P_matrix[state, next_state] += prob\n",
    "                R[state] += prob * reward\n",
    "    \n",
    "    # Solve linear system\n",
    "    A = I - gamma * P_matrix\n",
    "    V_exact = np.linalg.solve(A, R)\n",
    "    \n",
    "    # Add terminal state value\n",
    "    V_complete = np.zeros(env.observation_space.n)\n",
    "    V_complete[:n_states] = V_exact\n",
    "    \n",
    "    return V_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63d785",
   "metadata": {},
   "source": [
    "### 2.3.3 Policy Improvement Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da6aa2",
   "metadata": {},
   "source": [
    "#### Q-Value Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f9ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value(state, action, V, env, gamma=0.9):\n",
    "    \"\"\"Compute Q(s,a) = sum over next states [ P(s'|s,a) * (R + gamma*V(s')) ]\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    q = 0.0\n",
    "    for prob, next_state, reward, terminated in P[state][action]:\n",
    "        if terminated:\n",
    "            q += prob * reward\n",
    "        else:\n",
    "            q += prob * (reward + gamma * V[next_state])\n",
    "    return q\n",
    "\n",
    "def compute_q_values_from_v(V, env, gamma=0.9):\n",
    "    \"\"\"Compute Q-values from state values\"\"\"\n",
    "    Q = {}\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):\n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    q_value += prob * reward\n",
    "                else:\n",
    "                    q_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            Q[(state, action)] = q_value\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b317c14",
   "metadata": {},
   "source": [
    "#### Policy Improvement Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13938cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Improve policy by acting greedily with respect to value function\n",
    "    \"\"\"\n",
    "    improved_policy = {}\n",
    "    policy_stable = True\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal state\n",
    "        # Compute action values for all possible actions\n",
    "        action_values = []\n",
    "        for action in range(env.action_space.n):\n",
    "            action_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    action_value += prob * reward\n",
    "                else:\n",
    "                    action_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            action_values.append(action_value)\n",
    "        \n",
    "        # Select action with highest value (greedy policy)\n",
    "        best_action = np.argmax(action_values)\n",
    "        improved_policy[state] = best_action\n",
    "    \n",
    "    return improved_policy, policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6bbfc",
   "metadata": {},
   "source": [
    "### 2.3.4 Complete Policy Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8517fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Policy Iteration...\n",
      "Initial policy: {0: 0, 1: 3, 2: 2, 3: 0, 4: 1, 5: 1, 6: 0, 7: 2, 8: 3, 9: 3, 10: 2, 11: 2, 12: 1, 13: 1, 14: 1}\n",
      "\n",
      "Iteration 1: Policy Evaluation\n",
      "Iteration 1: Policy Improvement\n",
      "Policy iteration converged after 1 iterations\n",
      "\n",
      "Optimal Policy: {0: 0, 1: 3, 2: 2, 3: 0, 4: 1, 5: 1, 6: 0, 7: 2, 8: 3, 9: 3, 10: 2, 11: 2, 12: 1, 13: 1, 14: 1}\n",
      "Optimal State Values: [0.01177125 0.0207412  0.03662707 0.01569704 0.01569704 0.\n",
      " 0.06976699 0.         0.03662707 0.06976699 0.19593003 0.\n",
      " 0.         0.24999999 0.58333332 0.        ]\n",
      "\n",
      "Policy Evolution Analysis:\n",
      "==================================================\n",
      "Iteration 0: {0: 0, 1: 3, 2: 2, 3: 0, 4: 1, 5: 1, 6: 0, 7: 2, 8: 3, 9: 3, 10: 2, 11: 2, 12: 1, 13: 1, 14: 1}\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration_complete(env, gamma=0.9, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Complete policy iteration algorithm\n",
    "    \"\"\"\n",
    "    # Initialize with random policy\n",
    "    policy = {state: np.random.choice(env.action_space.n) \n",
    "              for state in range(env.observation_space.n - 1)}\n",
    "    \n",
    "    iteration = 0\n",
    "    policy_history = []\n",
    "    \n",
    "    print(\"Starting Policy Iteration...\")\n",
    "    print(f\"Initial policy: {policy}\")\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        # Policy Evaluation\n",
    "        print(f\"\\nIteration {iteration + 1}: Policy Evaluation\")\n",
    "        V = policy_evaluation_iterative(policy, env, gamma)\n",
    "        \n",
    "        # Policy Improvement  \n",
    "        print(f\"Iteration {iteration + 1}: Policy Improvement\")\n",
    "        improved_policy, policy_stable = policy_improvement(V, env, gamma)\n",
    "        \n",
    "        # Store policy for analysis\n",
    "        policy_history.append(policy.copy())\n",
    "        \n",
    "        # Check for convergence\n",
    "        if policy_stable or improved_policy == policy:\n",
    "            print(f\"Policy iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        policy = improved_policy\n",
    "        iteration += 1\n",
    "    \n",
    "    return policy, V, policy_history\n",
    "\n",
    "def analyze_policy_convergence(policy_history):\n",
    "    \"\"\"\n",
    "    Analyze how policy changes during iteration\n",
    "    \"\"\"\n",
    "    print(\"\\nPolicy Evolution Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, policy in enumerate(policy_history):\n",
    "        print(f\"Iteration {i}: {policy}\")\n",
    "        \n",
    "        if i > 0:\n",
    "            changes = sum(1 for state in policy.keys() \n",
    "                         if policy[state] != policy_history[i-1][state])\n",
    "            print(f\"  States changed: {changes}\")\n",
    "\n",
    "# Run complete policy iteration\n",
    "optimal_policy, optimal_V, history = policy_iteration_complete(env, gamma=0.9)\n",
    "print(f\"\\nOptimal Policy: {optimal_policy}\")\n",
    "print(f\"Optimal State Values: {optimal_V}\")\n",
    "analyze_policy_convergence(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6deac94",
   "metadata": {},
   "source": [
    "### 2.3.5 Visualization and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a0e214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Optimal Policy (per state):\n",
      "[[0 3 2 0]\n",
      " [1 1 0 2]\n",
      " [3 3 2 2]\n",
      " [1 1 1 0]]\n",
      "\n",
      "âœ… Optimal Value Function:\n",
      "[[0.01177125 0.0207412  0.03662707 0.01569704]\n",
      " [0.01569704 0.         0.06976699 0.        ]\n",
      " [0.03662707 0.06976699 0.19593003 0.        ]\n",
      " [0.         0.24999999 0.58333332 0.        ]]\n",
      "\n",
      "âœ… Policy with arrows:\n",
      "[['â†' 'â†‘' 'â†’' 'â†']\n",
      " ['â†“' 'â†“' 'â†' 'â†’']\n",
      " ['â†‘' 'â†‘' 'â†’' 'â†’']\n",
      " ['â†“' 'â†“' 'â†“' 'â†']]\n"
     ]
    }
   ],
   "source": [
    "# Pretty print results for FrozenLake (4x4 grid)\n",
    "print(\"âœ… Optimal Policy (per state):\")\n",
    "policy_array = np.array([optimal_policy.get(i, 0) for i in range(16)])\n",
    "print(policy_array.reshape((4, 4)))\n",
    "\n",
    "print(\"\\nâœ… Optimal Value Function:\")\n",
    "print(optimal_V.reshape((4, 4)))\n",
    "\n",
    "# Action mapping for better visualization\n",
    "action_map = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}\n",
    "print(\"\\nâœ… Policy with arrows:\")\n",
    "policy_arrows = np.array([action_map[optimal_policy.get(i, 0)] for i in range(16)]).reshape((4, 4))\n",
    "print(policy_arrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59942adb",
   "metadata": {},
   "source": [
    "## 2.4 **Policy Iteration Properties**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b30f96",
   "metadata": {},
   "source": [
    "### 2.4.1 Convergence Guarantees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff04071",
   "metadata": {},
   "source": [
    "**Finite Convergence**: Policy iteration converges in finite steps\n",
    "- At most $|A|^{|S|}$ possible deterministic policies\n",
    "- Each iteration either improves policy or finds optimal policy\n",
    "- Strict improvement until optimality reached\n",
    "\n",
    "**Optimality**: Converges to optimal policy $\\pi^*$\n",
    "- Final policy satisfies Bellman optimality equation\n",
    "- No further improvement possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ae2be",
   "metadata": {},
   "source": [
    "### 2.4.2 Policy Improvement Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582aeb6",
   "metadata": {},
   "source": [
    "For any policy $\\pi$ and state $s$, if we define a new policy $\\pi'$ such that:\n",
    "$$\\pi'(s) = \\arg\\max_a Q^{\\pi}(s,a)$$\n",
    "\n",
    "Then $V^{\\pi'}(s) \\geq V^{\\pi}(s)$ for all states $s$.\n",
    "\n",
    "**Proof Intuition**: \n",
    "- Taking the best action according to current Q-values can only improve or maintain performance\n",
    "- If improvement occurs in any state, it propagates through the value function\n",
    "- If no improvement occurs anywhere, we have found an optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa5fae",
   "metadata": {},
   "source": [
    "### 2.4.3 Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8f455",
   "metadata": {},
   "source": [
    "**Per Iteration**: \n",
    "- Policy Evaluation: O(|S|Â³) for exact solution or O(|S|Â²) per sweep for iterative\n",
    "- Policy Improvement: O(|S||A|)\n",
    "\n",
    "**Total Complexity**: O(k|S|Â³) where k is number of iterations\n",
    "- k is typically small in practice (much less than $|A|^{|S|}$)\n",
    "- Often converges in just a few iterations\n",
    "\n",
    "**Time Complexity**: $O(|S|Â²|A|)$ per iteration, where |S| is number of states and |A| is number of actions.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22068770",
   "metadata": {},
   "source": [
    "## 2.5 **Advantages and Disadvantages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72da0cc",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "- **Guaranteed convergence** to optimal policy\n",
    "- **Often fast convergence** in practice\n",
    "- **Clear separation** of evaluation and improvement phases\n",
    "- **Finds shortest path** to goal (optimal behavior)\n",
    "\n",
    "### Disadvantages:\n",
    "- **Requires exact policy evaluation** (computationally expensive)\n",
    "- **May be slow** for large state spaces\n",
    "- **Requires complete model** of environment (transition probabilities)\n",
    "- **Memory intensive** for storing complete value functions\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567e504",
   "metadata": {},
   "source": [
    "## 2.6 **Example Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486ca1b",
   "metadata": {},
   "source": [
    "For a FrozenLake environment, typical optimal policy might look like:\n",
    "```\n",
    "Optimal Policy: {0: 2, 1: 2, 2: 1, 3: 1, 4: 2, 5: 1, 6: 2, 7: 2}\n",
    "Optimal State Values: {0: 7, 1: 8, 2: 9, 3: 7, 4: 9, 5: 10, 6: 8, 7: 10, 8: 0}\n",
    "```\n",
    "\n",
    "The algorithm typically finds the shortest path to the goal state, demonstrating significant improvement over random initial policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338fb0c",
   "metadata": {},
   "source": [
    "# **ðŸ”–3. Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7012f8ba",
   "metadata": {},
   "source": [
    "## 3.1 **Algorithm Overview and Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33320170",
   "metadata": {},
   "source": [
    "**Value Iteration** combines policy evaluation and policy improvement in a single operation, directly computing the optimal value function through repeated application of the Bellman optimality operator.\n",
    "\n",
    "> **Definition:** Value Iteration combines policy evaluation and improvement in one step. It computes the optimal state-value function and derives the policy from it, rather than maintaining an explicit policy throughout the process.\n",
    "\n",
    "**Key Insight**: Instead of fully evaluating a policy (like Policy Iteration), perform only one sweep of value updates followed by implicit policy improvement, making it more computationally efficient per iteration.\n",
    "\n",
    "### Core Idea\n",
    "- **Speeds up** policy iteration by combining evaluation & improvement in a single update\n",
    "- **Direct approach**: Updates value estimates directly toward optimality\n",
    "- **Implicit policy**: Policy is derived from values rather than maintained explicitly\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093cf9c5",
   "metadata": {},
   "source": [
    "## 3.2 **Mathematical Foundation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed60568",
   "metadata": {},
   "source": [
    "### Bellman Optimality Equation\n",
    "Value iteration is based on the **Bellman Optimality Equation**:\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "### Bellman Optimality Operator\n",
    "The Bellman optimality operator $T^*$ is defined as:\n",
    "$$(T^*V)(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V(s')]$$\n",
    "\n",
    "#### Key Mathematical Properties:\n",
    "\n",
    "**Contraction Mapping**: When $\\gamma < 1$, $T^*$ is a contraction with modulus $\\gamma$\n",
    "- $\\|T^*V_1 - T^*V_2\\|_{\\infty} \\leq \\gamma \\|V_1 - V_2\\|_{\\infty}$\n",
    "- Guarantees unique fixed point (optimal value function)\n",
    "- Ensures geometric convergence rate\n",
    "\n",
    "**Monotonicity**: If $V_1(s) \\leq V_2(s)$ for all $s$, then $(T^*V_1)(s) \\leq (T^*V_2)(s)$\n",
    "- Preserves ordering between value functions\n",
    "- Ensures convergence from any initialization\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef02e1",
   "metadata": {},
   "source": [
    "## 3.3 **Algorithm Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5ff68",
   "metadata": {},
   "source": [
    "### Step 1: Initialize\n",
    "- Set $V_0(s) = 0$ for all states $s$\n",
    "\n",
    "### Step 2: Iterative Value Update\n",
    "For each state, apply the Bellman optimality operator:\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s',r} P(s'|s,a)[r + \\gamma V_k(s')]$$\n",
    "\n",
    "### Step 3: Policy Derivation (Implicit)\n",
    "Policy is implicitly defined as:\n",
    "$$\\pi(s) = \\arg\\max_a \\sum_{s',r} P(s'|s,a)[r + \\gamma V(s')]$$\n",
    "\n",
    "### Step 4: Convergence Check\n",
    "- Continue until value updates are below a threshold: $\\max_s |V_{k+1}(s) - V_k(s)| < \\theta$\n",
    "- Extract final policy using policy extraction\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15839234",
   "metadata": {},
   "source": [
    "## 3.4 **Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b60b9",
   "metadata": {},
   "source": [
    "### 3.4.1 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e2bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode=None)\n",
    "mdp_env = env.unwrapped  # access the underlying MDP to get .P\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "terminal_state = num_states - 1  # Goal state in FrozenLake\n",
    "gamma = 0.9  # Discount factor\n",
    "theta = 1e-6  # Convergence threshold\n",
    "\n",
    "# Access transition probabilities\n",
    "P = env.unwrapped.P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857cf91f",
   "metadata": {},
   "source": [
    "### 3.4.2 Core Value Iteration Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628021ac",
   "metadata": {},
   "source": [
    "#### Bellman Optimality Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e1797e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimality_update(V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Single step of Bellman optimality operator\n",
    "    \"\"\"\n",
    "    new_V = np.zeros_like(V)\n",
    "    policy = {}\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal state\n",
    "        action_values = []\n",
    "        \n",
    "        # Compute Q-value for each action\n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    q_value += prob * reward\n",
    "                else:\n",
    "                    q_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            action_values.append(q_value)\n",
    "        \n",
    "        # Take maximum over actions\n",
    "        max_value = max(action_values)\n",
    "        max_action = np.argmax(action_values)\n",
    "        \n",
    "        new_V[state] = max_value\n",
    "        policy[state] = max_action\n",
    "    \n",
    "    return new_V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9fed5a",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e00a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_action_and_value(state, V, env, gamma=0.9):\n",
    "    \"\"\"Helper function to get optimal action and value for a state.\"\"\"\n",
    "    Q_values = []\n",
    "    for action in range(env.action_space.n):\n",
    "        q_val = 0\n",
    "        for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
    "            q_val += prob * (reward + gamma * V[next_state])\n",
    "        Q_values.append(q_val)\n",
    "    \n",
    "    max_action = int(np.argmax(Q_values))\n",
    "    max_q_value = Q_values[max_action]\n",
    "    return max_action, max_q_value\n",
    "\n",
    "def compute_action_value(state, action, V, env, gamma=0.9):\n",
    "    \"\"\"Compute Q(s,a) using current value estimates\"\"\"\n",
    "    q_value = 0\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    \n",
    "    for prob, next_state, reward, is_terminal in transitions:\n",
    "        if is_terminal:\n",
    "            q_value += prob * reward\n",
    "        else:\n",
    "            q_value += prob * (reward + gamma * V[next_state])\n",
    "    \n",
    "    return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381dc3f8",
   "metadata": {},
   "source": [
    "### 3.4.3 Complete Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be09ff",
   "metadata": {},
   "source": [
    "#### Basic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b444b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, threshold=1e-3, max_iterations=1000):\n",
    "    \"\"\"Basic value iteration algorithm.\"\"\"\n",
    "    # Initialize\n",
    "    V = {state: 0 for state in range(env.observation_space.n)}\n",
    "    policy = {state: 0 for state in range(env.observation_space.n - 1)}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_V = {state: 0 for state in range(env.observation_space.n)}\n",
    "        \n",
    "        for state in range(env.observation_space.n - 1):  # Exclude terminal\n",
    "            if state == terminal_state:\n",
    "                new_V[state] = 0\n",
    "                continue\n",
    "            \n",
    "            # Compute Q-values for all actions\n",
    "            Q_values = []\n",
    "            for action in range(env.action_space.n):\n",
    "                q_val = 0\n",
    "                for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
    "                    q_val += prob * (reward + gamma * V[next_state])\n",
    "                Q_values.append(q_val)\n",
    "            \n",
    "            # Take maximum\n",
    "            max_q_value = max(Q_values)\n",
    "            max_action = int(np.argmax(Q_values))\n",
    "            new_V[state] = max_q_value\n",
    "            policy[state] = max_action\n",
    "        \n",
    "        # Check convergence\n",
    "        if all(abs(new_V[s] - V[s]) < threshold for s in range(env.observation_space.n)):\n",
    "            print(f\"Value iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        V = new_V\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150e87c",
   "metadata": {},
   "source": [
    "#### Enhanced Implementation with Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f7b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_enhanced(env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Enhanced value iteration algorithm with detailed tracking\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    print(\"Starting Value Iteration...\")\n",
    "    convergence_history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Apply Bellman optimality operator\n",
    "        new_V, current_policy = bellman_optimality_update(V, env, gamma)\n",
    "        \n",
    "        # Check for convergence\n",
    "        delta = np.max(np.abs(new_V - V))\n",
    "        convergence_history.append(delta)\n",
    "        V = new_V\n",
    "        \n",
    "        if iteration % 10 == 0:  # Print progress every 10 iterations\n",
    "            print(f\"Iteration {iteration}: max change = {delta:.6f}\")\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Extract final policy\n",
    "    _, final_policy = bellman_optimality_update(V, env, gamma)\n",
    "    \n",
    "    return V, final_policy, convergence_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57f921",
   "metadata": {},
   "source": [
    "### 3.4.4 Policy Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07913785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy_from_values(V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Extract optimal policy from optimal value function\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal\n",
    "        action_values = []\n",
    "        \n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    q_value += prob * reward\n",
    "                else:\n",
    "                    q_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            action_values.append(q_value)\n",
    "        \n",
    "        # Select action with maximum Q-value\n",
    "        policy[state] = np.argmax(action_values)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def verify_policy_optimality(policy, V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Verify that extracted policy is optimal\n",
    "    \"\"\"\n",
    "    print(\"Policy Optimality Verification:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    violations = 0\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):\n",
    "        # Compute value of current policy action\n",
    "        policy_action = policy[state]\n",
    "        policy_q = compute_action_value(state, policy_action, V, env, gamma)\n",
    "        \n",
    "        # Compute maximum Q-value over all actions\n",
    "        max_q = max(compute_action_value(state, action, V, env, gamma) \n",
    "                   for action in range(env.action_space.n))\n",
    "        \n",
    "        # Check optimality condition\n",
    "        if abs(policy_q - max_q) > 1e-6:\n",
    "            violations += 1\n",
    "            print(f\"State {state}: Policy Q={policy_q:.6f}, Max Q={max_q:.6f}\")\n",
    "    \n",
    "    if violations == 0:\n",
    "        print(\"âœ“ Policy is optimal (satisfies Bellman optimality)\")\n",
    "    else:\n",
    "        print(f\"âœ— Policy has {violations} optimality violations\")\n",
    "    \n",
    "    return violations == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5f0b6",
   "metadata": {},
   "source": [
    "## 3.5 **Convergence Analysis and Properties**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23922122",
   "metadata": {},
   "source": [
    "### 3.5.1 Convergence Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22159a3",
   "metadata": {},
   "source": [
    "**Geometric Convergence**: Value iteration converges at rate $\\gamma$\n",
    "- Error decreases by factor $\\gamma$ each iteration\n",
    "- Faster convergence for smaller discount factors\n",
    "\n",
    "**Asymptotic Convergence**: Unlike Policy Iteration's finite convergence\n",
    "- Values approach $V^*$ asymptotically\n",
    "- Practical convergence when changes fall below threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b499d",
   "metadata": {},
   "source": [
    "### 3.5.2 Convergence Analysis Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7db12943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_convergence_rate(env, gamma=0.9, true_V=None):\n",
    "    \"\"\"\n",
    "    Analyze convergence rate of value iteration\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    errors = []\n",
    "    \n",
    "    if true_V is None:\n",
    "        # Compute true optimal values using many iterations\n",
    "        true_V, _, _ = value_iteration_enhanced(env, gamma, theta=1e-12, max_iterations=10000)\n",
    "    \n",
    "    print(\"Convergence Analysis:\")\n",
    "    print(\"Iteration | Max Error | Convergence Rate\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for iteration in range(50):\n",
    "        V, _ = bellman_optimality_update(V, env, gamma)\n",
    "        error = np.max(np.abs(V - true_V))\n",
    "        errors.append(error)\n",
    "        \n",
    "        # Compute convergence rate\n",
    "        if iteration > 0:\n",
    "            rate = errors[iteration] / errors[iteration-1] if errors[iteration-1] > 0 else 0\n",
    "        else:\n",
    "            rate = 0\n",
    "        \n",
    "        if iteration % 5 == 0:\n",
    "            print(f\"{iteration:9} | {error:9.6f} | {rate:9.6f}\")\n",
    "    \n",
    "    # Theoretical vs empirical convergence rate\n",
    "    theoretical_rate = gamma\n",
    "    empirical_rate = np.mean([errors[i]/errors[i-1] for i in range(5, 20) if errors[i-1] > 1e-10])\n",
    "    \n",
    "    print(f\"\\nTheoretical convergence rate: {theoretical_rate:.6f}\")\n",
    "    print(f\"Empirical convergence rate: {empirical_rate:.6f}\")\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5460fd",
   "metadata": {},
   "source": [
    "### 3.5.3 Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36763248",
   "metadata": {},
   "source": [
    "**Per Iteration**: O(|S|Â²|A|)\n",
    "- For each state: compute Q-value for each action\n",
    "- Each Q-value computation: sum over next states\n",
    "\n",
    "**Total Complexity**: O(k|S|Â²|A|) where k is number of iterations\n",
    "- k depends on desired accuracy and discount factor\n",
    "- Typically many more iterations than Policy Iteration\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41773a69",
   "metadata": {},
   "source": [
    "## 3.6 **Value Iteration vs Policy Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557dcb6",
   "metadata": {},
   "source": [
    "### 3.6.1 Detailed Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa51718",
   "metadata": {},
   "source": [
    "| Aspect | **Policy Iteration** | **Value Iteration** |\n",
    "|--------|---------------------|-------------------|\n",
    "| **Approach** | Two clear steps: (1) **Policy Evaluation** â€“ compute $V^Ï€$, (2) **Policy Improvement** â€“ update policy greedily | Blends evaluation and improvement into **one step** using Bellman optimality equation |\n",
    "| **Convergence** | **Finite iterations** â€“ guaranteed to find optimal policy in finite steps | **Asymptotic convergence** â€“ values approach $V^*$ geometrically |\n",
    "| **Per Iteration Cost** | **High** â€“ O(|S|Â³) for exact policy evaluation | **Low** â€“ O(|S|Â²|A|) for value updates |\n",
    "| **Total Iterations** | **Fewer** â€“ makes big jumps with full policy evaluation | **More** â€“ small incremental improvements |\n",
    "| **Memory** | Must store **explicit policy** alongside value function | Policy **implicitly derived** from values |\n",
    "| **Practical Use** | Best for **small/medium state spaces** | Preferred for **large/complex environments** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198ffed",
   "metadata": {},
   "source": [
    "### 3.6.2 Performance Comparison Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "842b2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_policy_iteration(env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Compare value iteration results with policy iteration\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPARISON: Value Iteration vs Policy Iteration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Run value iteration\n",
    "    print(\"\\n1. Running Value Iteration...\")\n",
    "    start_time = time.time()\n",
    "    V_vi, policy_vi, history_vi = value_iteration_enhanced(env, gamma)\n",
    "    vi_time = time.time() - start_time\n",
    "    \n",
    "    # Run policy iteration (assuming policy_iteration_complete exists)\n",
    "    print(\"\\n2. Running Policy Iteration...\")\n",
    "    start_time = time.time()\n",
    "    policy_pi, V_pi, history_pi = policy_iteration_complete(env, gamma)\n",
    "    pi_time = time.time() - start_time\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n3. Comparing Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Compare policies\n",
    "    policy_match = all(policy_vi.get(s) == policy_pi.get(s) for s in range(env.observation_space.n - 1))\n",
    "    print(f\"Policies identical: {policy_match}\")\n",
    "    \n",
    "    # Compare values\n",
    "    value_diff = np.max(np.abs(V_vi - V_pi))\n",
    "    print(f\"Maximum value difference: {value_diff:.8f}\")\n",
    "    \n",
    "    # Compare performance\n",
    "    print(f\"Value Iteration time: {vi_time:.4f}s, iterations: {len(history_vi)}\")\n",
    "    print(f\"Policy Iteration time: {pi_time:.4f}s, iterations: {len(history_pi)}\")\n",
    "    \n",
    "    print(f\"\\nValue Iteration Policy: {policy_vi}\")\n",
    "    print(f\"Policy Iteration Policy: {policy_pi}\")\n",
    "    \n",
    "    return V_vi, policy_vi, V_pi, policy_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bf452",
   "metadata": {},
   "source": [
    "### 3.6.3 Intuitive Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17736e27",
   "metadata": {},
   "source": [
    "**Policy Iteration = \"Think hard, act big\"**\n",
    "- Each step is expensive but fewer steps needed\n",
    "- Complete policy evaluation ensures big improvements\n",
    "\n",
    "**Value Iteration = \"Think fast, act small\"**  \n",
    "- Each step is cheap but more steps required\n",
    "- Incremental improvements toward optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa06f183",
   "metadata": {},
   "source": [
    "### 3.6.4 When to Use Each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d6ee6",
   "metadata": {},
   "source": [
    "**Value Iteration**:\n",
    "- **Large action spaces** â€“ cheaper per iteration\n",
    "- **Approximate solutions acceptable** â€“ can stop early\n",
    "- **Limited computational memory** â€“ no policy storage\n",
    "- **Online/real-time applications** â€“ faster iterations\n",
    "\n",
    "**Policy Iteration**:\n",
    "- **Small to medium problems** â€“ exact evaluation feasible\n",
    "- **Exact solutions required** â€“ finite convergence\n",
    "- **Batch processing scenarios** â€“ can afford expensive iterations\n",
    "- **Policy stability important** â€“ explicit policy tracking\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba0197",
   "metadata": {},
   "source": [
    "## 3.7. **Advanced Topics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8b127",
   "metadata": {},
   "source": [
    "### 3.7.1 Modified Policy Iteration (Hybrid Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61e8ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_policy_iteration(env, gamma=0.9, k=10, theta=1e-6, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Modified policy iteration: partial policy evaluation + improvement\n",
    "    Bridges gap between Policy Iteration and Value Iteration\n",
    "    \"\"\"\n",
    "    # Initialize policy randomly\n",
    "    policy = {state: np.random.choice(env.action_space.n) \n",
    "              for state in range(env.observation_space.n - 1)}\n",
    "    \n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Partial policy evaluation (k steps instead of full convergence)\n",
    "        for _ in range(k):\n",
    "            new_V = np.zeros_like(V)\n",
    "            \n",
    "            for state in range(env.observation_space.n - 1):\n",
    "                if state in policy:\n",
    "                    action = policy[state]\n",
    "                    expected_value = 0\n",
    "                    \n",
    "                    transitions = env.unwrapped.P[state][action]\n",
    "                    for prob, next_state, reward, is_terminal in transitions:\n",
    "                        if is_terminal:\n",
    "                            expected_value += prob * reward\n",
    "                        else:\n",
    "                            expected_value += prob * (reward + gamma * V[next_state])\n",
    "                    \n",
    "                    new_V[state] = expected_value\n",
    "            \n",
    "            V = new_V\n",
    "        \n",
    "        # Policy improvement\n",
    "        improved_policy, policy_stable = policy_improvement(V, env, gamma)\n",
    "        \n",
    "        if policy_stable:\n",
    "            print(f\"Modified policy iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        policy = improved_policy\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1b10a",
   "metadata": {},
   "source": [
    "### 3.7.2 Visualization and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a772b71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE ALGORITHM COMPARISON\n",
      "============================================================\n",
      "\n",
      "1. Value Iteration\n",
      "Starting Value Iteration...\n",
      "Iteration 0: max change = 0.333333\n",
      "Iteration 10: max change = 0.008062\n",
      "Iteration 20: max change = 0.001935\n",
      "Iteration 30: max change = 0.000536\n",
      "Iteration 40: max change = 0.000139\n",
      "Iteration 50: max change = 0.000036\n",
      "Iteration 60: max change = 0.000009\n",
      "Iteration 70: max change = 0.000002\n",
      "Value iteration converged after 78 iterations\n",
      "\n",
      "2. Policy Iteration\n",
      "Starting Policy Iteration...\n",
      "Initial policy: {0: 1, 1: 2, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 2, 8: 0, 9: 3, 10: 2, 11: 0, 12: 1, 13: 1, 14: 2}\n",
      "\n",
      "Iteration 1: Policy Evaluation\n",
      "Iteration 1: Policy Improvement\n",
      "Policy iteration converged after 1 iterations\n",
      "\n",
      "3. Modified Policy Iteration\n",
      "Modified policy iteration converged after 1 iterations\n",
      "\n",
      "Value Iteration Policy:     {0: np.int64(0), 1: np.int64(3), 2: np.int64(0), 3: np.int64(3), 4: np.int64(0), 5: np.int64(0), 6: np.int64(0), 7: np.int64(0), 8: np.int64(3), 9: np.int64(1), 10: np.int64(0), 11: np.int64(0), 12: np.int64(0), 13: np.int64(2), 14: np.int64(1)}\n",
      "Policy Iteration Policy:    {0: 1, 1: 2, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 2, 8: 0, 9: 3, 10: 2, 11: 0, 12: 1, 13: 1, 14: 2}\n",
      "Modified Policy Iteration:  {0: 1, 1: 0, 2: 3, 3: 3, 4: 2, 5: 2, 6: 3, 7: 0, 8: 1, 9: 0, 10: 1, 11: 3, 12: 1, 13: 2, 14: 2}\n",
      "\n",
      "âœ… Value Iteration Results:\n",
      "Policy (per state):\n",
      "[[0 3 0 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n",
      "Value Function:\n",
      "[[0.06888466 0.06140928 0.07440591 0.05580308]\n",
      " [0.09184887 0.         0.11220613 0.        ]\n",
      " [0.14543176 0.24749386 0.29961541 0.        ]\n",
      " [0.         0.37993367 0.63901898 0.        ]]\n",
      "Policy with arrows:\n",
      "[['â†' 'â†‘' 'â†' 'â†‘']\n",
      " ['â†' 'â†' 'â†' 'â†']\n",
      " ['â†‘' 'â†“' 'â†' 'â†']\n",
      " ['â†' 'â†’' 'â†“' 'â†']]\n"
     ]
    }
   ],
   "source": [
    "# Run and compare all algorithms\n",
    "def run_comprehensive_comparison(env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Run all three algorithms and compare results\n",
    "    \"\"\"\n",
    "    print(\"COMPREHENSIVE ALGORITHM COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run all three methods\n",
    "    print(\"\\n1. Value Iteration\")\n",
    "    V_vi, policy_vi, _ = value_iteration_enhanced(env, gamma)\n",
    "    \n",
    "    print(\"\\n2. Policy Iteration\") \n",
    "    policy_pi, V_pi, _ = policy_iteration_complete(env, gamma)\n",
    "    \n",
    "    print(\"\\n3. Modified Policy Iteration\")\n",
    "    policy_mpi, V_mpi = modified_policy_iteration(env, gamma, k=5)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nValue Iteration Policy:     {policy_vi}\")\n",
    "    print(f\"Policy Iteration Policy:    {policy_pi}\")  \n",
    "    print(f\"Modified Policy Iteration:  {policy_mpi}\")\n",
    "    \n",
    "    # Pretty print for FrozenLake (4x4 grid)\n",
    "    if env.observation_space.n == 16:  # FrozenLake 4x4\n",
    "        print(\"\\nâœ… Value Iteration Results:\")\n",
    "        policy_array = np.array([policy_vi.get(i, 0) for i in range(16)])\n",
    "        print(\"Policy (per state):\")\n",
    "        print(policy_array.reshape((4, 4)))\n",
    "        \n",
    "        print(\"Value Function:\")\n",
    "        print(V_vi.reshape((4, 4)))\n",
    "        \n",
    "        # Action mapping for visualization\n",
    "        action_map = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}\n",
    "        print(\"Policy with arrows:\")\n",
    "        policy_arrows = np.array([action_map[policy_vi.get(i, 0)] for i in range(16)]).reshape((4, 4))\n",
    "        print(policy_arrows)\n",
    "    \n",
    "    return V_vi, policy_vi, V_pi, policy_pi, V_mpi, policy_mpi\n",
    "\n",
    "# Execute comprehensive comparison\n",
    "results = run_comprehensive_comparison(env, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce057dd",
   "metadata": {},
   "source": [
    "## 3.8 **Key Advantages and Disadvantages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d1622",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "- **Computationally efficient per iteration** â€“ $O(|S|Â²|A|)$ vs $O(|S|Â³)$\n",
    "- **Memory efficient** â€“ no explicit policy storage required\n",
    "- **Flexible stopping** â€“ can terminate early for approximate solutions\n",
    "- **Scalable** â€“ works well with large state spaces\n",
    "- **Same optimal result** as Policy Iteration\n",
    "\n",
    "### Disadvantages:\n",
    "- **More total iterations** required for convergence\n",
    "- **Asymptotic convergence** â€“ never truly reaches optimality\n",
    "- **Less intuitive** â€“ policy changes implicitly\n",
    "- **Threshold dependent** â€“ convergence criteria affects solution quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
