{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9013b754",
   "metadata": {},
   "source": [
    "# **‚ú®Policy & Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127651d",
   "metadata": {},
   "source": [
    "##  üìëTabel of content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e5ff3",
   "metadata": {},
   "source": [
    "> ### **Initialize policy** $\\longrightarrow$ **Evaluate policy** $\\longleftrightarrow$ **Improve policy** $\\longrightarrow$ **Optimal policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2bc52",
   "metadata": {},
   "source": [
    "## **üîñ1 Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86877bfb",
   "metadata": {},
   "source": [
    "* **Goal**: Find the **optimal policy** (œÄ\\*) that maximizes expected return in a Markov Decision Process (MDP).\n",
    "* Two key algorithms:\n",
    "\n",
    "  * **Policy Iteration (PI)** ‚Üí Iterative evaluation + improvement of a policy.\n",
    "  * **Value Iteration (VI)** ‚Üí A faster version that combines evaluation & improvement in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc936a5",
   "metadata": {},
   "source": [
    "# **üîñ2 Policy Iteration ($PI$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582aeb6",
   "metadata": {},
   "source": [
    "## 1. Algorithm Overview and Definition\n",
    "\n",
    "**Policy Iteration** is a dynamic programming algorithm that finds the optimal policy by alternating between policy evaluation and policy improvement until convergence.\n",
    "\n",
    "> **Definition:** Policy Iteration finds the optimal policy through an iterative process of policy evaluation and improvement phases, guaranteed to converge to the optimal policy œÄ* in finite steps.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The algorithm is based on the **Policy Improvement Theorem**: If $Q^{\\pi}(s, \\pi'(s)) \\geq V^{\\pi}(s)$ for all states $s$, then policy $\\pi'$ is at least as good as policy $\\pi$.\n",
    "\n",
    "**Process Flow**: Initialize policy ‚Üí Evaluate policy ‚Üí Improve policy ‚Üí Until policy stops changing ‚Üí Optimal policy\n",
    "\n",
    "***\n",
    "\n",
    "## 2. Algorithm Steps\n",
    "\n",
    "### Step 1: Initialize\n",
    "- Start with an arbitrary policy $\\pi_0$ (can be random)\n",
    "\n",
    "### Step 2: Policy Evaluation\n",
    "- Compute the **state-value function V(s)** under current policy œÄ:\n",
    "  $$V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} P(s'|s,a)[r + \\gamma V^{\\pi}(s')]$$\n",
    "- Iterate until values converge\n",
    "\n",
    "### Step 3: Policy Improvement  \n",
    "- For each state, pick the action that maximizes the Q-value:\n",
    "  $$\\pi'(s) = \\arg\\max_a Q^{\\pi}(s,a)$$\n",
    "- Update policy by acting greedily with respect to $V^{\\pi}$\n",
    "\n",
    "### Step 4: Convergence Check\n",
    "- If $\\pi' = \\pi$, stop (optimal policy found)\n",
    "- Otherwise, set $\\pi = \\pi'$ and repeat from Step 2\n",
    "\n",
    "***\n",
    "\n",
    "## 3. Implementation\n",
    "\n",
    "### 3.1 Environment Setup\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "gamma = 0.9  # discount factor\n",
    "theta = 1e-6  # convergence threshold\n",
    "terminal_state = num_states - 1\n",
    "\n",
    "# Access transition probabilities\n",
    "P = env.unwrapped.P\n",
    "```\n",
    "\n",
    "### 3.2 Policy Evaluation Phase\n",
    "\n",
    "#### Basic State Value Computation\n",
    "```python\n",
    "def compute_state_value(state, policy, V):\n",
    "    \"\"\"Computes the value of a state under the given policy.\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    value = 0.0\n",
    "    for prob, next_state, reward, terminated in P[state][action]:\n",
    "        value += prob * (reward + gamma * V[next_state])\n",
    "    return value\n",
    "```\n",
    "\n",
    "#### Iterative Policy Evaluation\n",
    "```python\n",
    "def policy_evaluation_iterative(policy, env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a policy by iteratively solving Bellman equation\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_V = np.zeros_like(V)\n",
    "        \n",
    "        # Update value for each non-terminal state\n",
    "        for state in range(env.observation_space.n - 1):\n",
    "            if state in policy:\n",
    "                action = policy[state]\n",
    "                expected_value = 0\n",
    "                \n",
    "                # Compute expected value over all possible transitions\n",
    "                transitions = env.unwrapped.P[state][action]\n",
    "                for prob, next_state, reward, is_terminal in transitions:\n",
    "                    if is_terminal:\n",
    "                        expected_value += prob * reward\n",
    "                    else:\n",
    "                        expected_value += prob * (reward + gamma * V[next_state])\n",
    "                \n",
    "                new_V[state] = expected_value\n",
    "        \n",
    "        # Check for convergence\n",
    "        delta = np.max(np.abs(new_V - V))\n",
    "        V = new_V\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "```\n",
    "\n",
    "#### Exact Policy Evaluation (Linear Algebra)\n",
    "```python\n",
    "def policy_evaluation_exact(policy, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Solve policy evaluation exactly using linear algebra\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n - 1  # Exclude terminal state\n",
    "    \n",
    "    # Build system of linear equations: V = R + Œ≥PV\n",
    "    # Rearrange to: (I - Œ≥P)V = R\n",
    "    \n",
    "    I = np.eye(n_states)\n",
    "    P_matrix = np.zeros((n_states, n_states))\n",
    "    R = np.zeros(n_states)\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        if state in policy:\n",
    "            action = policy[state]\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if not is_terminal and next_state < n_states:\n",
    "                    P_matrix[state, next_state] += prob\n",
    "                R[state] += prob * reward\n",
    "    \n",
    "    # Solve linear system\n",
    "    A = I - gamma * P_matrix\n",
    "    V_exact = np.linalg.solve(A, R)\n",
    "    \n",
    "    # Add terminal state value\n",
    "    V_complete = np.zeros(env.observation_space.n)\n",
    "    V_complete[:n_states] = V_exact\n",
    "    \n",
    "    return V_complete\n",
    "```\n",
    "\n",
    "### 3.3 Policy Improvement Phase\n",
    "\n",
    "#### Q-Value Computation\n",
    "```python\n",
    "def compute_q_value(state, action, V, env, gamma=0.9):\n",
    "    \"\"\"Compute Q(s,a) = sum over next states [ P(s'|s,a) * (R + gamma*V(s')) ]\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    q = 0.0\n",
    "    for prob, next_state, reward, terminated in P[state][action]:\n",
    "        if terminated:\n",
    "            q += prob * reward\n",
    "        else:\n",
    "            q += prob * (reward + gamma * V[next_state])\n",
    "    return q\n",
    "\n",
    "def compute_q_values_from_v(V, env, gamma=0.9):\n",
    "    \"\"\"Compute Q-values from state values\"\"\"\n",
    "    Q = {}\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):\n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    q_value += prob * reward\n",
    "                else:\n",
    "                    q_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            Q[(state, action)] = q_value\n",
    "    \n",
    "    return Q\n",
    "```\n",
    "\n",
    "#### Policy Improvement Function\n",
    "```python\n",
    "def policy_improvement(V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Improve policy by acting greedily with respect to value function\n",
    "    \"\"\"\n",
    "    improved_policy = {}\n",
    "    policy_stable = True\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal state\n",
    "        # Compute action values for all possible actions\n",
    "        action_values = []\n",
    "        for action in range(env.action_space.n):\n",
    "            action_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    action_value += prob * reward\n",
    "                else:\n",
    "                    action_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            action_values.append(action_value)\n",
    "        \n",
    "        # Select action with highest value (greedy policy)\n",
    "        best_action = np.argmax(action_values)\n",
    "        improved_policy[state] = best_action\n",
    "    \n",
    "    return improved_policy, policy_stable\n",
    "```\n",
    "\n",
    "### 3.4 Complete Policy Iteration Algorithm\n",
    "\n",
    "```python\n",
    "def policy_iteration_complete(env, gamma=0.9, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Complete policy iteration algorithm\n",
    "    \"\"\"\n",
    "    # Initialize with random policy\n",
    "    policy = {state: np.random.choice(env.action_space.n) \n",
    "              for state in range(env.observation_space.n - 1)}\n",
    "    \n",
    "    iteration = 0\n",
    "    policy_history = []\n",
    "    \n",
    "    print(\"Starting Policy Iteration...\")\n",
    "    print(f\"Initial policy: {policy}\")\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        # Policy Evaluation\n",
    "        print(f\"\\nIteration {iteration + 1}: Policy Evaluation\")\n",
    "        V = policy_evaluation_iterative(policy, env, gamma)\n",
    "        \n",
    "        # Policy Improvement  \n",
    "        print(f\"Iteration {iteration + 1}: Policy Improvement\")\n",
    "        improved_policy, policy_stable = policy_improvement(V, env, gamma)\n",
    "        \n",
    "        # Store policy for analysis\n",
    "        policy_history.append(policy.copy())\n",
    "        \n",
    "        # Check for convergence\n",
    "        if policy_stable or improved_policy == policy:\n",
    "            print(f\"Policy iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        policy = improved_policy\n",
    "        iteration += 1\n",
    "    \n",
    "    return policy, V, policy_history\n",
    "\n",
    "def analyze_policy_convergence(policy_history):\n",
    "    \"\"\"\n",
    "    Analyze how policy changes during iteration\n",
    "    \"\"\"\n",
    "    print(\"\\nPolicy Evolution Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, policy in enumerate(policy_history):\n",
    "        print(f\"Iteration {i}: {policy}\")\n",
    "        \n",
    "        if i > 0:\n",
    "            changes = sum(1 for state in policy.keys() \n",
    "                         if policy[state] != policy_history[i-1][state])\n",
    "            print(f\"  States changed: {changes}\")\n",
    "\n",
    "# Run complete policy iteration\n",
    "optimal_policy, optimal_V, history = policy_iteration_complete(env, gamma=0.9)\n",
    "print(f\"\\nOptimal Policy: {optimal_policy}\")\n",
    "print(f\"Optimal State Values: {optimal_V}\")\n",
    "analyze_policy_convergence(history)\n",
    "```\n",
    "\n",
    "### 3.5 Visualization and Results\n",
    "\n",
    "```python\n",
    "# Pretty print results for FrozenLake (4x4 grid)\n",
    "print(\"‚úÖ Optimal Policy (per state):\")\n",
    "policy_array = np.array([optimal_policy.get(i, 0) for i in range(16)])\n",
    "print(policy_array.reshape((4, 4)))\n",
    "\n",
    "print(\"\\n‚úÖ Optimal Value Function:\")\n",
    "print(optimal_V.reshape((4, 4)))\n",
    "\n",
    "# Action mapping for better visualization\n",
    "action_map = {0: '‚Üê', 1: '‚Üì', 2: '‚Üí', 3: '‚Üë'}\n",
    "print(\"\\n‚úÖ Policy with arrows:\")\n",
    "policy_arrows = np.array([action_map[optimal_policy.get(i, 0)] for i in range(16)]).reshape((4, 4))\n",
    "print(policy_arrows)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "## 4. Policy Iteration Properties\n",
    "\n",
    "### 4.1 Convergence Guarantees\n",
    "\n",
    "**Finite Convergence**: Policy iteration converges in finite steps\n",
    "- At most $|A|^{|S|}$ possible deterministic policies\n",
    "- Each iteration either improves policy or finds optimal policy\n",
    "- Strict improvement until optimality reached\n",
    "\n",
    "**Optimality**: Converges to optimal policy $\\pi^*$\n",
    "- Final policy satisfies Bellman optimality equation\n",
    "- No further improvement possible\n",
    "\n",
    "### 4.2 Policy Improvement Theorem\n",
    "\n",
    "For any policy $\\pi$ and state $s$, if we define a new policy $\\pi'$ such that:\n",
    "$$\\pi'(s) = \\arg\\max_a Q^{\\pi}(s,a)$$\n",
    "\n",
    "Then $V^{\\pi'}(s) \\geq V^{\\pi}(s)$ for all states $s$.\n",
    "\n",
    "**Proof Intuition**: \n",
    "- Taking the best action according to current Q-values can only improve or maintain performance\n",
    "- If improvement occurs in any state, it propagates through the value function\n",
    "- If no improvement occurs anywhere, we have found an optimal policy\n",
    "\n",
    "### 4.3 Computational Complexity\n",
    "\n",
    "**Per Iteration**: \n",
    "- Policy Evaluation: O(|S|¬≥) for exact solution or O(|S|¬≤) per sweep for iterative\n",
    "- Policy Improvement: O(|S||A|)\n",
    "\n",
    "**Total Complexity**: O(k|S|¬≥) where k is number of iterations\n",
    "- k is typically small in practice (much less than $|A|^{|S|}$)\n",
    "- Often converges in just a few iterations\n",
    "\n",
    "**Time Complexity**: $O(|S|¬≤|A|)$ per iteration, where |S| is number of states and |A| is number of actions.\n",
    "\n",
    "***\n",
    "\n",
    "## 5. Advantages and Disadvantages\n",
    "\n",
    "### Advantages:\n",
    "- **Guaranteed convergence** to optimal policy\n",
    "- **Often fast convergence** in practice\n",
    "- **Clear separation** of evaluation and improvement phases\n",
    "- **Finds shortest path** to goal (optimal behavior)\n",
    "\n",
    "### Disadvantages:\n",
    "- **Requires exact policy evaluation** (computationally expensive)\n",
    "- **May be slow** for large state spaces\n",
    "- **Requires complete model** of environment (transition probabilities)\n",
    "- **Memory intensive** for storing complete value functions\n",
    "\n",
    "***\n",
    "\n",
    "## 6. Example Results\n",
    "\n",
    "For a FrozenLake environment, typical optimal policy might look like:\n",
    "```\n",
    "Optimal Policy: {0: 2, 1: 2, 2: 1, 3: 1, 4: 2, 5: 1, 6: 2, 7: 2}\n",
    "Optimal State Values: {0: 7, 1: 8, 2: 9, 3: 7, 4: 9, 5: 10, 6: 8, 7: 10, 8: 0}\n",
    "```\n",
    "\n",
    "The algorithm typically finds the shortest path to the goal state, demonstrating significant improvement over random initial policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338fb0c",
   "metadata": {},
   "source": [
    "# **üîñ3. Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c74bc",
   "metadata": {},
   "source": [
    "## 1. Algorithm Overview and Definition\n",
    "\n",
    "**Value Iteration** combines policy evaluation and policy improvement in a single operation, directly computing the optimal value function through repeated application of the Bellman optimality operator.\n",
    "\n",
    "> **Definition:** Value Iteration combines policy evaluation and improvement in one step. It computes the optimal state-value function and derives the policy from it, rather than maintaining an explicit policy throughout the process.\n",
    "\n",
    "**Key Insight**: Instead of fully evaluating a policy (like Policy Iteration), perform only one sweep of value updates followed by implicit policy improvement, making it more computationally efficient per iteration.\n",
    "\n",
    "### Core Idea\n",
    "- **Speeds up** policy iteration by combining evaluation & improvement in a single update\n",
    "- **Direct approach**: Updates value estimates directly toward optimality\n",
    "- **Implicit policy**: Policy is derived from values rather than maintained explicitly\n",
    "\n",
    "***\n",
    "\n",
    "## 2. Mathematical Foundation\n",
    "\n",
    "### Bellman Optimality Equation\n",
    "Value iteration is based on the **Bellman Optimality Equation**:\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "### Bellman Optimality Operator\n",
    "The Bellman optimality operator $T^*$ is defined as:\n",
    "$$(T^*V)(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V(s')]$$\n",
    "\n",
    "#### Key Mathematical Properties:\n",
    "\n",
    "**Contraction Mapping**: When $\\gamma < 1$, $T^*$ is a contraction with modulus $\\gamma$\n",
    "- $\\|T^*V_1 - T^*V_2\\|_{\\infty} \\leq \\gamma \\|V_1 - V_2\\|_{\\infty}$\n",
    "- Guarantees unique fixed point (optimal value function)\n",
    "- Ensures geometric convergence rate\n",
    "\n",
    "**Monotonicity**: If $V_1(s) \\leq V_2(s)$ for all $s$, then $(T^*V_1)(s) \\leq (T^*V_2)(s)$\n",
    "- Preserves ordering between value functions\n",
    "- Ensures convergence from any initialization\n",
    "\n",
    "***\n",
    "\n",
    "## 3. Algorithm Steps\n",
    "\n",
    "### Step 1: Initialize\n",
    "- Set $V_0(s) = 0$ for all states $s$\n",
    "\n",
    "### Step 2: Iterative Value Update\n",
    "For each state, apply the Bellman optimality operator:\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s',r} P(s'|s,a)[r + \\gamma V_k(s')]$$\n",
    "\n",
    "### Step 3: Policy Derivation (Implicit)\n",
    "Policy is implicitly defined as:\n",
    "$$\\pi(s) = \\arg\\max_a \\sum_{s',r} P(s'|s,a)[r + \\gamma V(s')]$$\n",
    "\n",
    "### Step 4: Convergence Check\n",
    "- Continue until value updates are below a threshold: $\\max_s |V_{k+1}(s) - V_k(s)| < \\theta$\n",
    "- Extract final policy using policy extraction\n",
    "\n",
    "***\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "### 4.1 Environment Setup\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode=None)\n",
    "mdp_env = env.unwrapped  # access the underlying MDP to get .P\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "terminal_state = num_states - 1  # Goal state in FrozenLake\n",
    "gamma = 0.9  # Discount factor\n",
    "theta = 1e-6  # Convergence threshold\n",
    "\n",
    "# Access transition probabilities\n",
    "P = env.unwrapped.P\n",
    "```\n",
    "\n",
    "### 4.2 Core Value Iteration Functions\n",
    "\n",
    "#### Bellman Optimality Update\n",
    "```python\n",
    "def bellman_optimality_update(V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Single step of Bellman optimality operator\n",
    "    \"\"\"\n",
    "    new_V = np.zeros_like(V)\n",
    "    policy = {}\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal state\n",
    "        action_values = []\n",
    "        \n",
    "        # Compute Q-value for each action\n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    q_value += prob * reward\n",
    "                else:\n",
    "                    q_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            action_values.append(q_value)\n",
    "        \n",
    "        # Take maximum over actions\n",
    "        max_value = max(action_values)\n",
    "        max_action = np.argmax(action_values)\n",
    "        \n",
    "        new_V[state] = max_value\n",
    "        policy[state] = max_action\n",
    "    \n",
    "    return new_V, policy\n",
    "```\n",
    "\n",
    "#### Helper Functions\n",
    "```python\n",
    "def get_max_action_and_value(state, V, env, gamma=0.9):\n",
    "    \"\"\"Helper function to get optimal action and value for a state.\"\"\"\n",
    "    Q_values = []\n",
    "    for action in range(env.action_space.n):\n",
    "        q_val = 0\n",
    "        for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
    "            q_val += prob * (reward + gamma * V[next_state])\n",
    "        Q_values.append(q_val)\n",
    "    \n",
    "    max_action = int(np.argmax(Q_values))\n",
    "    max_q_value = Q_values[max_action]\n",
    "    return max_action, max_q_value\n",
    "\n",
    "def compute_action_value(state, action, V, env, gamma=0.9):\n",
    "    \"\"\"Compute Q(s,a) using current value estimates\"\"\"\n",
    "    q_value = 0\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    \n",
    "    for prob, next_state, reward, is_terminal in transitions:\n",
    "        if is_terminal:\n",
    "            q_value += prob * reward\n",
    "        else:\n",
    "            q_value += prob * (reward + gamma * V[next_state])\n",
    "    \n",
    "    return q_value\n",
    "```\n",
    "\n",
    "### 4.3 Complete Value Iteration Algorithm\n",
    "\n",
    "#### Basic Implementation\n",
    "```python\n",
    "def value_iteration(env, gamma=0.9, threshold=1e-3, max_iterations=1000):\n",
    "    \"\"\"Basic value iteration algorithm.\"\"\"\n",
    "    # Initialize\n",
    "    V = {state: 0 for state in range(env.observation_space.n)}\n",
    "    policy = {state: 0 for state in range(env.observation_space.n - 1)}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_V = {state: 0 for state in range(env.observation_space.n)}\n",
    "        \n",
    "        for state in range(env.observation_space.n - 1):  # Exclude terminal\n",
    "            if state == terminal_state:\n",
    "                new_V[state] = 0\n",
    "                continue\n",
    "            \n",
    "            # Compute Q-values for all actions\n",
    "            Q_values = []\n",
    "            for action in range(env.action_space.n):\n",
    "                q_val = 0\n",
    "                for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
    "                    q_val += prob * (reward + gamma * V[next_state])\n",
    "                Q_values.append(q_val)\n",
    "            \n",
    "            # Take maximum\n",
    "            max_q_value = max(Q_values)\n",
    "            max_action = int(np.argmax(Q_values))\n",
    "            new_V[state] = max_q_value\n",
    "            policy[state] = max_action\n",
    "        \n",
    "        # Check convergence\n",
    "        if all(abs(new_V[s] - V[s]) < threshold for s in range(env.observation_space.n)):\n",
    "            print(f\"Value iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        V = new_V\n",
    "    \n",
    "    return policy, V\n",
    "```\n",
    "\n",
    "#### Enhanced Implementation with Analysis\n",
    "```python\n",
    "def value_iteration_enhanced(env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Enhanced value iteration algorithm with detailed tracking\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    print(\"Starting Value Iteration...\")\n",
    "    convergence_history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Apply Bellman optimality operator\n",
    "        new_V, current_policy = bellman_optimality_update(V, env, gamma)\n",
    "        \n",
    "        # Check for convergence\n",
    "        delta = np.max(np.abs(new_V - V))\n",
    "        convergence_history.append(delta)\n",
    "        V = new_V\n",
    "        \n",
    "        if iteration % 10 == 0:  # Print progress every 10 iterations\n",
    "            print(f\"Iteration {iteration}: max change = {delta:.6f}\")\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Extract final policy\n",
    "    _, final_policy = bellman_optimality_update(V, env, gamma)\n",
    "    \n",
    "    return V, final_policy, convergence_history\n",
    "```\n",
    "\n",
    "### 4.4 Policy Extraction\n",
    "\n",
    "```python\n",
    "def extract_policy_from_values(V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Extract optimal policy from optimal value function\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal\n",
    "        action_values = []\n",
    "        \n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    q_value += prob * reward\n",
    "                else:\n",
    "                    q_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            action_values.append(q_value)\n",
    "        \n",
    "        # Select action with maximum Q-value\n",
    "        policy[state] = np.argmax(action_values)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def verify_policy_optimality(policy, V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Verify that extracted policy is optimal\n",
    "    \"\"\"\n",
    "    print(\"Policy Optimality Verification:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    violations = 0\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):\n",
    "        # Compute value of current policy action\n",
    "        policy_action = policy[state]\n",
    "        policy_q = compute_action_value(state, policy_action, V, env, gamma)\n",
    "        \n",
    "        # Compute maximum Q-value over all actions\n",
    "        max_q = max(compute_action_value(state, action, V, env, gamma) \n",
    "                   for action in range(env.action_space.n))\n",
    "        \n",
    "        # Check optimality condition\n",
    "        if abs(policy_q - max_q) > 1e-6:\n",
    "            violations += 1\n",
    "            print(f\"State {state}: Policy Q={policy_q:.6f}, Max Q={max_q:.6f}\")\n",
    "    \n",
    "    if violations == 0:\n",
    "        print(\"‚úì Policy is optimal (satisfies Bellman optimality)\")\n",
    "    else:\n",
    "        print(f\"‚úó Policy has {violations} optimality violations\")\n",
    "    \n",
    "    return violations == 0\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "## 5. Convergence Analysis and Properties\n",
    "\n",
    "### 5.1 Convergence Properties\n",
    "\n",
    "**Geometric Convergence**: Value iteration converges at rate $\\gamma$\n",
    "- Error decreases by factor $\\gamma$ each iteration\n",
    "- Faster convergence for smaller discount factors\n",
    "\n",
    "**Asymptotic Convergence**: Unlike Policy Iteration's finite convergence\n",
    "- Values approach $V^*$ asymptotically\n",
    "- Practical convergence when changes fall below threshold\n",
    "\n",
    "### 5.2 Convergence Analysis Implementation\n",
    "\n",
    "```python\n",
    "def analyze_convergence_rate(env, gamma=0.9, true_V=None):\n",
    "    \"\"\"\n",
    "    Analyze convergence rate of value iteration\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    errors = []\n",
    "    \n",
    "    if true_V is None:\n",
    "        # Compute true optimal values using many iterations\n",
    "        true_V, _, _ = value_iteration_enhanced(env, gamma, theta=1e-12, max_iterations=10000)\n",
    "    \n",
    "    print(\"Convergence Analysis:\")\n",
    "    print(\"Iteration | Max Error | Convergence Rate\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for iteration in range(50):\n",
    "        V, _ = bellman_optimality_update(V, env, gamma)\n",
    "        error = np.max(np.abs(V - true_V))\n",
    "        errors.append(error)\n",
    "        \n",
    "        # Compute convergence rate\n",
    "        if iteration > 0:\n",
    "            rate = errors[iteration] / errors[iteration-1] if errors[iteration-1] > 0 else 0\n",
    "        else:\n",
    "            rate = 0\n",
    "        \n",
    "        if iteration % 5 == 0:\n",
    "            print(f\"{iteration:9} | {error:9.6f} | {rate:9.6f}\")\n",
    "    \n",
    "    # Theoretical vs empirical convergence rate\n",
    "    theoretical_rate = gamma\n",
    "    empirical_rate = np.mean([errors[i]/errors[i-1] for i in range(5, 20) if errors[i-1] > 1e-10])\n",
    "    \n",
    "    print(f\"\\nTheoretical convergence rate: {theoretical_rate:.6f}\")\n",
    "    print(f\"Empirical convergence rate: {empirical_rate:.6f}\")\n",
    "    \n",
    "    return errors\n",
    "```\n",
    "\n",
    "### 5.3 Computational Complexity\n",
    "\n",
    "**Per Iteration**: O(|S|¬≤|A|)\n",
    "- For each state: compute Q-value for each action\n",
    "- Each Q-value computation: sum over next states\n",
    "\n",
    "**Total Complexity**: O(k|S|¬≤|A|) where k is number of iterations\n",
    "- k depends on desired accuracy and discount factor\n",
    "- Typically many more iterations than Policy Iteration\n",
    "\n",
    "***\n",
    "\n",
    "## 6. Value Iteration vs Policy Iteration\n",
    "\n",
    "### 6.1 Detailed Comparison\n",
    "\n",
    "| Aspect | **Policy Iteration** | **Value Iteration** |\n",
    "|--------|---------------------|-------------------|\n",
    "| **Approach** | Two clear steps: (1) **Policy Evaluation** ‚Äì compute $V^œÄ$, (2) **Policy Improvement** ‚Äì update policy greedily | Blends evaluation and improvement into **one step** using Bellman optimality equation |\n",
    "| **Convergence** | **Finite iterations** ‚Äì guaranteed to find optimal policy in finite steps | **Asymptotic convergence** ‚Äì values approach $V^*$ geometrically |\n",
    "| **Per Iteration Cost** | **High** ‚Äì O(|S|¬≥) for exact policy evaluation | **Low** ‚Äì O(|S|¬≤|A|) for value updates |\n",
    "| **Total Iterations** | **Fewer** ‚Äì makes big jumps with full policy evaluation | **More** ‚Äì small incremental improvements |\n",
    "| **Memory** | Must store **explicit policy** alongside value function | Policy **implicitly derived** from values |\n",
    "| **Practical Use** | Best for **small/medium state spaces** | Preferred for **large/complex environments** |\n",
    "\n",
    "### 6.2 Performance Comparison Implementation\n",
    "\n",
    "```python\n",
    "def compare_with_policy_iteration(env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Compare value iteration results with policy iteration\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPARISON: Value Iteration vs Policy Iteration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Run value iteration\n",
    "    print(\"\\n1. Running Value Iteration...\")\n",
    "    start_time = time.time()\n",
    "    V_vi, policy_vi, history_vi = value_iteration_enhanced(env, gamma)\n",
    "    vi_time = time.time() - start_time\n",
    "    \n",
    "    # Run policy iteration (assuming policy_iteration_complete exists)\n",
    "    print(\"\\n2. Running Policy Iteration...\")\n",
    "    start_time = time.time()\n",
    "    policy_pi, V_pi, history_pi = policy_iteration_complete(env, gamma)\n",
    "    pi_time = time.time() - start_time\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n3. Comparing Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Compare policies\n",
    "    policy_match = all(policy_vi.get(s) == policy_pi.get(s) \n",
    "                      for s in range(env.observation_space.n - 1))\n",
    "    print(f\"Policies identical: {policy_match}\")\n",
    "    \n",
    "    # Compare values\n",
    "    value_diff = np.max(np.abs(V_vi - V_pi))\n",
    "    print(f\"Maximum value difference: {value_diff:.8f}\")\n",
    "    \n",
    "    # Compare performance\n",
    "    print(f\"Value Iteration time: {vi_time:.4f}s, iterations: {len(history_vi)}\")\n",
    "    print(f\"Policy Iteration time: {pi_time:.4f}s, iterations: {len(history_pi)}\")\n",
    "    \n",
    "    print(f\"\\nValue Iteration Policy: {policy_vi}\")\n",
    "    print(f\"Policy Iteration Policy: {policy_pi}\")\n",
    "    \n",
    "    return V_vi, policy_vi, V_pi, policy_pi\n",
    "```\n",
    "\n",
    "### 6.3 Intuitive Understanding\n",
    "\n",
    "**Policy Iteration = \"Think hard, act big\"**\n",
    "- Each step is expensive but fewer steps needed\n",
    "- Complete policy evaluation ensures big improvements\n",
    "\n",
    "**Value Iteration = \"Think fast, act small\"**  \n",
    "- Each step is cheap but more steps required\n",
    "- Incremental improvements toward optimality\n",
    "\n",
    "### 6.4 When to Use Each\n",
    "\n",
    "**Value Iteration**:\n",
    "- **Large action spaces** ‚Äì cheaper per iteration\n",
    "- **Approximate solutions acceptable** ‚Äì can stop early\n",
    "- **Limited computational memory** ‚Äì no policy storage\n",
    "- **Online/real-time applications** ‚Äì faster iterations\n",
    "\n",
    "**Policy Iteration**:\n",
    "- **Small to medium problems** ‚Äì exact evaluation feasible\n",
    "- **Exact solutions required** ‚Äì finite convergence\n",
    "- **Batch processing scenarios** ‚Äì can afford expensive iterations\n",
    "- **Policy stability important** ‚Äì explicit policy tracking\n",
    "\n",
    "***\n",
    "\n",
    "## 7. Advanced Topics\n",
    "\n",
    "### 7.1 Modified Policy Iteration (Hybrid Approach)\n",
    "\n",
    "```python\n",
    "def modified_policy_iteration(env, gamma=0.9, k=10, theta=1e-6, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Modified policy iteration: partial policy evaluation + improvement\n",
    "    Bridges gap between Policy Iteration and Value Iteration\n",
    "    \"\"\"\n",
    "    # Initialize policy randomly\n",
    "    policy = {state: np.random.choice(env.action_space.n) \n",
    "              for state in range(env.observation_space.n - 1)}\n",
    "    \n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Partial policy evaluation (k steps instead of full convergence)\n",
    "        for _ in range(k):\n",
    "            new_V = np.zeros_like(V)\n",
    "            \n",
    "            for state in range(env.observation_space.n - 1):\n",
    "                if state in policy:\n",
    "                    action = policy[state]\n",
    "                    expected_value = 0\n",
    "                    \n",
    "                    transitions = env.unwrapped.P[state][action]\n",
    "                    for prob, next_state, reward, is_terminal in transitions:\n",
    "                        if is_terminal:\n",
    "                            expected_value += prob * reward\n",
    "                        else:\n",
    "                            expected_value += prob * (reward + gamma * V[next_state])\n",
    "                    \n",
    "                    new_V[state] = expected_value\n",
    "            \n",
    "            V = new_V\n",
    "        \n",
    "        # Policy improvement\n",
    "        improved_policy, policy_stable = policy_improvement(V, env, gamma)\n",
    "        \n",
    "        if policy_stable:\n",
    "            print(f\"Modified policy iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        policy = improved_policy\n",
    "    \n",
    "    return policy, V\n",
    "```\n",
    "\n",
    "### 7.2 Visualization and Results\n",
    "\n",
    "```python\n",
    "# Run and compare all algorithms\n",
    "def run_comprehensive_comparison(env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Run all three algorithms and compare results\n",
    "    \"\"\"\n",
    "    print(\"COMPREHENSIVE ALGORITHM COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run all three methods\n",
    "    print(\"\\n1. Value Iteration\")\n",
    "    V_vi, policy_vi, _ = value_iteration_enhanced(env, gamma)\n",
    "    \n",
    "    print(\"\\n2. Policy Iteration\") \n",
    "    policy_pi, V_pi, _ = policy_iteration_complete(env, gamma)\n",
    "    \n",
    "    print(\"\\n3. Modified Policy Iteration\")\n",
    "    policy_mpi, V_mpi = modified_policy_iteration(env, gamma, k=5)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nValue Iteration Policy:     {policy_vi}\")\n",
    "    print(f\"Policy Iteration Policy:    {policy_pi}\")  \n",
    "    print(f\"Modified Policy Iteration:  {policy_mpi}\")\n",
    "    \n",
    "    # Pretty print for FrozenLake (4x4 grid)\n",
    "    if env.observation_space.n == 16:  # FrozenLake 4x4\n",
    "        print(\"\\n‚úÖ Value Iteration Results:\")\n",
    "        policy_array = np.array([policy_vi.get(i, 0) for i in range(16)])\n",
    "        print(\"Policy (per state):\")\n",
    "        print(policy_array.reshape((4, 4)))\n",
    "        \n",
    "        print(\"Value Function:\")\n",
    "        print(V_vi.reshape((4, 4)))\n",
    "        \n",
    "        # Action mapping for visualization\n",
    "        action_map = {0: '‚Üê', 1: '‚Üì', 2: '‚Üí', 3: '‚Üë'}\n",
    "        print(\"Policy with arrows:\")\n",
    "        policy_arrows = np.array([action_map[policy_vi.get(i, 0)] for i in range(16)]).reshape((4, 4))\n",
    "        print(policy_arrows)\n",
    "    \n",
    "    return V_vi, policy_vi, V_pi, policy_pi, V_mpi, policy_mpi\n",
    "\n",
    "# Execute comprehensive comparison\n",
    "results = run_comprehensive_comparison(env, gamma=0.9)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "## 8. Key Advantages and Disadvantages\n",
    "\n",
    "### Advantages:\n",
    "- **Computationally efficient per iteration** ‚Äì O(|S|¬≤|A|) vs O(|S|¬≥)\n",
    "- **Memory efficient** ‚Äì no explicit policy storage required\n",
    "- **Flexible stopping** ‚Äì can terminate early for approximate solutions\n",
    "- **Scalable** ‚Äì works well with large state spaces\n",
    "- **Same optimal result** as Policy Iteration\n",
    "\n",
    "### Disadvantages:\n",
    "- **More total iterations** required for convergence\n",
    "- **Asymptotic convergence** ‚Äì never truly reaches optimality\n",
    "- **Less intuitive** ‚Äì policy changes implicitly\n",
    "- **Threshold dependent** ‚Äì convergence criteria affects solution quality\n",
    "\n",
    "***\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "Value Iteration provides an efficient alternative to Policy Iteration by combining evaluation and improvement steps. While it requires more iterations, each iteration is computationally cheaper, making it particularly suitable for large-scale problems. The algorithm is guaranteed to converge to the optimal policy, achieving the same result as Policy Iteration but through a different computational path.\n",
    "\n",
    "The choice between Value Iteration and Policy Iteration depends on the specific problem characteristics: use Value Iteration for large problems where computational efficiency per iteration is crucial, and Policy Iteration for smaller problems where exact solutions and fewer iterations are preferred."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
