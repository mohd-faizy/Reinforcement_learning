{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbb488b",
   "metadata": {},
   "source": [
    "## 🔹 1. **Q-Learning (Vanilla Q-Learning)**\n",
    "\n",
    "Q-learning is a **model-free reinforcement learning algorithm** that learns the value of taking an action in a given state.\n",
    "It uses a **Q-table** (state-action value table) to store values.\n",
    "\n",
    "**Update Rule**:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[ r + \\gamma \\max_{a'} Q(s', a') - Q(s,a) \\Big]\n",
    "$$\n",
    "\n",
    "* $s$: current state\n",
    "* $a$: action taken\n",
    "* $r$: reward\n",
    "* $s'$: next state\n",
    "* $\\alpha$: learning rate\n",
    "* $\\gamma$: discount factor\n",
    "\n",
    "👉 Works well in **small discrete state spaces**, but struggles with large or continuous spaces since the Q-table becomes huge.\n",
    "\n",
    "**Example:**\n",
    "Suppose an agent in a grid world wants to reach a goal.\n",
    "\n",
    "* States = grid cells\n",
    "* Actions = {up, down, left, right}\n",
    "* The Q-table might look like:\n",
    "\n",
    "| State | Up  | Down | Left | Right |\n",
    "| ----- | --- | ---- | ---- | ----- |\n",
    "| (0,0) | 0   | 0.2  | 0    | 0.1   |\n",
    "| (0,1) | 0.5 | 0.1  | 0.3  | 0.4   |\n",
    "\n",
    "The agent updates this table until it learns the best path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4bb43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 22.0\n",
      "Episode 100, Total Reward: 18.0\n",
      "Episode 200, Total Reward: 55.0\n",
      "Episode 300, Total Reward: 196.0\n",
      "Episode 400, Total Reward: 234.0\n",
      "Episode 500, Total Reward: 154.0\n",
      "Episode 600, Total Reward: 196.0\n",
      "Episode 700, Total Reward: 11.0\n",
      "Episode 800, Total Reward: 216.0\n",
      "Episode 900, Total Reward: 166.0\n",
      "Episode 1000, Total Reward: 179.0\n",
      "Episode 1100, Total Reward: 145.0\n",
      "Episode 1200, Total Reward: 160.0\n",
      "Episode 1300, Total Reward: 164.0\n",
      "Episode 1400, Total Reward: 140.0\n",
      "Episode 1500, Total Reward: 180.0\n",
      "Episode 1600, Total Reward: 146.0\n",
      "Episode 1700, Total Reward: 219.0\n",
      "Episode 1800, Total Reward: 170.0\n",
      "Episode 1900, Total Reward: 245.0\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning with State Discretization (Beginner Friendly Example)\n",
    "# -------------------------------------------------------------\n",
    "# This example uses Q-learning on the CartPole environment.\n",
    "# Since the environment has continuous states, we discretize them\n",
    "# into bins (categories) to apply tabular Q-learning.\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. Create the environment\n",
    "# -------------------------------------------------------------\n",
    "# \"CartPole-v1\" is a classic control problem.\n",
    "# Goal: Keep the pole balanced by moving left or right.\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. Discretization Setup\n",
    "# -------------------------------------------------------------\n",
    "# The CartPole observation has 4 continuous values:\n",
    "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "# We'll discretize only the pole angle and pole angular velocity,\n",
    "# since they are most important for balancing.\n",
    "\n",
    "n_bins = (6, 12)  # 6 bins for angle, 12 bins for angular velocity\n",
    "\n",
    "# Define limits for clipping (CartPole docs give infinite ranges for velocity)\n",
    "obs_space = np.array([\n",
    "    [-4.8, 4.8],       # cart position (unused)\n",
    "    [-5.0, 5.0],       # cart velocity (unused)\n",
    "    [-0.418, 0.418],   # pole angle (approx -24° to +24°)\n",
    "    [-5.0, 5.0]        # pole angular velocity\n",
    "])\n",
    "\n",
    "def discretize(obs):\n",
    "    \"\"\"\n",
    "    Convert continuous observation values (angle & angular velocity)\n",
    "    into discrete bin indices.\n",
    "    \"\"\"\n",
    "    # Select only indices 2 (angle) and 3 (angular velocity)\n",
    "    ratios = []\n",
    "    for i, bins in zip([2, 3], n_bins):\n",
    "        low, high = obs_space[i]\n",
    "        ratio = (obs[i] - low) / (high - low)\n",
    "        new_obs = int(round((bins - 1) * ratio))\n",
    "        # Clip to stay inside bin range\n",
    "        new_obs = np.clip(new_obs, 0, bins - 1)\n",
    "        ratios.append(new_obs)\n",
    "    return tuple(ratios)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Initialize Q-table\n",
    "# -------------------------------------------------------------\n",
    "# Q-table shape: (angle_bins, velocity_bins, actions)\n",
    "\n",
    "Q = np.zeros(n_bins + (env.action_space.n,))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1   # learning rate\n",
    "gamma = 0.99  # discount factor\n",
    "eps = 1.0     # exploration rate (epsilon for epsilon-greedy)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. Training Loop\n",
    "# -------------------------------------------------------------\n",
    "# We'll run multiple episodes, update Q-values using Q-learning,\n",
    "# and slowly reduce epsilon (exploration) over time.\n",
    "\n",
    "n_episodes = 2000\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = discretize(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose action (epsilon-greedy)\n",
    "        if np.random.rand() < eps:\n",
    "            action = env.action_space.sample()  # explore\n",
    "        else:\n",
    "            action = np.argmax(Q[state])        # exploit best action\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = discretize(next_obs)\n",
    "\n",
    "        # Q-learning update rule\n",
    "        best_next = np.max(Q[next_state])\n",
    "        Q[state + (action,)] += alpha * (reward + gamma * best_next - Q[state + (action,)])\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Decay epsilon (exploration rate)\n",
    "    eps = max(0.01, eps * 0.995)\n",
    "\n",
    "    # Print progress every 100 episodes\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5. Summary\n",
    "# -------------------------------------------------------------\n",
    "# - This is a **tabular Q-learning** example.\n",
    "# - It works because we discretized the continuous state space.\n",
    "# - Later, we will compare this with **Deep Q-Learning**, which\n",
    "#   uses a neural network to handle continuous states directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc5c36",
   "metadata": {},
   "source": [
    "## 🔹 2. **Deep Q-Learning (DQN or D-Q Learning)**\n",
    "\n",
    "Instead of a Q-table, we use a **neural network** to approximate the Q-function:\n",
    "\n",
    "$$\n",
    "Q(s,a;\\theta) \\approx Q(s,a)\n",
    "$$\n",
    "\n",
    "* $\\theta$: parameters of the neural network\n",
    "* Input: state (can be high-dimensional, e.g., images)\n",
    "* Output: Q-values for each possible action\n",
    "\n",
    "### Key Features of DQN\n",
    "\n",
    "1. **Experience Replay**: Store past experiences $(s,a,r,s')$ in a replay buffer, and sample mini-batches to break correlation between consecutive updates.\n",
    "2. **Target Network**: Maintain a separate network for stable Q-value updates.\n",
    "\n",
    "**Update Rule (with NN):**\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\Big[ r + \\gamma \\max_{a'} Q(s',a';\\theta^-) - Q(s,a;\\theta) \\Big]^2\n",
    "$$\n",
    "\n",
    "where $\\theta^-$ are the parameters of the target network.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Playing Atari (Breakout 🎮)\n",
    "\n",
    "* **Q-Learning**: Not feasible (huge state space, every pixel arrangement is a state).\n",
    "* **DQN**: Input the raw image into a convolutional neural network → output Q-values for {move left, move right, fire}.\n",
    "\n",
    "  * Example: The NN might learn that in state (ball near paddle, moving right), the Q-value for action “move right” is highest.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Key Differences\n",
    "\n",
    "| Feature              | Q-Learning                    | Deep Q-Learning (DQN)                            |        |   |   |                     |\n",
    "| -------------------- | ----------------------------- | ------------------------------------------------ | ------ | - | - | ------------------- |\n",
    "| Value Representation | **Q-table** (explicit lookup) | **Neural Network** (function approximation)      |        |   |   |                     |\n",
    "| State Space          | Small, discrete               | Large/continuous, high-dimensional               |        |   |   |                     |\n",
    "| Memory               | Needs table of size (         | S                                                | \\times | A | ) | Needs weights of NN |\n",
    "| Stability            | More stable, but limited      | Needs tricks (experience replay, target network) |        |   |   |                     |\n",
    "| Applications         | Gridworld, simple games       | Atari, robotics, real-world tasks                |        |   |   |                     |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* **Q-learning** = Good for small toy problems.\n",
    "* **DQN (Deep Q-learning)** = Scales Q-learning using neural nets → can solve complex problems like playing video games or controlling robots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d4dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohdf\\AppData\\Local\\Temp\\ipykernel_14524\\188651980.py:105: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  s = torch.FloatTensor(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Total Reward: 88.0\n",
      "Episode 100, Total Reward: 88.0\n",
      "Episode 150, Total Reward: 93.0\n",
      "Episode 200, Total Reward: 101.0\n",
      "Episode 250, Total Reward: 28.0\n",
      "Episode 300, Total Reward: 35.0\n",
      "Episode 350, Total Reward: 107.0\n",
      "Episode 400, Total Reward: 114.0\n",
      "Episode 450, Total Reward: 91.0\n"
     ]
    }
   ],
   "source": [
    "# Deep Q-Learning with Neural Network (Beginner Friendly Example)\n",
    "# -------------------------------------------------------------\n",
    "# In this example, we solve the CartPole environment using\n",
    "# Deep Q-Learning (DQN). Instead of discretizing states,\n",
    "# we use a neural network to approximate Q-values directly.\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. Create the environment\n",
    "# -------------------------------------------------------------\n",
    "# Same CartPole environment as before.\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. Define the Neural Network (Q-value Approximator)\n",
    "# -------------------------------------------------------------\n",
    "# Input: state (4 continuous values)\n",
    "# Output: Q-value for each action (left or right)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Setup Policy & Target Networks\n",
    "# -------------------------------------------------------------\n",
    "# Policy net: Learns during training\n",
    "# Target net: Provides stable targets (updated less often)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy_net = DQN(state_dim, action_dim)\n",
    "target_net = DQN(state_dim, action_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())  # sync initially\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. Replay Buffer (Experience Replay)\n",
    "# -------------------------------------------------------------\n",
    "# Stores past experiences and samples random batches\n",
    "# to break correlation between consecutive steps.\n",
    "\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "batch_size = 64\n",
    "gamma = 0.99  # discount factor\n",
    "eps = 1.0    # exploration rate (epsilon-greedy)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5. Action Selection (Epsilon-Greedy)\n",
    "# -------------------------------------------------------------\n",
    "def select_action(state):\n",
    "    if random.random() < eps:\n",
    "        return env.action_space.sample()  # explore\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    return policy_net(state).argmax().item()  # exploit\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 6. Training Loop\n",
    "# -------------------------------------------------------------\n",
    "# Each episode: play the game, store experiences, update network.\n",
    "\n",
    "n_episodes = 500\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose action\n",
    "        action = select_action(state)\n",
    "\n",
    "        # Step in the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Save transition in replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Train only if buffer has enough samples\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            s, a, r, ns, d = zip(*batch)\n",
    "\n",
    "            s = torch.FloatTensor(s)\n",
    "            a = torch.LongTensor(a).unsqueeze(1)\n",
    "            r = torch.FloatTensor(r).unsqueeze(1)\n",
    "            ns = torch.FloatTensor(ns)\n",
    "            d = torch.FloatTensor(d).unsqueeze(1)\n",
    "\n",
    "            # Current Q(s,a)\n",
    "            q_values = policy_net(s).gather(1, a)\n",
    "\n",
    "            # Target Q-values: r + gamma * max_a' Q(s',a')\n",
    "            max_next_q = target_net(ns).max(1, keepdim=True)[0]\n",
    "            target = r + gamma * max_next_q * (1 - d)\n",
    "\n",
    "            # Loss (MSE)\n",
    "            loss = nn.MSELoss()(q_values, target.detach())\n",
    "\n",
    "            # Gradient descent\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 7. Exploration Decay & Target Network Update\n",
    "    # ---------------------------------------------------------\n",
    "    eps = max(0.01, eps * 0.995)  # slowly reduce exploration\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Print progress every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 8. Summary\n",
    "# -------------------------------------------------------------\n",
    "# - Unlike Q-learning with discretization, here we use a neural\n",
    "#   network to handle the continuous state space.\n",
    "# - The target network and replay buffer are key innovations\n",
    "#   that make Deep Q-Learning stable.\n",
    "# - This will be compared with the tabular Q-learning example.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
