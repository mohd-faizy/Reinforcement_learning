{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbb488b",
   "metadata": {},
   "source": [
    "## 🔹 1. **Q-Learning (Vanilla Q-Learning)**\n",
    "\n",
    "Q-learning is a **model-free reinforcement learning algorithm** that learns the value of taking an action in a given state.\n",
    "It uses a **Q-table** (state-action value table) to store values.\n",
    "\n",
    "**Update Rule**:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[ r + \\gamma \\max_{a'} Q(s', a') - Q(s,a) \\Big]\n",
    "$$\n",
    "\n",
    "* $s$: current state\n",
    "* $a$: action taken\n",
    "* $r$: reward\n",
    "* $s'$: next state\n",
    "* $\\alpha$: learning rate\n",
    "* $\\gamma$: discount factor\n",
    "\n",
    "👉 Works well in **small discrete state spaces**, but struggles with large or continuous spaces since the Q-table becomes huge.\n",
    "\n",
    "**Example:**\n",
    "Suppose an agent in a grid world wants to reach a goal.\n",
    "\n",
    "* States = grid cells\n",
    "* Actions = {up, down, left, right}\n",
    "* The Q-table might look like:\n",
    "\n",
    "| State | Up  | Down | Left | Right |\n",
    "| ----- | --- | ---- | ---- | ----- |\n",
    "| (0,0) | 0   | 0.2  | 0    | 0.1   |\n",
    "| (0,1) | 0.5 | 0.1  | 0.3  | 0.4   |\n",
    "\n",
    "The agent updates this table until it learns the best path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4bb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym        \n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Discretization\n",
    "n_bins = (6, 12)   # angle, angular velocity bins\n",
    "obs_space = np.array([env.observation_space.low, env.observation_space.high]).T\n",
    "obs_space[1] = [4.8, 5, 0.418, 5]  # clip values\n",
    "\n",
    "def discretize(obs):\n",
    "    ratios = [(obs[i] + abs(obs_space[i][0])) / (obs_space[i][1] - obs_space[i][0]) for i in [2,3]]\n",
    "    new_obs = [int(round((n_bins[i] - 1) * ratios[i])) for i in range(2)]\n",
    "    return tuple(np.clip(new_obs, 0, np.array(n_bins)-1))\n",
    "\n",
    "# Q-table\n",
    "Q = np.zeros(n_bins + (env.action_space.n,))\n",
    "alpha, gamma, eps = 0.1, 0.99, 1.0\n",
    "\n",
    "for episode in range(5000):\n",
    "    obs, _ = env.reset()\n",
    "    state = discretize(obs)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated  # ✅ fix for Gymnasium\n",
    "\n",
    "        next_state = discretize(next_obs)\n",
    "\n",
    "        best_next = np.max(Q[next_state])\n",
    "        Q[state + (action,)] += alpha * (reward + gamma * best_next - Q[state + (action,)])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    eps = max(0.01, eps * 0.995)  # decay epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc5c36",
   "metadata": {},
   "source": [
    "## 🔹 2. **Deep Q-Learning (DQN or D-Q Learning)**\n",
    "\n",
    "Instead of a Q-table, we use a **neural network** to approximate the Q-function:\n",
    "\n",
    "$$\n",
    "Q(s,a;\\theta) \\approx Q(s,a)\n",
    "$$\n",
    "\n",
    "* $\\theta$: parameters of the neural network\n",
    "* Input: state (can be high-dimensional, e.g., images)\n",
    "* Output: Q-values for each possible action\n",
    "\n",
    "### Key Features of DQN\n",
    "\n",
    "1. **Experience Replay**: Store past experiences $(s,a,r,s')$ in a replay buffer, and sample mini-batches to break correlation between consecutive updates.\n",
    "2. **Target Network**: Maintain a separate network for stable Q-value updates.\n",
    "\n",
    "**Update Rule (with NN):**\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\Big[ r + \\gamma \\max_{a'} Q(s',a';\\theta^-) - Q(s,a;\\theta) \\Big]^2\n",
    "$$\n",
    "\n",
    "where $\\theta^-$ are the parameters of the target network.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Playing Atari (Breakout 🎮)\n",
    "\n",
    "* **Q-Learning**: Not feasible (huge state space, every pixel arrangement is a state).\n",
    "* **DQN**: Input the raw image into a convolutional neural network → output Q-values for {move left, move right, fire}.\n",
    "\n",
    "  * Example: The NN might learn that in state (ball near paddle, moving right), the Q-value for action “move right” is highest.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Key Differences\n",
    "\n",
    "| Feature              | Q-Learning                    | Deep Q-Learning (DQN)                            |        |   |   |                     |\n",
    "| -------------------- | ----------------------------- | ------------------------------------------------ | ------ | - | - | ------------------- |\n",
    "| Value Representation | **Q-table** (explicit lookup) | **Neural Network** (function approximation)      |        |   |   |                     |\n",
    "| State Space          | Small, discrete               | Large/continuous, high-dimensional               |        |   |   |                     |\n",
    "| Memory               | Needs table of size (         | S                                                | \\times | A | ) | Needs weights of NN |\n",
    "| Stability            | More stable, but limited      | Needs tricks (experience replay, target network) |        |   |   |                     |\n",
    "| Applications         | Gridworld, simple games       | Atari, robotics, real-world tasks                |        |   |   |                     |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* **Q-learning** = Good for small toy problems.\n",
    "* **DQN (Deep Q-learning)** = Scales Q-learning using neural nets → can solve complex problems like playing video games or controlling robots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohdf\\AppData\\Local\\Temp\\ipykernel_6664\\1651514110.py:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  s = torch.FloatTensor(s)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym           \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Neural Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Hyperparams\n",
    "state_dim, action_dim = env.observation_space.shape[0], env.action_space.n\n",
    "policy_net, target_net = DQN(state_dim, action_dim), DQN(state_dim, action_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "batch_size, gamma, eps = 64, 0.99, 1.0\n",
    "\n",
    "def select_action(state):\n",
    "    if random.random() < eps:\n",
    "        return env.action_space.sample()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    return policy_net(state).argmax().item()\n",
    "\n",
    "# Training\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        # Update\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            s, a, r, ns, d = zip(*batch)\n",
    "\n",
    "            s = torch.FloatTensor(s)\n",
    "            a = torch.LongTensor(a).unsqueeze(1)\n",
    "            r = torch.FloatTensor(r).unsqueeze(1)\n",
    "            ns = torch.FloatTensor(ns)\n",
    "            d = torch.FloatTensor(d).unsqueeze(1)\n",
    "\n",
    "            q_values = policy_net(s).gather(1, a)\n",
    "            max_next_q = target_net(ns).max(1, keepdim=True)[0]\n",
    "            target = r + gamma * max_next_q * (1 - d)\n",
    "\n",
    "            loss = nn.MSELoss()(q_values, target.detach())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    eps = max(0.01, eps * 0.995)\n",
    "    if episode % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
