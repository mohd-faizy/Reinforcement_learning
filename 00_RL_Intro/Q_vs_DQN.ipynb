{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbb488b",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 1. **Q-Learning (Vanilla Q-Learning)**\n",
    "\n",
    "Q-learning is a **model-free reinforcement learning algorithm** that learns the value of taking an action in a given state.\n",
    "It uses a **Q-table** (state-action value table) to store values.\n",
    "\n",
    "**Update Rule**:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[ r + \\gamma \\max_{a'} Q(s', a') - Q(s,a) \\Big]\n",
    "$$\n",
    "\n",
    "* $s$: current state\n",
    "* $a$: action taken\n",
    "* $r$: reward\n",
    "* $s'$: next state\n",
    "* $\\alpha$: learning rate\n",
    "* $\\gamma$: discount factor\n",
    "\n",
    "ðŸ‘‰ Works well in **small discrete state spaces**, but struggles with large or continuous spaces since the Q-table becomes huge.\n",
    "\n",
    "**Example:**\n",
    "Suppose an agent in a grid world wants to reach a goal.\n",
    "\n",
    "* States = grid cells\n",
    "* Actions = {up, down, left, right}\n",
    "* The Q-table might look like:\n",
    "\n",
    "| State | Up  | Down | Left | Right |\n",
    "| ----- | --- | ---- | ---- | ----- |\n",
    "| (0,0) | 0   | 0.2  | 0    | 0.1   |\n",
    "| (0,1) | 0.5 | 0.1  | 0.3  | 0.4   |\n",
    "\n",
    "The agent updates this table until it learns the best path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4bb43c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (4,) into shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m n_bins = (\u001b[32m6\u001b[39m, \u001b[32m12\u001b[39m)\n\u001b[32m     10\u001b[39m obs_space = np.array([env.observation_space.low, env.observation_space.high]).T\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mobs_space\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m = [\u001b[32m4.8\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m0.418\u001b[39m, \u001b[32m5\u001b[39m]  \u001b[38;5;66;03m# clip values\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdiscretize\u001b[39m(obs):\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Convert continuous state (angle, angular velocity) into discrete bins\u001b[39;00m\n\u001b[32m     15\u001b[39m     ratios = [(obs[i] + \u001b[38;5;28mabs\u001b[39m(obs_space[i][\u001b[32m0\u001b[39m])) / (obs_space[i][\u001b[32m1\u001b[39m] - obs_space[i][\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m]]\n",
      "\u001b[31mValueError\u001b[39m: could not broadcast input array from shape (4,) into shape (2,)"
     ]
    }
   ],
   "source": [
    "# Q-Learning with Discretization (Beginner Friendly)\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Make the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Discretize only 2 important variables (pole angle, pole velocity)\n",
    "n_bins = (6, 12)\n",
    "obs_space = np.array([env.observation_space.low, env.observation_space.high]).T\n",
    "obs_space[1] = [4.8, 5, 0.418, 5]  # clip values\n",
    "\n",
    "def discretize(obs):\n",
    "    # Convert continuous state (angle, angular velocity) into discrete bins\n",
    "    ratios = [(obs[i] + abs(obs_space[i][0])) / (obs_space[i][1] - obs_space[i][0]) for i in [2,3]]\n",
    "    new_obs = [int(round((n_bins[i] - 1) * ratios[i])) for i in range(2)]\n",
    "    return tuple(np.clip(new_obs, 0, np.array(n_bins)-1))\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros(n_bins + (env.action_space.n,))\n",
    "alpha, gamma, eps = 0.1, 0.99, 1.0\n",
    "\n",
    "# Training loop\n",
    "for episode in range(2000):\n",
    "    obs, _ = env.reset()\n",
    "    state = discretize(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action\n",
    "        if np.random.rand() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        # Step in environment\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = discretize(next_obs)\n",
    "\n",
    "        # Q-learning update rule\n",
    "        best_next = np.max(Q[next_state])\n",
    "        Q[state + (action,)] += alpha * (reward + gamma * best_next - Q[state + (action,)])\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Decay exploration\n",
    "    eps = max(0.01, eps * 0.995)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc5c36",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 2. **Deep Q-Learning (DQN or D-Q Learning)**\n",
    "\n",
    "Instead of a Q-table, we use a **neural network** to approximate the Q-function:\n",
    "\n",
    "$$\n",
    "Q(s,a;\\theta) \\approx Q(s,a)\n",
    "$$\n",
    "\n",
    "* $\\theta$: parameters of the neural network\n",
    "* Input: state (can be high-dimensional, e.g., images)\n",
    "* Output: Q-values for each possible action\n",
    "\n",
    "### Key Features of DQN\n",
    "\n",
    "1. **Experience Replay**: Store past experiences $(s,a,r,s')$ in a replay buffer, and sample mini-batches to break correlation between consecutive updates.\n",
    "2. **Target Network**: Maintain a separate network for stable Q-value updates.\n",
    "\n",
    "**Update Rule (with NN):**\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\Big[ r + \\gamma \\max_{a'} Q(s',a';\\theta^-) - Q(s,a;\\theta) \\Big]^2\n",
    "$$\n",
    "\n",
    "where $\\theta^-$ are the parameters of the target network.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Playing Atari (Breakout ðŸŽ®)\n",
    "\n",
    "* **Q-Learning**: Not feasible (huge state space, every pixel arrangement is a state).\n",
    "* **DQN**: Input the raw image into a convolutional neural network â†’ output Q-values for {move left, move right, fire}.\n",
    "\n",
    "  * Example: The NN might learn that in state (ball near paddle, moving right), the Q-value for action â€œmove rightâ€ is highest.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Key Differences\n",
    "\n",
    "| Feature              | Q-Learning                    | Deep Q-Learning (DQN)                            |        |   |   |                     |\n",
    "| -------------------- | ----------------------------- | ------------------------------------------------ | ------ | - | - | ------------------- |\n",
    "| Value Representation | **Q-table** (explicit lookup) | **Neural Network** (function approximation)      |        |   |   |                     |\n",
    "| State Space          | Small, discrete               | Large/continuous, high-dimensional               |        |   |   |                     |\n",
    "| Memory               | Needs table of size (         | S                                                | \\times | A | ) | Needs weights of NN |\n",
    "| Stability            | More stable, but limited      | Needs tricks (experience replay, target network) |        |   |   |                     |\n",
    "| Applications         | Gridworld, simple games       | Atari, robotics, real-world tasks                |        |   |   |                     |\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "\n",
    "* **Q-learning** = Good for small toy problems.\n",
    "* **DQN (Deep Q-learning)** = Scales Q-learning using neural nets â†’ can solve complex problems like playing video games or controlling robots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d4dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 18.0\n",
      "Episode 50, Reward: 18.0\n",
      "Episode 100, Reward: 74.0\n",
      "Episode 150, Reward: 117.0\n",
      "Episode 200, Reward: 35.0\n",
      "Episode 250, Reward: 26.0\n",
      "Episode 300, Reward: 187.0\n",
      "Episode 350, Reward: 500.0\n",
      "Episode 400, Reward: 334.0\n",
      "Episode 450, Reward: 229.0\n"
     ]
    }
   ],
   "source": [
    "# Deep Q-Learning with Neural Network (Beginner Friendly)\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Make the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Neural Network to approximate Q-values\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# Setup\n",
    "state_dim, action_dim = env.observation_space.shape[0], env.action_space.n\n",
    "policy_net = DQN(state_dim, action_dim)\n",
    "target_net = DQN(state_dim, action_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "batch_size, gamma, eps = 64, 0.99, 1.0\n",
    "\n",
    "def select_action(state):\n",
    "    if random.random() < eps:\n",
    "        return env.action_space.sample()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    return policy_net(state).argmax().item()\n",
    "\n",
    "# Training loop\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Save experience\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Train if enough samples\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            s, a, r, ns, d = zip(*batch)\n",
    "\n",
    "            s = torch.FloatTensor(s)\n",
    "            a = torch.LongTensor(a).unsqueeze(1)\n",
    "            r = torch.FloatTensor(r).unsqueeze(1)\n",
    "            ns = torch.FloatTensor(ns)\n",
    "            d = torch.FloatTensor(d).unsqueeze(1)\n",
    "\n",
    "            # Q(s,a)\n",
    "            q_values = policy_net(s).gather(1, a)\n",
    "\n",
    "            # Target: r + gamma * max_a' Q(s',a')\n",
    "            max_next_q = target_net(ns).max(1, keepdim=True)[0]\n",
    "            target = r + gamma * max_next_q * (1 - d)\n",
    "\n",
    "            # Loss + update\n",
    "            loss = nn.MSELoss()(q_values, target.detach())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Update exploration\n",
    "    eps = max(0.01, eps * 0.995)\n",
    "\n",
    "    # Copy weights to target network every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
