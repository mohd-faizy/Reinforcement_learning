{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d71407",
   "metadata": {},
   "source": [
    "# **✨State-Value $V^\\pi(s)$ & Action-Value Functions $Q^\\pi(s, a)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdfcdfc",
   "metadata": {},
   "source": [
    "## **📑Table of Contents**\n",
    "\n",
    "1. [Policies $π$ and State-Value Functions $V^π(s)$](#1-policies-π-and-state-value-functions-vπs)\n",
    "   - 1.1 [Policy](#11-policy)\n",
    "   - 1.2 [Grid World Policy Example](#12-grid-world-policy-example)\n",
    "   - 1.3 [State-Value Functions $V^π(s)$](#13-state-value-functions-vπs)\n",
    "   - 1.4 [Computing State-Values with Recursion](#14-computing-state-values-with-recursion)\n",
    "\n",
    "2. [The Bellman Equation](#2-the-bellman-equation)\n",
    "   - 2.1 [Bellman Equation (Recursive Form)](#21-bellman-equation-recursive-form)\n",
    "   - 2.2 [Mathematical Foundation of the Bellman Equation](#22-mathematical-foundation-of-the-bellman-equation)\n",
    "   - 2.3 [Intuitive Understanding](#23-intuitive-understanding)\n",
    "   - 2.4 [Bellman Equation Implementation](#24-bellman-equation-implementation)\n",
    "   - 2.5 [Bellman Optimality Equation](#25-bellman-optimality-equation)\n",
    "   - 2.6 [Properties and Convergence](#26-properties-and-convergence)\n",
    "\n",
    "3. [Action-Value Functions $Q^π(s, a)$](#3-action-value-functions-qπs)\n",
    "   - 3.1 [What $Q^π(s,a)$ Means](#31-what-qπsa-means)\n",
    "   - 3.2 [Break the Return into Immediate + Future (Derivation)](#32-break-the-return-into-immediate--future-derivation)\n",
    "   - 3.3 [Deterministic One-step Bellman Form](#33-deterministic-one-step-bellman-form)\n",
    "   - 3.4 [Stochastic Transitions — Expectation over Next States](#34-stochastic-transitions--expectation-over-next-states)\n",
    "   - 3.5 [Bellman Expectation in Terms of $Q$ (No $V$ Needed)](#35-bellman-expectation-in-terms-of-q-no-v-needed)\n",
    "   - 3.6 [Relationship $V^π ↔ Q^π$](#36-relationship-vπ--qπ)\n",
    "   - 3.7 [Bellman Optimality Equation (Why the Max Appears)](#37-bellman-optimality-equation-why-the-max-appears)\n",
    "   - 3.8 [Sample-based Updates (How Algorithms Use These Formulas)](#38-sample-based-updates-how-algorithms-use-these-formulas)\n",
    "   - 3.9 [Terminal-state Conventions](#39-terminal-state-conventions)\n",
    "   - 3.10 [Quick Intuitive Checklist](#310-quick-intuitive-checklist)\n",
    "   - 3.11 [Quick Comparison: $V^π$ vs $Q^π$](#311-quick-comparison-vπ-vs-qπ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb74df",
   "metadata": {},
   "source": [
    "## **🔖1. Policies $\\pi$ and State-Value Functions $V^\\pi(s)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a095d",
   "metadata": {},
   "source": [
    "### 1.1 **Policy** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8634c037",
   "metadata": {},
   "source": [
    "> Policies: A `policy` $\\pi$  is a strategy that defines how an `agent` selects `actions` based on its `current state` to maximize expected `rewards`.\n",
    "\n",
    "#### Mathematical Foundation\n",
    "- **Definition**: Policy $\\pi$ are the core decision-making mechanism that maps environmental states to specific actions.\n",
    "\n",
    "#### Policy Types:\n",
    "- **Deterministic Policy**: $\\pi(s) = a$\n",
    "    - Maps each state to exactly one action\n",
    "    - Simpler to analyze and implement\n",
    "    - Sufficient for optimal policies in MDPs\n",
    "\n",
    "  - **Stochastic Policy**: $\\pi(a|s) = P(A_t = a | S_t = s)$\n",
    "    - Probability distribution over actions for each state\n",
    "    - More flexible for exploration\n",
    "    - Required for some advanced algorithms\n",
    "\n",
    "#### Mathematical Properties:\n",
    "- For stochastic policies: $\\sum_a \\pi(a|s) = 1$ for all states $s$\n",
    "    - Where:\n",
    "        - $\\pi(a|s)$ = Probability of selecting action $a$ in state $s$\n",
    "        - Sum over all actions must equal 1 (valid probability distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f03f1",
   "metadata": {},
   "source": [
    "### 1.2 **Grid World Policy Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6db5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CUSTOM 3x3 GRIDWORLD WITH POLICY AND STATE-VALUES ===\n",
      "\n",
      "Custom 3x3 GridWorld Layout:\n",
      "S    \n",
      "  M M\n",
      "    D\n",
      "\n",
      "State-values: {0: 1, 1: 8, 2: 9, 3: 2, 4: 7, 5: 10, 6: 3, 7: 5, 8: 0}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CUSTOM 3x3 GRIDWORLD WITH POLICY AND STATE-VALUES ===\")\n",
    "print()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Custom 3x3 GridWorld Env\n",
    "# -------------------------------\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"ansi\"]}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shape = (3, 3)\n",
    "        self.observation_space = spaces.Discrete(9)\n",
    "        self.action_space = spaces.Discrete(4)  # 0:left,1:down,2:right,3:up\n",
    "        \n",
    "        # Define rewards\n",
    "        self.terminal_state = 8\n",
    "        self.rewards = {8: 10, 4: -2, 7: -2}\n",
    "        \n",
    "        # Precompute P like Gym\n",
    "        self.P = {s: {a: [] for a in range(4)} for s in range(9)}\n",
    "        for s in range(9):\n",
    "            for a in range(4):\n",
    "                ns = self._move(s, a)\n",
    "                r = self.rewards.get(ns, -1)\n",
    "                done = ns == self.terminal_state\n",
    "                self.P[s][a] = [(1.0, ns, r, done)]  # deterministic\n",
    "                \n",
    "    def _move(self, state, action):\n",
    "        if state == self.terminal_state:\n",
    "            return state\n",
    "        row, col = state // 3, state % 3\n",
    "        if action == 0:    # left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 1:  # down\n",
    "            row = min(2, row + 1)\n",
    "        elif action == 2:  # right\n",
    "            col = min(2, col + 1)\n",
    "        elif action == 3:  # up\n",
    "            row = max(0, row - 1)\n",
    "        return row * 3 + col\n",
    "    \n",
    "    def render(self, mode=\"ansi\"):\n",
    "        grid = np.full(self.shape, \" \")\n",
    "        grid[0,0] = \"S\"  # start\n",
    "        grid[2,2] = \"D\"  # diamond\n",
    "        grid[1,1] = grid[1,2] = \"M\"  # mountains\n",
    "        return \"\\n\".join([\" \".join(row) for row in grid])\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnv()\n",
    "num_states = env.observation_space.n\n",
    "gamma = 1\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define a deterministic policy\n",
    "# -------------------------------\n",
    "policy = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3,  # up\n",
    "    8: 0   # terminal\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Compute state values\n",
    "# -------------------------------\n",
    "def compute_state_value(state):\n",
    "    if state == env.terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "\n",
    "V = {s: compute_state_value(s) for s in range(num_states)}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Display results\n",
    "# -------------------------------\n",
    "print(\"Custom 3x3 GridWorld Layout:\")\n",
    "print(env.render())\n",
    "print()\n",
    "print(\"State-values:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d7ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy 1: {0: 'down', 1: 'right', 2: 'down', 3: 'down', 4: 'up', 5: 'down', 6: 'right', 7: 'up'}\n",
      "Policy 2: {0: 'right', 1: 'right', 2: 'down', 3: 'right', 4: 'right', 5: 'down', 6: 'right', 7: 'right'}\n",
      "\n",
      "2. STATE-VALUE FUNCTIONS\n",
      "========================\n",
      "V(s) = Expected return starting from state s following policy π\n",
      "\n",
      "Computing state values...\n",
      "\n",
      "RESULTS:\n",
      "========\n",
      "State-values for Policy 1: {0: 1.0, 1: 8.0, 2: 9.0, 3: 2.0, 4: 7.0, 5: 10.0, 6: 3.0, 7: 5.0, 8: 0}\n",
      "State-values for Policy 2: {0: 7.0, 1: 8.0, 2: 9.0, 3: 7.0, 4: 9.0, 5: 10.0, 6: 8.0, 7: 10.0, 8: 0}\n",
      "\n",
      "EXAMPLE CALCULATION (Policy 1, State 2):\n",
      "========================================\n",
      "State 2 → Action down → State 5\n",
      "Reward: -1\n",
      "V(2) = -1 + 1.0 × V(5) = -1 + 1.0 × 10.0 = 9.0\n",
      "\n",
      "POLICY COMPARISON:\n",
      "==================\n",
      "Total value (Policy 1): 45.0\n",
      "Total value (Policy 2): 68.0\n",
      "Better policy: Policy 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# 1. POLICIES\n",
    "# ==========================\n",
    "\n",
    "# Actions: 0: left, 1: down, 2: right, 3: up\n",
    "policy1 = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3   # up\n",
    "}\n",
    "\n",
    "policy2 = {\n",
    "    0: 2,  # right\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 2,  # right\n",
    "    4: 2,  # right\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 2   # right\n",
    "}\n",
    "\n",
    "action_names = {0: 'left', 1: 'down', 2: 'right', 3: 'up'}\n",
    "\n",
    "print(\"Policy 1:\", {s: action_names[a] for s, a in policy1.items()})\n",
    "print(\"Policy 2:\", {s: action_names[a] for s, a in policy2.items()})\n",
    "print()\n",
    "\n",
    "# ==========================\n",
    "# 2. ENVIRONMENT MODEL (P)\n",
    "# ==========================\n",
    "gamma = 1.0          # Discount factor\n",
    "num_states = 9       # 3x3 grid\n",
    "terminal_state = 8   # Diamond\n",
    "\n",
    "# Build transition table: P[state][action] = [(prob, next_state, reward, done)]\n",
    "P = {s: {a: [] for a in range(4)} for s in range(num_states)}\n",
    "\n",
    "def move(state, action):\n",
    "    \"\"\"Return next state after taking an action.\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return state\n",
    "    \n",
    "    row, col = state // 3, state % 3\n",
    "    if action == 0:    # left\n",
    "        col = max(0, col - 1)\n",
    "    elif action == 1:  # down\n",
    "        row = min(2, row + 1)\n",
    "    elif action == 2:  # right\n",
    "        col = min(2, col + 1)\n",
    "    elif action == 3:  # up\n",
    "        row = max(0, row - 1)\n",
    "    return row * 3 + col\n",
    "\n",
    "def reward(next_state):\n",
    "    \"\"\"Return reward for landing in next_state.\"\"\"\n",
    "    if next_state == 8:   # Diamond\n",
    "        return 10\n",
    "    elif next_state in [4, 7]:  # Mountains\n",
    "        return -2\n",
    "    else:  # All other states\n",
    "        return -1\n",
    "\n",
    "# Fill transition table\n",
    "for s in range(num_states):\n",
    "    for a in range(4):\n",
    "        ns = move(s, a)\n",
    "        r = reward(ns)\n",
    "        done = (ns == terminal_state)\n",
    "        P[s][a] = [(1.0, ns, r, done)]   # deterministic env\n",
    "\n",
    "# ==========================\n",
    "# 3. STATE-VALUE FUNCTIONS\n",
    "# ==========================\n",
    "print(\"2. STATE-VALUE FUNCTIONS\")\n",
    "print(\"========================\")\n",
    "print(\"V(s) = Expected return starting from state s following policy π\")\n",
    "print()\n",
    "\n",
    "def compute_state_value(state, policy):\n",
    "    \"\"\"Bellman expectation with env.P\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    transitions = P[state][action]\n",
    "    \n",
    "    value = 0\n",
    "    for prob, next_state, reward, _ in transitions:\n",
    "        value += prob * (reward + gamma * compute_state_value(next_state, policy))\n",
    "    return value\n",
    "\n",
    "# Calculate state values for both policies\n",
    "print(\"Computing state values...\")\n",
    "print()\n",
    "\n",
    "V1 = {s: compute_state_value(s, policy1) for s in range(num_states)}\n",
    "V2 = {s: compute_state_value(s, policy2) for s in range(num_states)}\n",
    "\n",
    "# ==========================\n",
    "# 4. RESULTS\n",
    "# ==========================\n",
    "print(\"RESULTS:\")\n",
    "print(\"========\")\n",
    "print(\"State-values for Policy 1:\", V1)\n",
    "print(\"State-values for Policy 2:\", V2)\n",
    "print()\n",
    "\n",
    "# Example calculation walkthrough\n",
    "print(\"EXAMPLE CALCULATION (Policy 1, State 2):\")\n",
    "print(\"========================================\")\n",
    "state = 2\n",
    "action = policy1[state]\n",
    "prob, next_state, reward, _ = P[state][action][0]\n",
    "print(f\"State 2 → Action {action_names[action]} → State {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"V(2) = {reward} + {gamma} × V({next_state}) = {reward} + {gamma} × {V1[next_state]} = {V1[2]}\")\n",
    "print()\n",
    "\n",
    "# Compare policies\n",
    "print(\"POLICY COMPARISON:\")\n",
    "print(\"==================\")\n",
    "total1 = sum(V1[s] for s in range(8))  # exclude terminal\n",
    "total2 = sum(V2[s] for s in range(8))\n",
    "print(f\"Total value (Policy 1): {total1}\")\n",
    "print(f\"Total value (Policy 2): {total2}\")\n",
    "print(f\"Better policy: Policy {'2' if total2 > total1 else '1'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a9444a",
   "metadata": {},
   "source": [
    "### 1.3 **State-Value Functions $V^\\pi(s)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa62ed",
   "metadata": {},
   "source": [
    "**Definition:**\n",
    "- The **state-value function** $V^{\\pi}(s)$ estimates how good it is to be in a given state $s$.\n",
    "- The state-value function $V^{\\pi}(s)$ represents the expected cumulative discounted reward starting from state $s$ and following policy $\\pi$.\n",
    "\n",
    "**Formal Definition**:\n",
    "- $V^{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]$\n",
    "- Where:\n",
    "    - $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ is the discounted return\n",
    "    - $\\mathbb{E}_{\\pi}$ denotes expectation under policy $\\pi$\n",
    "    - $\\gamma$ is the discount factor (0 ≤ γ ≤ 1)\n",
    "  \n",
    "- Expanded Mathematical Form:\n",
    "    - $V(s) = r_{s+1} + \\gamma r_{s+2} + \\gamma^2 r_{s+3} + \\cdots + \\gamma^{n-1} r_{s+n}$\n",
    "    - $V^{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s]$\n",
    "\n",
    "- **Interpretation:**\n",
    "    - $r_{s+1}$: Immediate reward after leaving state $s$\n",
    "    - $\\gamma r_{s+2}$: Next reward, discounted by factor $\\gamma$\n",
    "    - $\\gamma^2 r_{s+3}$: Reward two steps later, discounted further\n",
    "    - $\\cdots$ Continues infinitely\n",
    "    - $\\gamma \\in [0,1]$: Discount factor that balances **present vs. future rewards**\n",
    "\n",
    "- **When to use:**\n",
    "    - **Conceptual / Theoretical explanation** of value functions.\n",
    "    - When introducing RL to beginners → easy to show “why future rewards are discounted.”\n",
    "    - To **manually calculate returns** in very short episodes (e.g., toy problems like a 3-step grid world).\n",
    "    - Useful in **Monte Carlo methods**, where we sample entire episodes and directly compute the return.\n",
    "    - Not practical for real-world problems because we can’t compute `inf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcce1b",
   "metadata": {},
   "source": [
    "#### Grid World State-Values Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83527312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values:\n",
      "V(0) = 0\n",
      "V(1) = 7\n",
      "V(2) = 8\n",
      "V(3) = 1\n",
      "V(4) = 5\n",
      "V(5) = 9\n",
      "V(6) = 2\n",
      "V(7) = 3\n",
      "V(8) = 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Computes how valuable each state is under a fixed action policy in a 3x3 grid world.\n",
    "- Simulates following the policy from each state to accumulate rewards until reaching a terminal state or revisiting a state.\n",
    "- Uses a simple deterministic transition function mapping an action and current position to a next position.\n",
    "- Assigns rewards: -2 for mountain states, +10 for the terminal diamond state, and -1 for other states.\n",
    "- Stops simulation to avoid infinite loops or once the terminal state is reached, adding its reward.\n",
    "- Prints the total expected reward (state value) starting from each state when following the policy.\n",
    "'''\n",
    "\n",
    "def compute_state_value(state, policy, gamma=1.0):\n",
    "    \"\"\"Compute state value for deterministic policy\"\"\"\n",
    "    if state == 8:  # Terminal state (diamond)\n",
    "        return 0\n",
    "    \n",
    "    # Simulate following policy from this state\n",
    "    current_state = state\n",
    "    total_reward = 0\n",
    "    visited = set()\n",
    "    \n",
    "    while current_state not in visited and current_state != 8:\n",
    "        visited.add(current_state)\n",
    "        action = policy[current_state]\n",
    "        \n",
    "        # Get reward for current state (simplified grid world)\n",
    "        if current_state in [4, 7]:  # Mountain states\n",
    "            reward = -2\n",
    "        elif current_state == 8:     # Diamond state  \n",
    "            reward = 10\n",
    "        else:                       # Other states\n",
    "            reward = -1\n",
    "            \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Move to next state based on action (simplified transitions)\n",
    "        next_state = get_next_state(current_state, action)\n",
    "        current_state = next_state\n",
    "    \n",
    "    # Add terminal reward if reached\n",
    "    if current_state == 8:\n",
    "        total_reward += 10\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    \"\"\"Simple deterministic transition function for 3x3 grid\"\"\"\n",
    "    # Convert state to row, col\n",
    "    row, col = state // 3, state % 3\n",
    "    \n",
    "    if action == 0:    # left\n",
    "        col = max(0, col - 1)\n",
    "    elif action == 1:  # down  \n",
    "        row = min(2, row + 1)\n",
    "    elif action == 2:  # right\n",
    "        col = min(2, col + 1)\n",
    "    elif action == 3:  # up\n",
    "        row = max(0, row - 1)\n",
    "    \n",
    "    return row * 3 + col\n",
    "\n",
    "\n",
    "# Define policy including terminal state with a dummy action (e.g. 0)\n",
    "grid_policy = {0:1, 1:2, 2:1, 3:1, 4:3, 5:1, 6:2, 7:3, 8:0}\n",
    "state_values = {}\n",
    "\n",
    "for state in range(9):\n",
    "    state_values[state] = compute_state_value(state, grid_policy)\n",
    "\n",
    "print(\"State Values:\")\n",
    "\n",
    "for state, value in state_values.items():\n",
    "    print(f\"V({state}) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04709b39",
   "metadata": {},
   "source": [
    "- **State-Value Interpretation:**\n",
    "  \n",
    "    - **High Values**: States that lead to goal with minimal cost\n",
    "        - Closer to diamond with fewer mountain encounters\n",
    "        - Optimal pathways through environment\n",
    "\n",
    "    - **Low Values**: States requiring longer paths or mountain traversal\n",
    "        - Farther from goal or poor intermediate positions\n",
    "        - Suboptimal or risky pathways\n",
    "\n",
    "    - **Zero Value**: Terminal goal state\n",
    "      - No further rewards possible from terminal state\n",
    "      - Standard convention in episodic tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f875fb",
   "metadata": {},
   "source": [
    "### 1.4 **Computing State-Values with Recursion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a00779",
   "metadata": {},
   "source": [
    "```python\n",
    "def compute_state_value(state):  \n",
    "    if state == terminal_state: \n",
    "        return 0 \n",
    " \n",
    "    action = policy[state]  \n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]  \n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dfaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values for FrozenLake:\n",
      "V(0)=-3.00 | V(1)=8.00 | V(2)=9.00 | V(3)=-2.00\n",
      "V(4)=-4.00 | V(5)=10.00 | V(6)=-1.00 | V(7)=-2.00\n",
      "V(8)=10.00 | V(9)=0.00 | V(10)=0.00 | V(11)=0.00\n",
      "V(12)=0.00 | V(13)=0.00 | V(14)=0.00 | V(15)=0.00\n"
     ]
    }
   ],
   "source": [
    "def compute_state_value_recursive(state, policy, env, gamma=1.0, memo=None, visited=None, max_depth=100):\n",
    "    \"\"\"\n",
    "    Compute state value using recursive Bellman equation\n",
    "    with memoization to handle cycles and recursion depth limit\n",
    "    \"\"\"\n",
    "    if memo is None:\n",
    "        memo = {}\n",
    "        \n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    # Base case: terminal state or max recursion depth reached\n",
    "    if state == 15 or max_depth <= 0:  # terminal state or max depth\n",
    "        return 0\n",
    "\n",
    "    # Avoid cycles/infinite loops\n",
    "    if state in visited:\n",
    "        # Return 0 or memoized value to break cycle\n",
    "        return memo.get(state, 0)\n",
    "\n",
    "    # Check memo to avoid recomputation\n",
    "    if state in memo:\n",
    "        return memo[state]\n",
    "\n",
    "    visited.add(state)\n",
    "\n",
    "    # Get action from policy\n",
    "    action = policy.get(state, 0)  # default action can be 0 if not in policy\n",
    "\n",
    "    # For stochastic environments, compute expected value\n",
    "    expected_value = 0\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "\n",
    "    for prob, next_state, reward, is_terminal in transitions:\n",
    "        if is_terminal:\n",
    "            value_contribution = reward\n",
    "        else:\n",
    "            value_contribution = reward + gamma * compute_state_value_recursive(\n",
    "                next_state, policy, env, gamma, memo, visited, max_depth - 1)\n",
    "        expected_value += prob * value_contribution\n",
    "\n",
    "    visited.remove(state)\n",
    "    memo[state] = expected_value\n",
    "    return expected_value\n",
    "\n",
    "\n",
    "# Example usage (assumes 'env' is the FrozenLake environment and 'frozen_lake_policy' is defined)\n",
    "\n",
    "gamma = 1.0\n",
    "\n",
    "frozen_lake_policy = {\n",
    "    0: 1, 1: 2, 2: 1, 3: 1,    # First row\n",
    "    4: 1, 5: 1, 6: 1, 7: 1,    # Second row  \n",
    "    8: 2, 9: 1, 10: 1, 11: 1,  # Third row\n",
    "    12: 2, 13: 2, 14: 2        # Fourth row (excluding terminal)\n",
    "}\n",
    "\n",
    "V = {}\n",
    "for state in range(env.observation_space.n):\n",
    "    V[state] = compute_state_value_recursive(state, frozen_lake_policy, env, gamma)\n",
    "\n",
    "print(\"State Values for FrozenLake:\")\n",
    "for i in range(4):\n",
    "    row = [f\"V({j})={V.get(j, 0):.2f}\" for j in range(i*4, (i+1)*4)]\n",
    "    print(\" | \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40463ba",
   "metadata": {},
   "source": [
    "- **Code Explanation:**\n",
    "    - **Memoization**: Prevents infinite recursion and improves efficiency\n",
    "    - **Stochastic Handling**: Computes expected value over all possible transitions\n",
    "    - **Terminal Handling**: Properly handles terminal states with zero continuation value\n",
    "    - **Expected Value**: Weighted sum of outcomes by their probabilities\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5545764",
   "metadata": {},
   "source": [
    "## **🔖2. The Bellman Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a2b94",
   "metadata": {},
   "source": [
    "### 2.1 **Bellman Equation (Recursive Form)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16890b",
   "metadata": {},
   "source": [
    "- **Primary Concept:**\n",
    "  - $V(s) = r_{s+1} + \\gamma V(s+1)$\n",
    "    - Where:\n",
    "      - $r_{s+1}$ → Reward immediately after leaving state $s$\n",
    "      - $\\gamma V(s+1)$ → Discounted value of the next state, turning an infinite sum into a **recursive relationship**\n",
    "\n",
    "- **State-Value Function Under Policy $\\pi$:**\n",
    "  - $V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P_{ss'}^a \\left[ R_{ss'}^a + \\gamma V^{\\pi}(s') \\right]$\n",
    "  - For deterministic policies:\n",
    "    - $V^{\\pi}(s) = \\sum_{s'} P_{ss'}^{\\pi(s)} \\left[ R_{ss'}^{\\pi(s)} + \\gamma V^{\\pi}(s') \\right]$\n",
    "    - Where:\n",
    "      - $\\pi(s) $ = Action selected by deterministic policy in state $s$\n",
    "      - $P_{ss'}^{\\pi(s)}$ = Transition probability under policy action\n",
    "      - $R_{ss'}^{\\pi(s)}$ = Expected reward for the transition\n",
    "      - $\\gamma$ = Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba92f9",
   "metadata": {},
   "source": [
    "### 2.2 **Mathematical Foundation of the Bellman Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e1fb1",
   "metadata": {},
   "source": [
    "- **Primary Concept:**\n",
    "  - The **Bellman Equation** is the cornerstone of dynamic programming in reinforcement learning, providing a recursive relationship between the value of a state and the values of its successor states.\n",
    "\n",
    "- **Complete Mathematical Formulation:**\n",
    "\n",
    "  - **For State-Value Functions:**\n",
    "    - $V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V^{\\pi}(s') \\right]$\n",
    "\n",
    "  - **For Deterministic Policies (simplified):**\n",
    "    - $V^{\\pi}(s) = \\sum_{s'} P(s'|s,\\pi(s)) \\left[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s') \\right]$\n",
    "\n",
    "  - **For Deterministic Environments (further simplified):**\n",
    "    - $V^{\\pi}(s) = R(s,\\pi(s)) + \\gamma V^{\\pi}(s')$\n",
    "\n",
    "- **Where:**\n",
    "  - $V^{\\pi}(s)$ = Value of state $s$ under policy $\\pi$\n",
    "  - $R(s,a,s')$ = Immediate reward for the transition from $s$ to $s'$ using action $$a$\n",
    "  - $\\gamma$ = Discount factor (0 ≤ $\\gamma$ ≤ 1)\n",
    "  - $P(s'|s,a)$ = Transition probability of moving to state $s'$ from $s$ by action $a$\n",
    "  - $s'$ = Next state after taking action $a $ in state $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12366de",
   "metadata": {},
   "source": [
    "### 2.3 **Intuitive Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca45f70",
   "metadata": {},
   "source": [
    "#### Why the Bellman Equation Works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48254f69",
   "metadata": {},
   "source": [
    "**Recursive Structure**: \n",
    "- The value of being in a state equals the immediate reward plus the (discounted) value of where you end up\n",
    "- This creates a system of equations that can be solved for optimal values\n",
    "\n",
    "**Optimality Principle**:\n",
    "- Optimal solutions have the property that remaining decisions are optimal for the subproblem starting from the current state\n",
    "- This enables breaking complex problems into simpler subproblems\n",
    "\n",
    "**Dynamic Programming Foundation**:\n",
    "- Enables bottom-up solution construction\n",
    "- Avoids recomputing overlapping subproblems\n",
    "- Guarantees convergence to optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03934d0",
   "metadata": {},
   "source": [
    "### 2.4 **Bellman Equation Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912e8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 30 iterations\n",
      "State Values computed using Bellman Equation:\n",
      " 0.015 |  0.015 |  0.034 |  0.015\n",
      " 0.021 |  0.000 |  0.066 |  0.000\n",
      " 0.054 |  0.159 |  0.219 |  0.000\n",
      " 0.000 |  0.313 |  0.570 |  0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bellman_equation_single_state(state, policy, env, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Apply Bellman equation to compute value of a single state\n",
    "    \"\"\"\n",
    "    if state >= env.observation_space.n - 1:  # Terminal state\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    expected_value = 0\n",
    "    \n",
    "    # Get all possible transitions for this state-action pair\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    \n",
    "    for prob, next_state, reward, is_terminal in transitions:\n",
    "        if is_terminal:\n",
    "            # Terminal transition: only immediate reward\n",
    "            value_contribution = reward\n",
    "        else:\n",
    "            # Non-terminal: immediate reward + discounted future value\n",
    "            value_contribution = reward + gamma * V[next_state]\n",
    "        \n",
    "        expected_value += prob * value_contribution\n",
    "    \n",
    "    return expected_value\n",
    "\n",
    "def bellman_update_all_states(policy, env, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Apply Bellman equation to update all state values\n",
    "    \"\"\"\n",
    "    new_V = V.copy()\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        if state < len(policy):  # Non-terminal state\n",
    "            new_V[state] = bellman_equation_single_state(state, policy, env, V, gamma)\n",
    "        else:\n",
    "            new_V[state] = 0  # Terminal state\n",
    "    \n",
    "    return new_V\n",
    "\n",
    "def iterative_policy_evaluation(policy, env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Iteratively apply Bellman equation until convergence\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_V = bellman_update_all_states(policy, env, V, gamma)\n",
    "        \n",
    "        # Check for convergence\n",
    "        delta = np.max(np.abs(new_V - V))\n",
    "        V = new_V\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "# Example usage with FrozenLake\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "# Simple policy: always go right when possible, otherwise down\n",
    "simple_policy = {\n",
    "    0: 2, 1: 2, 2: 2, 3: 1,     # Row 1: right, right, right, down\n",
    "    4: 2, 5: 1, 6: 1, 7: 1,     # Row 2: right, down, down, down  \n",
    "    8: 2, 9: 2, 10: 1, 11: 1,   # Row 3: right, right, down, down\n",
    "    12: 2, 13: 2, 14: 2         # Row 4: right, right, right\n",
    "}\n",
    "\n",
    "# Compute state values using Bellman equation\n",
    "V_bellman = iterative_policy_evaluation(simple_policy, env, gamma=0.9)\n",
    "\n",
    "print(\"State Values computed using Bellman Equation:\")\n",
    "for i in range(4):\n",
    "    row = [f\"{V_bellman[i*4 + j]:.3f}\" for j in range(4)]\n",
    "    print(\" | \".join(f\"{val:>6}\" for val in row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b88ef",
   "metadata": {},
   "source": [
    "#### Code Explanation:\n",
    "- **Single State Update**: Applies Bellman equation to compute new value for one state\n",
    "- **Batch Update**: Updates all states using current value estimates\n",
    "- **Iterative Process**: Repeats until values converge (change less than threshold)\n",
    "- **Convergence Check**: Monitors maximum change in values across all states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59f698",
   "metadata": {},
   "source": [
    "### 2.5 **Bellman Optimality Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ddf73",
   "metadata": {},
   "source": [
    "#### Mathematical Formulation:\n",
    "- **For Optimal State Values**:\n",
    "  - $V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$\n",
    "\n",
    "- **For Optimal Action Values**:\n",
    "  - $Q^*(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')]$\n",
    "\n",
    "#### Key Differences from Policy-Specific Bellman Equation:\n",
    "- **Optimization**: Takes maximum over actions instead of following fixed policy\n",
    "- **Optimal Values**: Represents best possible performance from each state\n",
    "- **Policy Independence**: Defines optimal values regardless of current policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f768429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged after 78 iterations\n",
      "\n",
      "Optimal State Values:\n",
      " 0.069 |  0.061 |  0.074 |  0.056\n",
      " 0.092 |  0.000 |  0.112 |  0.000\n",
      " 0.145 |  0.247 |  0.300 |  0.000\n",
      " 0.000 |  0.380 |  0.639 |  0.000\n"
     ]
    }
   ],
   "source": [
    "def bellman_optimality_operator(V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Apply Bellman optimality operator to find optimal values\n",
    "    \"\"\"\n",
    "    new_V = np.zeros_like(V)\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal state\n",
    "        action_values = []\n",
    "        \n",
    "        for action in range(env.action_space.n):\n",
    "            action_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    action_value += prob * reward\n",
    "                else:\n",
    "                    action_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            action_values.append(action_value)\n",
    "        \n",
    "        # Take maximum over all actions\n",
    "        new_V[state] = max(action_values)\n",
    "    \n",
    "    return new_V\n",
    "\n",
    "def value_iteration(env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Find optimal value function using value iteration\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_V = bellman_optimality_operator(V, env, gamma)\n",
    "        \n",
    "        delta = np.max(np.abs(new_V - V))\n",
    "        V = new_V\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "# Find optimal values\n",
    "V_optimal = value_iteration(env, gamma=0.9)\n",
    "print(\"\\nOptimal State Values:\")\n",
    "for i in range(4):\n",
    "    row = [f\"{V_optimal[i*4 + j]:.3f}\" for j in range(4)]\n",
    "    print(\" | \".join(f\"{val:>6}\" for val in row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84342b",
   "metadata": {},
   "source": [
    "### 2.6 **Properties and Convergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93035a",
   "metadata": {},
   "source": [
    "#### **Convergence Guarantees:**\n",
    "\n",
    "- **Contraction Mapping**: Bellman operator is a contraction when γ < 1\n",
    "    - Guaranteed convergence to unique fixed point\n",
    "    - Convergence rate depends on discount factor\n",
    "\n",
    "- **Fixed Point**: Optimal value function satisfies $V^* = T^*V^*$ where $T^*$ is Bellman optimality operator\n",
    "    - Solution to system of Bellman optimality equations\n",
    "    - Represents true optimal values\n",
    "\n",
    "#### Computational Complexity:\n",
    "- **Time Complexity**: $O(|S|²|A|)$ per iteration\n",
    "    - Must examine all state-action pairs\n",
    "    - Transition probabilities determine exact complexity\n",
    "\n",
    "- **Space Complexity**: $O(|S|)$\n",
    "    - Store value function for all states\n",
    "    - Additional space for transition probabilities\n",
    "\n",
    "- **Convergence Rate**: Geometric with rate γ\n",
    "    - Faster convergence with smaller discount factors\n",
    "    - Trade-off between solution quality and computation time\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29079ea4",
   "metadata": {},
   "source": [
    "## **🔖3. Action-Value Functions $Q^\\pi(s, a)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97cf5e",
   "metadata": {},
   "source": [
    "### 3.1 **what $Q^\\pi(s,a)$ means**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb4d5e7",
   "metadata": {},
   "source": [
    "* Formula:\n",
    "\n",
    "  $Q^\\pi(s,a) = \\mathbb{E}_\\pi\\big[G_t \\mid S_t=s, A_t=a\\big]$\n",
    "\n",
    "* Symbols:\n",
    "\n",
    "  * $G_t$ = total return from time $t$: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots$\n",
    "  * $\\mathbb{E}_\\pi[\\cdot]$ = expectation when actions after time $t$ are chosen according to policy $\\pi$.\n",
    "  * Condition $\\mid S_t=s, A_t=a$ means: **we start in state $s$ and take action $a$ now**; randomness afterward is from environment transitions and the policy.\n",
    "* Intuition: average (over possible randomness) of **immediate reward + discounted future rewards** after taking $a$ in $s$.\n",
    "* Tiny numeric example:\n",
    "\n",
    "  * Suppose $\\gamma=0.9$, immediate reward $r=2$, and expected future return $V^\\pi(s')=5$.\n",
    "  * Then $Q^\\pi(s,a)=2 + 0.9\\times 5 = 6.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ada4a",
   "metadata": {},
   "source": [
    "### 3.2 **Break the return into immediate + future (derivation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48debc0",
   "metadata": {},
   "source": [
    "* Start from definition:\n",
    "\n",
    "  $Q^\\pi(s,a) = \\mathbb{E}\\big[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\big]$\n",
    "\n",
    "* Because expectation is linear:\n",
    "\n",
    "  $Q^\\pi(s,a) = \\mathbb{E}[R_{t+1}\\mid s,a] \\;+\\; \\gamma \\,\\mathbb{E}[G_{t+1}\\mid s,a]$\n",
    "  \n",
    "* Interpretations:\n",
    "\n",
    "  * $\\mathbb{E}[R_{t+1}\\mid s,a]$ = expected immediate reward after doing $a$ in $s$.\n",
    "  * $\\mathbb{E}[G_{t+1}\\mid s,a]$ = expected future return from the next time step onward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496fa64",
   "metadata": {},
   "source": [
    "### 3.3 **Deterministic one-step Bellman form**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb808731",
   "metadata": {},
   "source": [
    "* Formula (deterministic next state $s'$ and deterministic immediate reward $r_a$):\n",
    "  \n",
    "  $Q^\\pi(s,a) = r_a + \\gamma V^\\pi(s')$\n",
    "\n",
    "* Why this follows:\n",
    "  * If taking $a$ from $s$ always lands in $s'$ and gives reward $r_a$, then $\\mathbb{E}[R_{t+1}\\mid s,a]=r_a$ and $\\mathbb{E}[G_{t+1}\\mid s,a]=V^\\pi(s')$.\n",
    "  \n",
    "* Intuition: **immediate reward** + **discounted value of the known next state**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b744e",
   "metadata": {},
   "source": [
    "### 3.4 **Stochastic transitions — expectation over next states**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575898ba",
   "metadata": {},
   "source": [
    "* Formula:\n",
    "\n",
    "  $Q^\\pi(s,a) = \\sum_{s'} P(s'\\mid s,a)\\,\\big[ R(s,a,s') + \\gamma V^\\pi(s')\\big]$\n",
    "\n",
    "* Explanation:\n",
    "\n",
    "  * If taking $a$ can lead to several possible next states $s'$, each with probability $P(s'|s,a)$, you average the immediate reward + future value for each possible $s'$.\n",
    "* Numeric example (showing every arithmetic step):\n",
    "\n",
    "  * Let two next states $s_1,s_2$ with probabilities $0.7$ and $0.3$.\n",
    "  * Rewards: $R(s,a,s_1)=1$, $R(s,a,s_2)=2$.\n",
    "  * Values: $V^\\pi(s_1)=3$, $V^\\pi(s_2)=4$.\n",
    "  * $\\gamma=0.9$.\n",
    "  * Compute for $s_1$: inner = $1 + 0.9\\times 3 = 2.7$.\n",
    "    * Multiply by prob: $0.7\\times 3.7 = 2.59$.\n",
    "  \n",
    "  * Compute for $s_2$: inner = $2 + 0.9\\times 4 = 5.6$.\n",
    "    * Multiply by prob: $0.3\\times 5.6 = 1.68$.\n",
    "    * So $Q^\\pi(s,a) = 2.59 + 1.68 = 4.27$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28dcf45",
   "metadata": {},
   "source": [
    "### 3.5 **Bellman expectation in terms of $Q$ (no $V$ needed)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfaf6ee",
   "metadata": {},
   "source": [
    "* Start: $V^\\pi(s') = \\sum_{a'} \\pi(a'\\mid s')\\,Q^\\pi(s',a')$.\n",
    "  \n",
    "* Substitute into previous expression:\n",
    "  $Q^\\pi(s,a) = \\sum_{s'} P(s'\\mid s,a)\\Big[ R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'\\mid s')\\,Q^\\pi(s',a')\\Big]$\n",
    "\n",
    "* Why useful:\n",
    "  * This is a **self-consistent equation** for $Q^\\pi$ only. Solve it (analytically or iteratively) to find the Q-values of a policy.\n",
    "\n",
    "* Iteration form (policy evaluation):\n",
    "  $Q_{k+1}(s,a) \\leftarrow \\sum_{s'} P(s'\\mid s,a)\\Big[R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'\\mid s') Q_k(s',a')\\Big]$\n",
    "\n",
    "  * Keep updating $Q$ until it converges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b7df6",
   "metadata": {},
   "source": [
    "### 3.6 **Relationship $V^\\pi$ ↔ $Q^\\pi$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8796731e",
   "metadata": {},
   "source": [
    "* Formula:\n",
    "\n",
    "  $V^\\pi(s) = \\sum_a \\pi(a\\mid s)\\,Q^\\pi(s,a)$\n",
    "\n",
    "* Meaning: state-value = **expected Q-value when actions are sampled from $\\pi$**.\n",
    "* Quick numeric example:\n",
    "\n",
    "  * Suppose two actions with probabilities $0.6$ and $0.4$.\n",
    "  * $Q(s,a_1)=10,\\; Q(s,a_2)=2$.\n",
    "  * Then $V(s)=0.6\\times 10 + 0.4\\times 2 = 6.8$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55ab6d",
   "metadata": {},
   "source": [
    "### 3.7 **Bellman *optimality* equation (why the $\\max$ appears)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721ba28",
   "metadata": {},
   "source": [
    "* Formula:\n",
    "  $Q^*(s,a) = \\sum_{s'} P(s'\\mid s,a)\\Big[ R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')\\Big]$\n",
    "\n",
    "* Explanation:\n",
    "  * $Q^*$ assumes **after taking action $a$ now we will act optimally thereafter**.\n",
    "  * So the future value from $s'$ is the **maximum** Q-value over all actions available in $s'$: $V^*(s')=\\max_{a'}Q^*(s',a')$\n",
    "\n",
    "* Small numeric illustration:\n",
    "  * Suppose deterministic next state $s'$, $R=1$, $\\gamma=0.9$, and $\\max_{a'}Q^*(s',a')=9$\n",
    "  * Then $Q^*(s,a)=1 + 0.9\\times 9 = 9.1$.\n",
    "\n",
    "* The optimal policy uses:\n",
    "  $\\pi^*(s) = \\arg\\max_a Q^*(s,a)$\n",
    "\n",
    "  * Pick the action that yields the largest $Q^*$ in that state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187acc2d",
   "metadata": {},
   "source": [
    "### 3.8 **Sample-based updates (how algorithms use these formulas)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00feb0ef",
   "metadata": {},
   "source": [
    "* **Q-Learning (off-policy)** one-step sample update (common in practice):\n",
    "\n",
    "  $Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[ r + \\gamma\\max_{a'}Q(s',a') - Q(s,a)\\big]$\n",
    "\n",
    "  * Use when you sample a transition $(s,a,r,s')$.\n",
    "  * Intuition: move $Q(s,a)$ toward the sample target $r + \\gamma\\max_{a'}Q(s',a')$.\n",
    "\n",
    "* **SARSA (on-policy)**:\n",
    "  $Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[ r + \\gamma Q(s',a') - Q(s,a)\\big]$\n",
    "\n",
    "  * Here $a'$ is the actual next action chosen by current policy; used when learning while following that policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c110ff0",
   "metadata": {},
   "source": [
    "### 3.9 **Terminal-state conventions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10762f1b",
   "metadata": {},
   "source": [
    "* Common options:\n",
    "  * Set $Q(\\text{terminal},a)=0$ for all $a$.\n",
    "  * Or skip/ignore those states in updates.\n",
    "  \n",
    "* Both are fine if applied consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416566e3",
   "metadata": {},
   "source": [
    "### 3.10 **Quick intuitive checklist** 🧭"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667fcec",
   "metadata": {},
   "source": [
    "* If you *know* the next state exactly → use $Q = r + \\gamma V(\\text{next})$.\n",
    "* If next state is random → average over next states with $P(s'|s,a)$.\n",
    "* If you have a policy $\\pi$ and want to compute its values → use the Bellman expectation (in terms of $Q$ or $V$).\n",
    "* If you want the best possible behavior → replace expected future actions by $\\max$ (Bellman optimality), then act greedily w\\.r.t. $Q^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43508c88",
   "metadata": {},
   "source": [
    "### Complete Q-Value Computation\n",
    "\n",
    "```python\n",
    "def compute_q_value(state, action):  \n",
    "    if state == terminal_state: \n",
    "        return None \n",
    " \n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0] \n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d75b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values:\n",
      "{(0, 0): np.float64(0.0), (0, 1): np.float64(0.0), (0, 2): np.float64(0.0), (0, 3): np.float64(0.0), (1, 0): np.float64(0.0), (1, 1): np.float64(0.0), (1, 2): np.float64(0.0), (1, 3): np.float64(0.0), (2, 0): np.float64(0.0), (2, 1): np.float64(0.0), (2, 2): np.float64(0.0), (2, 3): np.float64(0.0), (3, 0): np.float64(0.0), (3, 1): np.float64(0.0), (3, 2): np.float64(0.0), (3, 3): np.float64(0.0), (4, 0): np.float64(0.0), (4, 1): np.float64(0.0), (4, 2): np.float64(0.0), (4, 3): np.float64(0.0), (5, 0): np.float64(0.0), (5, 1): np.float64(0.0), (5, 2): np.float64(0.0), (5, 3): np.float64(0.0), (6, 0): np.float64(0.0), (6, 1): np.float64(0.0), (6, 2): np.float64(0.0), (6, 3): np.float64(0.0), (7, 0): np.float64(0.0), (7, 1): np.float64(0.0), (7, 2): np.float64(0.0), (7, 3): np.float64(0.0), (8, 0): np.float64(0.0), (8, 1): np.float64(0.0), (8, 2): np.float64(0.0), (8, 3): np.float64(0.0), (9, 0): np.float64(0.0), (9, 1): np.float64(0.0), (9, 2): np.float64(0.0), (9, 3): np.float64(0.0), (10, 0): np.float64(0.0), (10, 1): np.float64(0.0), (10, 2): np.float64(0.0), (10, 3): np.float64(0.0), (11, 0): np.float64(0.0), (11, 1): np.float64(0.0), (11, 2): np.float64(0.0), (11, 3): np.float64(0.0), (12, 0): np.float64(0.0), (12, 1): np.float64(0.0), (12, 2): np.float64(0.0), (12, 3): np.float64(0.0), (13, 0): np.float64(0.0), (13, 1): np.float64(0.0), (13, 2): np.float64(0.0), (13, 3): np.float64(0.0), (14, 0): np.float64(0.0), (14, 1): np.float64(0.0), (14, 2): np.float64(0.0), (14, 3): np.float64(1.9), (15, 0): 0, (15, 1): 0, (15, 2): 0, (15, 3): 0}\n",
      "\n",
      "Improved Policy:\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 3}\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 12, Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Import Required Libraries\n",
    "# ============================================\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. Initialize Environment & Parameters\n",
    "# ============================================\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "num_states = env.observation_space.n     # Number of states in FrozenLake\n",
    "num_actions = env.action_space.n         # Number of possible actions\n",
    "terminal_state = 15                      # Goal state index in FrozenLake\n",
    "gamma = 0.9                              # Discount factor\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. Initialize Value Function\n",
    "# ============================================\n",
    "# Value function stores the \"goodness\" of each state\n",
    "V = np.zeros(num_states)     # Start with all states = 0\n",
    "V[terminal_state] = 1.0      # Goal state is assigned value 1\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Q-Value Computation Function\n",
    "# ============================================\n",
    "def compute_q_value(state, action, V):\n",
    "    \"\"\"\n",
    "    Compute the Q-value for a given (state, action) pair.\n",
    "\n",
    "    Parameters:\n",
    "        state  (int): Current state\n",
    "        action (int): Action taken in this state\n",
    "        V      (array): Current value function\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated Q-value\n",
    "    \"\"\"\n",
    "    # Terminal state has no future rewards\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    # env.unwrapped.P[state][action] gives the transition dynamics:\n",
    "    # [(probability, next_state, reward, done), ...]\n",
    "    probability, next_state, reward, done = env.unwrapped.P[state][action][0]\n",
    "    \n",
    "    # Bellman expectation equation\n",
    "    return reward + gamma * V[next_state]\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. Compute Q-values for All State-Action Pairs\n",
    "# ============================================\n",
    "Q = {\n",
    "    (state, action): compute_q_value(state, action, V)\n",
    "    for state in range(num_states)\n",
    "    for action in range(num_actions)\n",
    "}\n",
    "\n",
    "print(\"Q-values:\")\n",
    "print(Q)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. Greedy Policy Improvement\n",
    "# ============================================\n",
    "def improve_policy(Q, num_states, num_actions):\n",
    "    \"\"\"\n",
    "    Improve policy using greedy selection:\n",
    "    For each state, pick the action with the maximum Q-value.\n",
    "    \"\"\"\n",
    "    improved_policy = {}\n",
    "    \n",
    "    for state in range(num_states - 1):  # Exclude terminal state\n",
    "        max_action = max(range(num_actions), key=lambda action: Q[(state, action)])\n",
    "        improved_policy[state] = max_action\n",
    "    \n",
    "    return improved_policy\n",
    "\n",
    "\n",
    "# Generate improved policy\n",
    "policy = improve_policy(Q, num_states, num_actions)\n",
    "\n",
    "print(\"\\nImproved Policy:\")\n",
    "print(policy)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. Test the Improved Policy\n",
    "# ============================================\n",
    "state, _ = env.reset()   # Reset environment to initial state\n",
    "terminated = False\n",
    "\n",
    "while not terminated:\n",
    "    action = policy[state]  # Select action according to policy\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"State: {state}, Reward: {reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fdccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for State 4:\n",
      "Q(4, Left) = 2.667\n",
      "Q(4, Down) = 2.333\n",
      "Q(4, Right) = 0.333\n",
      "Q(4, Up) = 2.667\n"
     ]
    }
   ],
   "source": [
    "def compute_q_value_deterministic(state, action, env, V, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Q-value for deterministic environment\n",
    "    \"\"\"\n",
    "    if state >= env.observation_space.n - 1:  # Terminal state\n",
    "        return 0\n",
    "    \n",
    "    # For deterministic transition, take first (and only) outcome\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    prob, next_state, reward, is_terminal = transitions[0]\n",
    "    \n",
    "    if is_terminal:\n",
    "        return reward\n",
    "    else:\n",
    "        return reward + gamma * V[next_state]\n",
    "\n",
    "def compute_q_value_stochastic(state, action, env, V, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Q-value for stochastic environment (expected value)\n",
    "    \"\"\"\n",
    "    if state >= env.observation_space.n - 1:  # Terminal state\n",
    "        return 0\n",
    "    \n",
    "    expected_q_value = 0\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    \n",
    "    for prob, next_state, reward, is_terminal in transitions:\n",
    "        if is_terminal:\n",
    "            contribution = reward\n",
    "        else:\n",
    "            contribution = reward + gamma * V[next_state]\n",
    "        \n",
    "        expected_q_value += prob * contribution\n",
    "    \n",
    "    return expected_q_value\n",
    "\n",
    "# Example: Computing all Q-values for a specific state\n",
    "def compute_all_q_values_for_state(state, env, V, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Q-values for all actions in a given state\n",
    "    \"\"\"\n",
    "    q_values = {}\n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    \n",
    "    for action in range(env.action_space.n):\n",
    "        q_val = compute_q_value_stochastic(state, action, env, V, gamma)\n",
    "        q_values[action] = q_val\n",
    "        print(f\"Q({state}, {action_names[action]}) = {q_val:.3f}\")\n",
    "    \n",
    "    return q_values\n",
    "\n",
    "# Example state values (from previous computation)\n",
    "example_V = {0: 1,\n",
    "            1: 8,\n",
    "            2: 9, \n",
    "            3: 2, \n",
    "            4: 7, \n",
    "            5: 10, \n",
    "            6: 3, \n",
    "            7: 5, \n",
    "            8: 0}\n",
    "\n",
    "# Compute Q-values for state 4\n",
    "print(\"Q-values for State 4:\")\n",
    "\n",
    "q_values_state_4 = compute_all_q_values_for_state(4, env, example_V, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ae655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete Q-Value Table:\n",
      "============================================================\n",
      "State  Left     Down     Right    Up      \n",
      "------------------------------------------------------------\n",
      "0      0.015    0.015    0.015    0.014   \n",
      "1      0.009    0.015    0.015    0.019   \n",
      "2      0.034    0.029    0.034    0.019   \n",
      "3      0.015    0.015    0.009    0.019   \n",
      "4      0.027    0.022    0.021    0.011   \n",
      "5      0.000    0.000    0.000    0.000   \n",
      "6      0.076    0.066    0.076    0.010   \n",
      "7      0.000    0.000    0.000    0.000   \n",
      "8      0.022    0.064    0.054    0.070   \n",
      "9      0.110    0.176    0.159    0.082   \n",
      "10     0.239    0.219    0.191    0.068   \n",
      "11     0.000    0.000    0.000    0.000   \n",
      "12     0.000    0.000    0.000    0.000   \n",
      "13     0.142    0.265    0.313    0.219   \n",
      "14     0.330    0.598    0.570    0.493   \n"
     ]
    }
   ],
   "source": [
    "def compute_all_q_values(policy, env, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Compute Q-values for all state-action pairs\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        for action in range(env.action_space.n):\n",
    "            if state >= env.observation_space.n - 1:  # Terminal state\n",
    "                Q[(state, action)] = None\n",
    "            else:\n",
    "                Q[(state, action)] = compute_q_value_stochastic(state, action, env, V, gamma)\n",
    "    return Q\n",
    "\n",
    "def display_q_table(Q, env):\n",
    "    \"\"\"\n",
    "    Display Q-values in a readable table format\n",
    "    \"\"\"\n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    \n",
    "    print(\"\\nComplete Q-Value Table:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'State':<6} {'Left':<8} {'Down':<8} {'Right':<8} {'Up':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal\n",
    "        row = [f\"{state:<6}\"]\n",
    "        for action in range(env.action_space.n):\n",
    "            q_val = Q[(state, action)]\n",
    "            if q_val is not None:\n",
    "                row.append(f\"{q_val:<8.3f}\")\n",
    "            else:\n",
    "                row.append(f\"{'N/A':<8}\")\n",
    "        print(\" \".join(row))\n",
    "\n",
    "# Compute complete Q-table\n",
    "complete_Q = compute_all_q_values(simple_policy, env, V_bellman, gamma=0.9)\n",
    "display_q_table(complete_Q, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4668d",
   "metadata": {},
   "source": [
    "### 3.11 **Quick Comparison: $V^\\pi$ Vs $Q^\\pi$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feacf228",
   "metadata": {},
   "source": [
    "| Function         | Input                 | Output         | Question it answers                                         | Intuition                                                                                              | Example                                                                                         |\n",
    "| ---------------- | --------------------- | -------------- | ----------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------- |\n",
    "| **$V^\\pi(s)$**   | State $s$             | Number (value) | “How good is this state under policy $\\pi$?”                | Expected long-term return starting **from state $s$** and following policy $\\pi$.                      | In chess: value of a board position assuming you continue playing according to strategy $\\pi$.  |\n",
    "| **$Q^\\pi(s,a)$** | State $s$, Action $a$ | Number (value) | “How good is this action in this state under policy $\\pi$?” | Expected long-term return starting **from state $s$**, taking action $a$, then following policy $\\pi$. | In chess: value of moving a knight to a specific square (action) given the current board state. |\n",
    "\n",
    "\n",
    "\n",
    "### 🔑 Key Differences\n",
    "\n",
    "1. **Scope**\n",
    "\n",
    "   * $V^\\pi(s)$ looks at **how desirable a state is overall**.\n",
    "   * $Q^\\pi(s,a)$ looks at **how desirable a specific action is in that state**.\n",
    "\n",
    "2. **Decision-making granularity**\n",
    "\n",
    "   * $V^\\pi(s)$ is coarse-grained (state-level).\n",
    "   * $Q^\\pi(s,a)$ is fine-grained (state-action level, more detailed).\n",
    "\n",
    "3. **Relation**\n",
    "\n",
    "   * $V^\\pi(s) = \\sum_a \\pi(a|s)\\, Q^\\pi(s,a)$\n",
    "     (state value is the expected action value under the policy).\n",
    "\n",
    "4. **Use cases**\n",
    "\n",
    "   * $V^\\pi(s)$: Used in **policy evaluation** (how good is my strategy overall?).\n",
    "   * $Q^\\pi(s,a)$: Used in **policy improvement** and **control** (which action should I pick?).\n",
    "\n",
    "\n",
    "\n",
    "### ⭐ Intuitive Summary\n",
    "\n",
    "* **$V$** = *“Value of being in a place (state).”*\n",
    "* **$Q$** = *“Quality of making a move (action) in that place.”*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
