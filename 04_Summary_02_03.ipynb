{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0b5593",
   "metadata": {},
   "source": [
    "# **Summary Notebook `02` and `03`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d658e",
   "metadata": {},
   "source": [
    "## **ðŸ“‘Table of Contents**\n",
    "\n",
    "1. [Part A: Environment & Setup](##Part-A:-Environment-&-Setup)\n",
    "2. [Part B: Policy Iteration](##Part-B:-Policy-Iteration)\n",
    "3. [Part C: Value Iteration](##Part-C:-Value-Iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9b640",
   "metadata": {},
   "source": [
    "## **Part A: Environment & Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f31f3",
   "metadata": {},
   "source": [
    "* Imports, FrozenLake setup\n",
    "* Initial policy definition (arbitrary)\n",
    "* Helper functions (`compute_state_value`, `compute_q_value`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc986b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 16\n",
      "Number of actions: 4\n",
      "[(1.0, 5, 0, True)]\n",
      "Initial Value Function: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0}\n",
      "Q-values: {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.0, (0, 3): 0.0, (1, 0): 0.0, (1, 1): 0.0, (1, 2): 0.0, (1, 3): 0.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0, (3, 0): 0.0, (3, 1): 0.0, (3, 2): 0.0, (3, 3): 0.0, (4, 0): 0.0, (4, 1): 0.0, (4, 2): 0.0, (4, 3): 0.0, (5, 0): 0.0, (5, 1): 0.0, (5, 2): 0.0, (5, 3): 0.0, (6, 0): 0.0, (6, 1): 0.0, (6, 2): 0.0, (6, 3): 0.0, (7, 0): 0.0, (7, 1): 0.0, (7, 2): 0.0, (7, 3): 0.0, (8, 0): 0.0, (8, 1): 0.0, (8, 2): 0.0, (8, 3): 0.0, (9, 0): 0.0, (9, 1): 0.0, (9, 2): 0.0, (9, 3): 0.0, (10, 0): 0.0, (10, 1): 0.0, (10, 2): 0.0, (10, 3): 0.0, (11, 0): 0.0, (11, 1): 0.0, (11, 2): 0.0, (11, 3): 0.0, (12, 0): 0.0, (12, 1): 0.0, (12, 2): 0.0, (12, 3): 0.0, (13, 0): 0.0, (13, 1): 0.0, (13, 2): 0.0, (13, 3): 0.0, (14, 0): 0.0, (14, 1): 0.0, (14, 2): 0.0, (14, 3): 1.0, (15, 0): None, (15, 1): None, (15, 2): None, (15, 3): None}\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================\n",
    "# PART A: ENVIRONMENT SETUP & INITIAL DEFINITIONS\n",
    "# ==================================================================================================\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create FrozenLake environment with slippery surface\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Print environment dimensions\n",
    "print(\"Number of states:\", env.observation_space.n)   # 16 states (4x4 grid)\n",
    "print(\"Number of actions:\", env.action_space.n)       # 4 actions (left, down, right, up)\n",
    "\n",
    "# Environment parameters\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "discount_factor = 0.9  # Gamma value for future rewards\n",
    "terminal_state = num_states - 1   # Goal state (state 15)\n",
    "\n",
    "# Define initial policy (state -> action mapping)\n",
    "policy = {\n",
    "    0:1, 1:2, 2:1, 3:1,\n",
    "    4:3, 5:1, 6:2, 7:3,\n",
    "    8:0, 9:1, 10:2, 11:3,\n",
    "    12:0, 13:1, 14:2, 15:3\n",
    "}\n",
    "\n",
    "# Run one episode using the policy\n",
    "state, info = env.reset()\n",
    "terminated = False\n",
    "\n",
    "while not terminated:\n",
    "    action = policy[state]  # Get action from policy\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    state = next_state\n",
    "\n",
    "# Check transition probabilities for current state-action pair\n",
    "print(env.unwrapped.P[state][action])  \n",
    "# Format: (probability, next_state, reward, is_terminal)\n",
    "\n",
    "# ======================================================================================\n",
    "# Helpers for state-value and Q-value\n",
    "# ======================================================================================\n",
    "value = {state: 0 for state in range(num_states)}  # Initialize value function\n",
    "\n",
    "def compute_state_value(state):\n",
    "    \"\"\"Calculate V(s) using Bellman equation\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    action = policy[state]  # Get policy action\n",
    "    # Simplified: only uses first transition outcome\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + discount_factor * value[next_state]\n",
    "\n",
    "def compute_q_value(state, action):\n",
    "    \"\"\"Calculate Q(s,a) using Bellman equation\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return None\n",
    "    # Simplified: only uses first transition outcome\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + discount_factor * value[next_state]\n",
    "\n",
    "# Compute value function for all states\n",
    "value = {state: compute_state_value(state) for state in range(num_states)}\n",
    "print(\"Initial Value Function:\", value)\n",
    "\n",
    "# Compute Q-values for all state-action pairs\n",
    "Q = {(state, action): compute_q_value(state, action) for state in range(num_states) for action in range(num_actions)}\n",
    "print(\"Q-values:\", Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249856d",
   "metadata": {},
   "source": [
    "## **Part B: Policy Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40e39a",
   "metadata": {},
   "source": [
    "- `policy_evaluation(policy)`\n",
    "- `policy_improvement(policy)`\n",
    "- `policy_iteration()`\n",
    "- Print final optimal policy + state values\n",
    "\n",
    "ðŸ“Œ Flow:\n",
    "**Initialize policy â†’ Evaluate policy âŸ· Improve policy â†’ Optimal policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab90926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy from Policy Iteration: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 3}\n",
      "Optimal State Values from Policy Iteration: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 1.0, 15: 0}\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================\n",
    "# PART B: POLICY ITERATION\n",
    "# ==================================================================================================\n",
    "def policy_evaluation(policy):\n",
    "    \"\"\"Evaluate policy by computing state values V(s)\"\"\"\n",
    "    v = {}\n",
    "    for state in range(num_states):\n",
    "        if state == terminal_state:\n",
    "            v[state] = 0  # Terminal state has zero value\n",
    "        else:\n",
    "            action = policy[state]  # Get action from policy\n",
    "            # Simplified: uses first transition outcome only\n",
    "            _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "            v[state] = reward + discount_factor * value[next_state]\n",
    "    return v \n",
    "\n",
    "def policy_improvement(policy):\n",
    "    \"\"\"Improve policy by selecting best action for each state\"\"\"\n",
    "    improved_policy = {}\n",
    "    for state in range(num_states - 1):  # Exclude terminal state\n",
    "        # Calculate Q-values for all actions in this state\n",
    "        q_values = [compute_q_value(state, action) for action in range(num_actions)]\n",
    "        # Select action with highest Q-value (greedy policy)\n",
    "        max_action = q_values.index(max(q_values))\n",
    "        improved_policy[state] = max_action\n",
    "    return improved_policy\n",
    "\n",
    "def policy_iteration():\n",
    "    \"\"\"Main policy iteration algorithm\"\"\"\n",
    "    # Initialize with arbitrary policy (all actions = 0)\n",
    "    policy = {s: 0 for s in range(num_states - 1)}\n",
    "\n",
    "    while True:\n",
    "        # Step 1: Policy Evaluation\n",
    "        V = policy_evaluation(policy)\n",
    "        # Step 2: Policy Improvement\n",
    "        improved_policy = policy_improvement(policy)\n",
    "\n",
    "        # Check for convergence (policy unchanged)\n",
    "        if improved_policy == policy:\n",
    "            break\n",
    "        policy = improved_policy\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Execute policy iteration algorithm\n",
    "policy_pi, V_pi = policy_iteration()\n",
    "print(\"Optimal Policy from Policy Iteration:\", policy_pi)\n",
    "print(\"Optimal State Values from Policy Iteration:\", V_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872dc7d1",
   "metadata": {},
   "source": [
    "## **Part C: Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef5b5a",
   "metadata": {},
   "source": [
    "- `compute_q_value_for_V()`\n",
    "- `get_max_action_and_values()`\n",
    "- Value Iteration loop\n",
    "- Print final optimal policy + values\n",
    "\n",
    "ðŸ“Œ Flow:\n",
    "**Initialize V â†’ Bellman updates (value iteration) â†’ Optimal V & Policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "835c275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy from Value Iteration: {0: 2, 1: 3, 2: 2, 3: 1, 4: 2, 5: 0, 6: 2, 7: 0, 8: 3, 9: 2, 10: 2, 11: 0, 12: 0, 13: 3, 14: 3}\n",
      "Optimal State Values from Value Iteration: {0: 0.5904900000000002, 1: 0.6561000000000001, 2: 0.7290000000000001, 3: 0.6561000000000001, 4: 0.6561000000000001, 5: 0.0, 6: 0.81, 7: 0.0, 8: 0.7290000000000001, 9: 0.81, 10: 0.9, 11: 0.0, 12: 0.0, 13: 0.9, 14: 1.0, 15: 0}\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================\n",
    "# PART C: VALUE ITERATION\n",
    "# ==================================================================================================\n",
    "def compute_q_value_for_V(state, action, V):\n",
    "    \"\"\"Calculate Q(s,a) given current value function V\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0  # Terminal state has zero Q-value\n",
    "    # Simplified: uses first transition outcome only\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + discount_factor * V[next_state]\n",
    "\n",
    "def get_max_action_and_values(state, V):\n",
    "    \"\"\"Find best action and its Q-value for given state\"\"\"\n",
    "    # Calculate Q-values for all actions\n",
    "    Q_values = [compute_q_value_for_V(state, action, V) for action in range(num_actions)]\n",
    "    # Find action with maximum Q-value\n",
    "    max_action = max(range(num_actions), key=lambda a: Q_values[a])\n",
    "    max_q_value = Q_values[max_action]\n",
    "    return max_action, max_q_value\n",
    "\n",
    "# Initialize value function and policy\n",
    "V = {state: 0 for state in range(num_states)}\n",
    "policy = {state: 0 for state in range(num_states - 1)}\n",
    "threshold = 0.001  # Convergence threshold\n",
    "\n",
    "# Value Iteration main loop\n",
    "while True:\n",
    "    new_V = {state: 0 for state in range(num_states)}  # New value function\n",
    "\n",
    "    # Update value for each non-terminal state\n",
    "    for state in range(num_states - 1):\n",
    "        max_action, max_q_value = get_max_action_and_values(state, V)\n",
    "        new_V[state] = max_q_value  # Bellman optimality equation\n",
    "        policy[state] = max_action  # Extract greedy policy\n",
    "\n",
    "    # Check for convergence (values stop changing significantly)\n",
    "    if all(abs(new_V[state] - V[state]) < threshold for state in V):\n",
    "        break\n",
    "\n",
    "    V = new_V  # Update value function\n",
    "\n",
    "print(\"Optimal Policy from Value Iteration:\", policy)\n",
    "print(\"Optimal State Values from Value Iteration:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f647c317",
   "metadata": {},
   "source": [
    "- Value Iteration provides an efficient alternative to Policy Iteration by combining evaluation and improvement steps.\n",
    "- While it requires more iterations, each iteration is computationally cheaper, making it particularly suitable for large-scale problems.\n",
    "- The algorithm is guaranteed to converge to the optimal policy, achieving the same result as Policy Iteration but through a different computational path.\n",
    "\n",
    "- The choice between `Value Iteration` and `Policy Iteration` depends on the specific problem characteristics: \n",
    "  - Use Value Iteration for large problems where computational efficiency per iteration is crucial, \n",
    "  - and Policy Iteration for smaller problems where exact solutions and fewer iterations are preferred."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
