{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d48b308",
   "metadata": {},
   "source": [
    "# **‚ú®Markov Decision Processes(MDPs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ecb7c",
   "metadata": {},
   "source": [
    "## **üìëTable of Contents**\n",
    "\n",
    "1. [Introduction to Markov Decision Processes (MDPs)](#1-introduction-to-markov-decision-processes-mdps)\n",
    "2. [Core Components of MDPs](#2-core-components-of-mdps)\n",
    "3. [The Markov Property](#3-the-markov-property)\n",
    "4. [Frozen Lake Environment as MDP](#4-frozen-lake-environment-as-mdp)\n",
    "5. [Gymnasium Implementation](#5-gymnasium-implementation)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88eecb",
   "metadata": {},
   "source": [
    "## **üîñ1. Introduction to Markov Decision Processes (MDPs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3046ac",
   "metadata": {},
   "source": [
    "### Core Concept:\n",
    "> \"**MDP: Models RL environments mathematically**\"\n",
    "\n",
    "### Mathematical Foundation of MDPs\n",
    "\n",
    ">**Definition:** A Markov Decision Process (MDP) is a mathematical framework used to model decision-making in environments where outcomes are partly random and partly under the control of a decision-maker (agent). It's foundational in reinforcement learning and dynamic programming.\n",
    "\n",
    "#### Why MDPs Matter\n",
    "- **Universal Framework**: MDPs serve as the theoretical foundation for virtually all reinforcement learning algorithms\n",
    "- **Mathematical Rigor**: They provide precise mathematical definitions for `states`, `actions`, and `rewards`\n",
    "- **Optimization Target**: MDPs enable the formulation of optimal policies through well-defined mathematical principles\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "An MDP is formally defined as a 5-tuple: $(S, A, P, R, \\gamma)$\n",
    "\n",
    "Where:\n",
    "- $S$ = Set of all possible `states`\n",
    "- $A$ = Set of all possible `actions`  \n",
    "- $P$ = `Transition probability function`\n",
    "- $R$ = `Reward` function\n",
    "- $\\gamma$ = `Discount factor` (0 ‚â§ Œ≥ ‚â§ 1)\n",
    "\n",
    "#### Complex Environment Modeling\n",
    "\n",
    "MDPs excel at modeling complex environments because they:\n",
    "\n",
    "**Capture Uncertainty**: Through probabilistic state transitions\n",
    "- Real environments rarely have deterministic outcomes\n",
    "- Actions may lead to unintended results due to noise, physics, or external factors\n",
    "\n",
    "**Handle Sequential Decisions**: Through multi-step planning horizons\n",
    "- Decisions affect not just immediate rewards but future opportunities\n",
    "- Long-term consequences must be balanced against short-term gains\n",
    "\n",
    "**Enable Optimization**: Through value-based solution methods\n",
    "- Mathematical optimization techniques can find provably optimal policies\n",
    "- Algorithms like dynamic programming guarantee convergence to optimal solutions\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182452be",
   "metadata": {},
   "source": [
    "## **üîñ2. Core Components of MDPs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503614a",
   "metadata": {},
   "source": [
    "- ### **Core Components:**\n",
    "  - ***States***\n",
    "  - ***Action***\n",
    "  - ***Rewards***\n",
    "  - ***Transition probabilities***\n",
    "\n",
    "- ### 2.1 **States (S)**\n",
    "    - **Definition**: States represent all the information necessary to make optimal decisions at any point in time.\n",
    "\n",
    "    - Technical Details:\n",
    "        - **State Space**: The complete set $S = \\{s_1, s_2, ..., s_n\\}$ of all possible states\n",
    "        - **Current State**: Denoted as $s_t$ at time step $t$\n",
    "        - **State Representation**: Must capture all relevant environmental information\n",
    "\n",
    "    - State Design Principles:\n",
    "        - **Completeness**: States must include all information relevant to decision-making\n",
    "        - Missing information leads to suboptimal policies\n",
    "        - Over-inclusion increases computational complexity unnecessarily\n",
    "\n",
    "    - **Markov Property Compliance**: \n",
    "        - Current state must fully determine future possibilities\n",
    "        - Past history should be irrelevant given current state\n",
    "        - This enables efficient dynamic programming solutions\n",
    "\n",
    "- ### **2.2 Actions (A)**\n",
    "\n",
    "    - **Definition**: Actions represent the set of choices available to the agent in each state.\n",
    "\n",
    "    -  #### Technical Details:\n",
    "        - **Action Space**: $A(s)$ represents actions available in state $s$\n",
    "        - **Action Selection**: Denoted as $a_t$ at time step $t$\n",
    "        - **Action Constraints**: Some actions may be state-dependent\n",
    "\n",
    "    - #### Action Space Types:\n",
    "        - **Discrete Actions**: Finite set of distinct choices\n",
    "            - Examples: {Move Left, Move Right, Move Up, Move Down}\n",
    "            - Easier to analyze mathematically\n",
    "            - Common in grid worlds and board games\n",
    "\n",
    "        - **Continuous Actions**: Infinite set within bounded ranges\n",
    "            - Examples: Steering angle, throttle position, joint torques\n",
    "            - Requires specialized algorithms (policy gradients, actor-critic)\n",
    "            - Common in robotics and control systems\n",
    "\n",
    "- ### **2.3 Rewards (R)**\n",
    "\n",
    "    - **Definition**: Rewards provide the optimization signal that guides the agent toward desired behaviors.\n",
    "\n",
    "    - #### Mathematical Formulation:\n",
    "        - **Reward Function**:  $R(s, a)$\n",
    "        - **Immediate Reward**: $r_t$ received at time step $t$\n",
    "        - **Cumulative Return**: $G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$\n",
    "\n",
    "          - Where:\n",
    "              - $r_t$ = Immediate reward at time $t$\n",
    "              - $\\gamma$ = Discount factor\n",
    "              - $G_t$ = Total discounted return from time $t$\n",
    "\n",
    "    - #### Reward Design Strategies:\n",
    "        - **Sparse Rewards**: Rewards only at goal achievement\n",
    "            - Pro: Clearly defined objectives\n",
    "            - Con: Difficult learning due to credit assignment problem\n",
    "\n",
    "        - **Dense Rewards**: Frequent intermediate rewards\n",
    "            - Pro: Faster learning through immediate feedback\n",
    "            - Con: Risk of reward hacking and unintended behaviors\n",
    "\n",
    "        - **Shaped Rewards**: Carefully designed intermediate rewards\n",
    "            - Guides agent toward goal while maintaining true objective\n",
    "            - Requires domain expertise to design effectively\n",
    "\n",
    "- ### **2.4 Transition Probabilities (P)**\n",
    "\n",
    "  - **Definition**: Transition probabilities define the stochastic dynamics of the environment.\n",
    "\n",
    "  - #### Mathematical Formulation:\n",
    "    - $P(s_{t+1} = s' | s_t = s, a_t = a) = P_{ss'}^a$\n",
    "      - Where:\n",
    "          - $P_{ss'}^a$ = Probability of transitioning from state $s$ to state $s'$ under action $a$\n",
    "          - $\\sum_{s'} P_{ss'}^a = 1$ for all $s, a$ (probability distribution property)\n",
    "\n",
    "  - #### Deterministic vs Stochastic Environments:\n",
    "    - **Deterministic**: $P_{ss'}^a \\in \\{0, 1\\}$\n",
    "      - Outcomes are completely predictable\n",
    "      - Simpler to analyze and solve\n",
    "      - Examples: Chess (ignoring time constraints), perfect mazes\n",
    "\n",
    "    - **Stochastic**: $0 < P_{ss'}^a < 1$ for multiple $s'$\n",
    "      - Outcomes involve uncertainty\n",
    "      - More realistic for real-world problems\n",
    "      - Requires handling of multiple possible outcomes\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b4697",
   "metadata": {},
   "source": [
    "## **üîñ3. The Markov Property**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425ddb9",
   "metadata": {},
   "source": [
    "> \"**Markov property: Future state depends only on current state and action**\"\n",
    "\n",
    "**Definition:** The Markov Property states that the future state depends only on the `current state` and `action`, **NOT** on the entire history of past states and actions.\n",
    "\n",
    "- **Mathematical Expression:**\n",
    "$$P(S_{t+1} = s' | S_t = s, A_t = a, S_{t-1}, A_{t-1}, ..., S_0, A_0) = P(S_{t+1} = s' | S_t = s, A_t = a)$$\n",
    "\n",
    "- **Intuitive Explanation:** The current state contains all the information needed to make optimal decisions about the future.\n",
    "\n",
    "- **Goal of an Agent in MDP**\n",
    "  - The agent‚Äôs objective is to find a policy `ùúã(ùëé‚à£ùë†)` that maximizes the expected cumulative reward over time. This is often formalized using:\n",
    "      - **Value functions**: Estimate how good it is to be in a state or take an action.\n",
    "      - **Policy iteration** and **value iteration**: Algorithms to compute optimal policies.\n",
    "\n",
    "- **Why the Markov Property Matters**\n",
    "    - **Computational Efficiency**: \n",
    "        - Eliminates need to track and process entire history\n",
    "        - Reduces memory requirements from exponential to linear\n",
    "        - Enables efficient dynamic programming algorithms\n",
    "\n",
    "    - **Mathematical Tractability**:\n",
    "        - Allows use of well-established optimization techniques\n",
    "        - Guarantees convergence properties for many algorithms\n",
    "        - Simplifies analysis of algorithm behavior\n",
    "\n",
    "    - **Policy Optimality**:\n",
    "        - Optimal policies can be expressed as functions of current state only: $\\pi^*(a|s)$\n",
    "        - No need to condition on history: $\\pi^*(a|s, h_t)$ where $h_t$ is history\n",
    "\n",
    "- **Violations and Solutions**\n",
    "    - **Common Violations**:\n",
    "        - **Partial Observability**: Agent cannot observe full state\n",
    "        - **Non-Markovian Dynamics**: Environment has memory or hidden variables\n",
    "        - **Insufficient State Representation**: Missing crucial information\n",
    "\n",
    "    - **Solutions**:\n",
    "        - **State Augmentation**: Include relevant history in state representation\n",
    "        - **Recurrent Policies**: Use RNNs or LSTMs to maintain internal memory\n",
    "        - **Belief States**: Maintain probability distributions over possible states\n",
    "\n",
    "- **Practical Example: Navigation Robot**\n",
    "    - **Markovian Representation**:\n",
    "        - State includes: position, orientation, velocity, sensor readings\n",
    "        - Future position depends only on current state and chosen action\n",
    "        - Past trajectory irrelevant given complete current state\n",
    "\n",
    "    - **Non-Markovian Representation**:\n",
    "        - State includes only: position\n",
    "        - Missing information: orientation, velocity, momentum\n",
    "        - Past movements become relevant for predicting future positions\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123573a7",
   "metadata": {},
   "source": [
    "## **üîñ4. Frozen Lake Environment as MDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed30f0",
   "metadata": {},
   "source": [
    "- Imagine a 4x4 grid that represents a frozen lake. There are three types of tiles:\n",
    "    - **`S`** : The starting point (safe).\n",
    "    - **`F`** : Frozen surface (safe, you can walk on it).\n",
    "    - **`H`** : A hole in the ice (dangerous, you fall in and the episode ends).\n",
    "    - **`G`** : The goal (where you receive a reward).\n",
    "\n",
    "- A simple layout might look like this:\n",
    "    ```\n",
    "    S  F  F  F\n",
    "    F  H  F  H\n",
    "    F  F  F  H\n",
    "    H  F  F  G\n",
    "    ```\n",
    "\n",
    "> You are an agent (a person trying to cross the lake). Your goal is to find a path from `S` to `G` without falling into a hole `H`.\n",
    "\n",
    "\n",
    "- **Mapping the Problem to a Markov Decision Process (MDP)**\n",
    "    - An MDP is defined by a 5-tuple `(S, A, P, R, Œ≥)`:\n",
    "        - **S**: Set of states\n",
    "        - **A**: Set of actions\n",
    "        - **P**: Transition probabilities `P(s' | s, a)`\n",
    "        - **R**: Reward function `R(s, a, s')`\n",
    "        - **Œ≥**: Discount factor (between 0 and 1)\n",
    "\n",
    "    - Let's break down the Frozen Lake problem into these components.\n",
    "        - üü¢`States (S)`\n",
    "            - Each tile (cell) in the grid is a state. We can represent them by their coordinates.\n",
    "               - **S**: `{(0,0), (0,1), (0,2), (0,3), (1,0), ..., (3,3)}`\n",
    "               - The **terminal states** are the holes `H` and the goal `G`. Once you enter one, the episode is over. For example, `(1,1)` is a hole and `(3,3)` is the goal.\n",
    "\n",
    "        - üü¢`Actions (A)`\n",
    "            - The actions are the possible moves the agent can take from any state (if the move is possible).\n",
    "            -   **A**: `{UP, DOWN, LEFT, RIGHT}`\n",
    "\n",
    "        - üü¢`Transition Probabilities (P)`\n",
    "            - This is the core of the \"Markov\" property. The outcome of an action is **stochastic** (non-deterministic). This mimics the slippery nature of ice.\n",
    "                - **Intended Action**: 33.3% chance\n",
    "                - **Slipping Left**: 33.3% chance\n",
    "                - **Slipping Right**: 33.3% chance\n",
    "\n",
    "            - **Example:** From state `(0,1)` (a frozen tile), if the agent intends to go `DOWN`:\n",
    "                - With ~33% probability, it successfully moves `DOWN` to `(1,1)`.\n",
    "                - With ~33% probability, it slips and moves `LEFT` to `(0,0)`.\n",
    "                - With ~33% probability, it slips and moves `RIGHT` to `(0,2)`.\n",
    "\n",
    "            - If a move would take the agent into a wall (e.g., moving `LEFT` from `(0,0)`), the agent simply stays in its current state. The probability mass for that invalid move is added to the probability of remaining in the current state.\n",
    "\n",
    "        - üü¢`Reward Function (R)`\n",
    "            - The reward defines the goal of the agent. We give a reward only when the agent reaches a meaningful state.\n",
    "                - **Reaching the Goal (G):** `R = +1`\n",
    "                - **Falling into a Hole (H):** `R = 0` (Some versions use a small negative reward like `-1` to penalize failure)\n",
    "                - **Stepping on any other Frozen (F) tile:** `R = 0`\n",
    "\n",
    "            - The agent gets this reward *upon entering* the new state `s'`.\n",
    "                - **Example:**\n",
    "                    - `R((2,3), DOWN, (3,3)) = +1` (Moving into the goal from above)\n",
    "                    - `R((1,0), RIGHT, (1,1)) = 0` (Moving into a hole)\n",
    "                    - `R((0,0), RIGHT, (0,1)) = 0` (Moving onto a frozen tile)\n",
    "\n",
    "        - üü¢`Discount Factor (Œ≥)`\n",
    "            - This determines how much the agent cares about `future rewards` vs. `immediate rewards`.\n",
    "            - Let's choose **Œ≥ = 0.9** for this example. This means the agent strongly prefers reaching the goal quickly but still values eventually reaching it over never reaching it at all.\n",
    "\n",
    "\n",
    "\n",
    "- **How an Episode Unrolls**\n",
    "  - Let's simulate a few steps of a possible episode:\n",
    "    - 1.  **Time t=0**: State `s‚ÇÄ = (0,0)` (Start).\n",
    "    - 2.  **Action a‚ÇÄ**: The agent chooses `RIGHT` (intending to go to `(0,1)`).\n",
    "    - 3.  **Transition**: Due to slippiness, it actually slips and moves `DOWN` to `s‚ÇÅ = (1,0)`. Reward `r‚ÇÄ = 0`.\n",
    "    - 4.  **Time t=1**: State `s‚ÇÅ = (1,0)`.\n",
    "    - 5.  **Action a‚ÇÅ**: The agent chooses `RIGHT` again (intending to go to `(1,1)` - a hole!).\n",
    "    - 6.  **Transition**: It successfully executes the action and moves into the hole at `s‚ÇÇ = (1,1)`. This is a terminal state.\n",
    "    - 7.  **Reward r‚ÇÅ**: The agent receives `r‚ÇÅ = 0` for entering a hole. The episode ends. The total return for this episode is `0`.\n",
    "\n",
    "> **A successful episode** would involve the agent navigating the slippery ice, potentially getting lucky with slips, and eventually landing on `(3,3)` to collect a reward of `+1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a4bde6",
   "metadata": {},
   "source": [
    "### The Solution to the MDP\n",
    "\n",
    "- The goal of solving an MDP is to find a **policy (œÄ)**, which is a strategy that tells the agent what action to take in every state (`œÄ(s) -> a`).\n",
    "- The optimal policy `œÄ*` is the one that maximizes the expected cumulative discounted reward (the **return**). \n",
    "- In this case, it's the policy that has the highest chance of getting the agent to the goal without falling in a hole.\n",
    "- For a small grid like this, we can compute this optimal policy using algorithms like **Value Iteration** or **Policy Iteration**. \n",
    "- The result would be a map showing the best action for every tile:\n",
    "\n",
    "![frozen-lake-png](_img\\frozen-lake.png)\n",
    "\n",
    "### **Frozen Lake**\n",
    "- **Environment Description:** An agent must navigate across a frozen lake to reach a goal while avoiding holes.\n",
    "- **Components:**\n",
    "  - **States:** 16 positions (4√ó4 grid) numbered 0-15\n",
    "  - **Actions:** 4 possible moves (0: left, 1: down, 2: right, 3: up)\n",
    "  - **Terminal States:** Goal state (rewards +1) and hole states (episode ends) ~ 6\n",
    "  - **Transition Probabilities:** Actions don't always lead to expected outcomes due to slippery ice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0234d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n",
      "Number of actions: 4\n",
      "Number of states: 16\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "# Check state and action spaces\n",
    "print(env.action_space)          \n",
    "print(env.observation_space)    \n",
    "\n",
    "print(\"Number of actions:\", env.action_space.n)      \n",
    "print(\"Number of states:\", env.observation_space.n)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff35a22",
   "metadata": {},
   "source": [
    "## **üîñ5. CliffWalking Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192c299",
   "metadata": {},
   "source": [
    "- The Cliff Walking environment involves an agent crossing a grid world from start to goal while avoiding falling off a cliff.\n",
    "- If the player moves to a cliff location it returns to the start location.\n",
    "- The player makes moves until they reach the goal, which ends the episode.\n",
    "- Your task is to explore the state and action spaces of this environment.\n",
    "\n",
    "![cliff-walking-gif](_img\\cliff_walking.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d23f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "Number of states: 48\n",
      "[(1.0, np.int64(23), -1, False)]\n",
      "Action: 0 | Probability: 1.0, Next State: 23, Reward: -1, Done: False\n",
      "[(1.0, np.int64(35), -1, False)]\n",
      "Action: 1 | Probability: 1.0, Next State: 35, Reward: -1, Done: False\n",
      "[(1.0, np.int64(47), -1, True)]\n",
      "Action: 2 | Probability: 1.0, Next State: 47, Reward: -1, Done: True\n",
      "[(1.0, np.int64(34), -1, False)]\n",
      "Action: 3 | Probability: 1.0, Next State: 34, Reward: -1, Done: False\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym   \n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Environment Setup\n",
    "# ==============================\n",
    "# Create the CliffWalking environment.\n",
    "# \"CliffWalking-v1\" is a classic control problem from reinforcement learning.\n",
    "# \"render_mode='rgb_array'\" means the environment won't open a window;\n",
    "# instead, it keeps the visual output as an image array (useful for debugging or rendering later).\n",
    "env = gym.make('CliffWalking-v1', render_mode='rgb_array')\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Action and State Spaces\n",
    "# ==============================\n",
    "# Number of possible actions the agent can take (Up, Right, Down, Left = 4).\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Number of possible states in the gridworld (4 rows √ó 12 columns = 48).\n",
    "num_states = env.observation_space.n\n",
    "\n",
    "print(\"Number of actions:\", num_actions)\n",
    "print(\"Number of states:\", num_states)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Exploring Transitions\n",
    "# ==============================\n",
    "# Each state has a set of transitions, depending on the chosen action.\n",
    "# Let's pick a specific state (for example, 35) and explore what happens when we try different actions.\n",
    "state = 35\n",
    "\n",
    "# Loop through all possible actions from this state\n",
    "for action in range(num_actions):\n",
    "    # The environment has an internal dictionary \"P\" that stores transitions.\n",
    "    # P[state][action] gives a list of possible outcomes when taking `action` in `state`.\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    print(transitions)\n",
    "\n",
    "    # Each transition has the format: (probability, next_state, reward, done)\n",
    "    # -> probability: chance of this outcome (usually 1.0 for deterministic envs like CliffWalking)\n",
    "    # -> next_state: the state you land in after the action\n",
    "    # -> reward: the reward received for this action\n",
    "    # -> done: whether the episode ends after this transition\n",
    "    for transition in transitions:\n",
    "        probability, next_state, reward, done = transition\n",
    "        print(f\"Action: {action} | Probability: {probability}, Next State: {next_state}, Reward: {reward}, Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c2915",
   "metadata": {},
   "source": [
    "### ‚ú®6.1 Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414ed37",
   "metadata": {},
   "source": [
    "> Policies: A `policy` $\\pi$  is a strategy that defines how an `agent` selects `actions` based on its `current state` to maximize expected `rewards`.\n",
    "\n",
    "#### Mathematical Foundation\n",
    "- **Definition**: Policy $\\pi$ are the core decision-making mechanism that maps environmental states to specific actions.\n",
    "\n",
    "#### Policy Types:\n",
    "- **Deterministic Policy**: $\\pi(s) = a$\n",
    "    - Maps each state to exactly one action\n",
    "    - Simpler to analyze and implement\n",
    "    - Sufficient for optimal policies in MDPs\n",
    "\n",
    "  - **Stochastic Policy**: $\\pi(a|s) = P(A_t = a | S_t = s)$\n",
    "    - Probability distribution over actions for each state\n",
    "    - More flexible for exploration\n",
    "    - Required for some advanced algorithms\n",
    "\n",
    "#### Mathematical Properties:\n",
    "- For stochastic policies: $\\sum_a \\pi(a|s) = 1$ for all states $s$\n",
    "    - Where:\n",
    "        - $\\pi(a|s)$ = Probability of selecting action $a$ in state $s$\n",
    "        - Sum over all actions must equal 1 (valid probability distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79d525",
   "metadata": {},
   "source": [
    "### 6.2 Grid World Policy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce12cc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CUSTOM 3x3 GRIDWORLD WITH POLICY AND STATE-VALUES ===\n",
      "\n",
      "Custom 3x3 GridWorld Layout:\n",
      "S    \n",
      "  M M\n",
      "    D\n",
      "\n",
      "State-values: {0: 1, 1: 8, 2: 9, 3: 2, 4: 7, 5: 10, 6: 3, 7: 5, 8: 0}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CUSTOM 3x3 GRIDWORLD WITH POLICY AND STATE-VALUES ===\")\n",
    "print()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Custom 3x3 GridWorld Env\n",
    "# -------------------------------\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"ansi\"]}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shape = (3, 3)\n",
    "        self.observation_space = spaces.Discrete(9)\n",
    "        self.action_space = spaces.Discrete(4)  # 0:left,1:down,2:right,3:up\n",
    "        \n",
    "        # Define rewards\n",
    "        self.terminal_state = 8\n",
    "        self.rewards = {8: 10, 4: -2, 7: -2}\n",
    "        \n",
    "        # Precompute P like Gym\n",
    "        self.P = {s: {a: [] for a in range(4)} for s in range(9)}\n",
    "        for s in range(9):\n",
    "            for a in range(4):\n",
    "                ns = self._move(s, a)\n",
    "                r = self.rewards.get(ns, -1)\n",
    "                done = ns == self.terminal_state\n",
    "                self.P[s][a] = [(1.0, ns, r, done)]  # deterministic\n",
    "                \n",
    "    def _move(self, state, action):\n",
    "        if state == self.terminal_state:\n",
    "            return state\n",
    "        row, col = state // 3, state % 3\n",
    "        if action == 0:    # left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 1:  # down\n",
    "            row = min(2, row + 1)\n",
    "        elif action == 2:  # right\n",
    "            col = min(2, col + 1)\n",
    "        elif action == 3:  # up\n",
    "            row = max(0, row - 1)\n",
    "        return row * 3 + col\n",
    "    \n",
    "    def render(self, mode=\"ansi\"):\n",
    "        grid = np.full(self.shape, \" \")\n",
    "        grid[0,0] = \"S\"  # start\n",
    "        grid[2,2] = \"D\"  # diamond\n",
    "        grid[1,1] = grid[1,2] = \"M\"  # mountains\n",
    "        return \"\\n\".join([\" \".join(row) for row in grid])\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnv()\n",
    "num_states = env.observation_space.n\n",
    "gamma = 1\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define a deterministic policy\n",
    "# -------------------------------\n",
    "policy = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3,  # up\n",
    "    8: 0   # terminal\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Compute state values\n",
    "# -------------------------------\n",
    "def compute_state_value(state):\n",
    "    if state == env.terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "\n",
    "V = {s: compute_state_value(s) for s in range(num_states)}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Display results\n",
    "# -------------------------------\n",
    "print(\"Custom 3x3 GridWorld Layout:\")\n",
    "print(env.render())\n",
    "print()\n",
    "print(\"State-values:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc56687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy 1: {0: 'down', 1: 'right', 2: 'down', 3: 'down', 4: 'up', 5: 'down', 6: 'right', 7: 'up'}\n",
      "Policy 2: {0: 'right', 1: 'right', 2: 'down', 3: 'right', 4: 'right', 5: 'down', 6: 'right', 7: 'right'}\n",
      "\n",
      "2. STATE-VALUE FUNCTIONS\n",
      "========================\n",
      "V(s) = Expected return starting from state s following policy œÄ\n",
      "\n",
      "Computing state values...\n",
      "\n",
      "RESULTS:\n",
      "========\n",
      "State-values for Policy 1: {0: 1.0, 1: 8.0, 2: 9.0, 3: 2.0, 4: 7.0, 5: 10.0, 6: 3.0, 7: 5.0, 8: 0}\n",
      "State-values for Policy 2: {0: 7.0, 1: 8.0, 2: 9.0, 3: 7.0, 4: 9.0, 5: 10.0, 6: 8.0, 7: 10.0, 8: 0}\n",
      "\n",
      "EXAMPLE CALCULATION (Policy 1, State 2):\n",
      "========================================\n",
      "State 2 ‚Üí Action down ‚Üí State 5\n",
      "Reward: -1\n",
      "V(2) = -1 + 1.0 √ó V(5) = -1 + 1.0 √ó 10.0 = 9.0\n",
      "\n",
      "POLICY COMPARISON:\n",
      "==================\n",
      "Total value (Policy 1): 45.0\n",
      "Total value (Policy 2): 68.0\n",
      "Better policy: Policy 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# 1. POLICIES\n",
    "# ==========================\n",
    "\n",
    "# Actions: 0: left, 1: down, 2: right, 3: up\n",
    "policy1 = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3   # up\n",
    "}\n",
    "\n",
    "policy2 = {\n",
    "    0: 2,  # right\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 2,  # right\n",
    "    4: 2,  # right\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 2   # right\n",
    "}\n",
    "\n",
    "action_names = {0: 'left', 1: 'down', 2: 'right', 3: 'up'}\n",
    "\n",
    "print(\"Policy 1:\", {s: action_names[a] for s, a in policy1.items()})\n",
    "print(\"Policy 2:\", {s: action_names[a] for s, a in policy2.items()})\n",
    "print()\n",
    "\n",
    "# ==========================\n",
    "# 2. ENVIRONMENT MODEL (P)\n",
    "# ==========================\n",
    "gamma = 1.0          # Discount factor\n",
    "num_states = 9       # 3x3 grid\n",
    "terminal_state = 8   # Diamond\n",
    "\n",
    "# Build transition table: P[state][action] = [(prob, next_state, reward, done)]\n",
    "P = {s: {a: [] for a in range(4)} for s in range(num_states)}\n",
    "\n",
    "def move(state, action):\n",
    "    \"\"\"Return next state after taking an action.\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return state\n",
    "    \n",
    "    row, col = state // 3, state % 3\n",
    "    if action == 0:    # left\n",
    "        col = max(0, col - 1)\n",
    "    elif action == 1:  # down\n",
    "        row = min(2, row + 1)\n",
    "    elif action == 2:  # right\n",
    "        col = min(2, col + 1)\n",
    "    elif action == 3:  # up\n",
    "        row = max(0, row - 1)\n",
    "    return row * 3 + col\n",
    "\n",
    "def reward(next_state):\n",
    "    \"\"\"Return reward for landing in next_state.\"\"\"\n",
    "    if next_state == 8:   # Diamond\n",
    "        return 10\n",
    "    elif next_state in [4, 7]:  # Mountains\n",
    "        return -2\n",
    "    else:  # All other states\n",
    "        return -1\n",
    "\n",
    "# Fill transition table\n",
    "for s in range(num_states):\n",
    "    for a in range(4):\n",
    "        ns = move(s, a)\n",
    "        r = reward(ns)\n",
    "        done = (ns == terminal_state)\n",
    "        P[s][a] = [(1.0, ns, r, done)]   # deterministic env\n",
    "\n",
    "# ==========================\n",
    "# 3. STATE-VALUE FUNCTIONS\n",
    "# ==========================\n",
    "print(\"2. STATE-VALUE FUNCTIONS\")\n",
    "print(\"========================\")\n",
    "print(\"V(s) = Expected return starting from state s following policy œÄ\")\n",
    "print()\n",
    "\n",
    "def compute_state_value(state, policy):\n",
    "    \"\"\"Bellman expectation with env.P\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    transitions = P[state][action]\n",
    "    \n",
    "    value = 0\n",
    "    for prob, next_state, reward, _ in transitions:\n",
    "        value += prob * (reward + gamma * compute_state_value(next_state, policy))\n",
    "    return value\n",
    "\n",
    "# Calculate state values for both policies\n",
    "print(\"Computing state values...\")\n",
    "print()\n",
    "\n",
    "V1 = {s: compute_state_value(s, policy1) for s in range(num_states)}\n",
    "V2 = {s: compute_state_value(s, policy2) for s in range(num_states)}\n",
    "\n",
    "# ==========================\n",
    "# 4. RESULTS\n",
    "# ==========================\n",
    "print(\"RESULTS:\")\n",
    "print(\"========\")\n",
    "print(\"State-values for Policy 1:\", V1)\n",
    "print(\"State-values for Policy 2:\", V2)\n",
    "print()\n",
    "\n",
    "# Example calculation walkthrough\n",
    "print(\"EXAMPLE CALCULATION (Policy 1, State 2):\")\n",
    "print(\"========================================\")\n",
    "state = 2\n",
    "action = policy1[state]\n",
    "prob, next_state, reward, _ = P[state][action][0]\n",
    "print(f\"State 2 ‚Üí Action {action_names[action]} ‚Üí State {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"V(2) = {reward} + {gamma} √ó V({next_state}) = {reward} + {gamma} √ó {V1[next_state]} = {V1[2]}\")\n",
    "print()\n",
    "\n",
    "# Compare policies\n",
    "print(\"POLICY COMPARISON:\")\n",
    "print(\"==================\")\n",
    "total1 = sum(V1[s] for s in range(8))  # exclude terminal\n",
    "total2 = sum(V2[s] for s in range(8))\n",
    "print(f\"Total value (Policy 1): {total1}\")\n",
    "print(f\"Total value (Policy 2): {total2}\")\n",
    "print(f\"Better policy: Policy {'2' if total2 > total1 else '1'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc3b20",
   "metadata": {},
   "source": [
    "### ‚ú®6.3 State-Value Functions $V^\\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e56553",
   "metadata": {},
   "source": [
    "**Definition:**\n",
    "- The **state-value function** $V^{\\pi}(s)$ estimates how good it is to be in a given state $s$.\n",
    "- The state-value function $V^{\\pi}(s)$ represents the expected cumulative discounted reward starting from state $s$ and following policy $\\pi$.\n",
    "\n",
    "**Formal Definition**:\n",
    "- $V^{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]$\n",
    "- Where:\n",
    "    - $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ is the discounted return\n",
    "    - $\\mathbb{E}_{\\pi}$ denotes expectation under policy $\\pi$\n",
    "    - $\\gamma$ is the discount factor (0 ‚â§ Œ≥ ‚â§ 1)\n",
    "  \n",
    "- Expanded Mathematical Form:\n",
    "    - $V(s) = r_{s+1} + \\gamma r_{s+2} + \\gamma^2 r_{s+3} + \\cdots + \\gamma^{n-1} r_{s+n}$\n",
    "    - $V^{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s]$\n",
    "\n",
    "- **Interpretation:**\n",
    "    - $r_{s+1}$: Immediate reward after leaving state $s$\n",
    "    - $\\gamma r_{s+2}$: Next reward, discounted by factor $\\gamma$\n",
    "    - $\\gamma^2 r_{s+3}$: Reward two steps later, discounted further\n",
    "    - $\\cdots$ Continues infinitely\n",
    "    - $\\gamma \\in [0,1]$: Discount factor that balances **present vs. future rewards**\n",
    "\n",
    "- **When to use:**\n",
    "    - **Conceptual / Theoretical explanation** of value functions.\n",
    "    - When introducing RL to beginners ‚Üí easy to show ‚Äúwhy future rewards are discounted.‚Äù\n",
    "    - To **manually calculate returns** in very short episodes (e.g., toy problems like a 3-step grid world).\n",
    "    - Useful in **Monte Carlo methods**, where we sample entire episodes and directly compute the return.\n",
    "    - Not practical for real-world problems because we can‚Äôt compute `inf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92313d55",
   "metadata": {},
   "source": [
    "#### Grid World State-Values Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa44bd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values:\n",
      "V(0) = 0\n",
      "V(1) = 7\n",
      "V(2) = 8\n",
      "V(3) = 1\n",
      "V(4) = 5\n",
      "V(5) = 9\n",
      "V(6) = 2\n",
      "V(7) = 3\n",
      "V(8) = 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Computes how valuable each state is under a fixed action policy in a 3x3 grid world.\n",
    "- Simulates following the policy from each state to accumulate rewards until reaching a terminal state or revisiting a state.\n",
    "- Uses a simple deterministic transition function mapping an action and current position to a next position.\n",
    "- Assigns rewards: -2 for mountain states, +10 for the terminal diamond state, and -1 for other states.\n",
    "- Stops simulation to avoid infinite loops or once the terminal state is reached, adding its reward.\n",
    "- Prints the total expected reward (state value) starting from each state when following the policy.\n",
    "'''\n",
    "\n",
    "def compute_state_value(state, policy, gamma=1.0):\n",
    "    \"\"\"Compute state value for deterministic policy\"\"\"\n",
    "    if state == 8:  # Terminal state (diamond)\n",
    "        return 0\n",
    "    \n",
    "    # Simulate following policy from this state\n",
    "    current_state = state\n",
    "    total_reward = 0\n",
    "    visited = set()\n",
    "    \n",
    "    while current_state not in visited and current_state != 8:\n",
    "        visited.add(current_state)\n",
    "        action = policy[current_state]\n",
    "        \n",
    "        # Get reward for current state (simplified grid world)\n",
    "        if current_state in [4, 7]:  # Mountain states\n",
    "            reward = -2\n",
    "        elif current_state == 8:     # Diamond state  \n",
    "            reward = 10\n",
    "        else:                       # Other states\n",
    "            reward = -1\n",
    "            \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Move to next state based on action (simplified transitions)\n",
    "        next_state = get_next_state(current_state, action)\n",
    "        current_state = next_state\n",
    "    \n",
    "    # Add terminal reward if reached\n",
    "    if current_state == 8:\n",
    "        total_reward += 10\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    \"\"\"Simple deterministic transition function for 3x3 grid\"\"\"\n",
    "    # Convert state to row, col\n",
    "    row, col = state // 3, state % 3\n",
    "    \n",
    "    if action == 0:    # left\n",
    "        col = max(0, col - 1)\n",
    "    elif action == 1:  # down  \n",
    "        row = min(2, row + 1)\n",
    "    elif action == 2:  # right\n",
    "        col = min(2, col + 1)\n",
    "    elif action == 3:  # up\n",
    "        row = max(0, row - 1)\n",
    "    \n",
    "    return row * 3 + col\n",
    "\n",
    "\n",
    "# Define policy including terminal state with a dummy action (e.g. 0)\n",
    "grid_policy = {0:1, 1:2, 2:1, 3:1, 4:3, 5:1, 6:2, 7:3, 8:0}\n",
    "state_values = {}\n",
    "\n",
    "for state in range(9):\n",
    "    state_values[state] = compute_state_value(state, grid_policy)\n",
    "\n",
    "print(\"State Values:\")\n",
    "\n",
    "for state, value in state_values.items():\n",
    "    print(f\"V({state}) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902cdf3",
   "metadata": {},
   "source": [
    "- **State-Value Interpretation:**\n",
    "  \n",
    "    - **High Values**: States that lead to goal with minimal cost\n",
    "        - Closer to diamond with fewer mountain encounters\n",
    "        - Optimal pathways through environment\n",
    "\n",
    "    - **Low Values**: States requiring longer paths or mountain traversal\n",
    "        - Farther from goal or poor intermediate positions\n",
    "        - Suboptimal or risky pathways\n",
    "\n",
    "    - **Zero Value**: Terminal goal state\n",
    "      - No further rewards possible from terminal state\n",
    "      - Standard convention in episodic tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052f6d3",
   "metadata": {},
   "source": [
    "### 6.4 Computing State-Values with Recursion\n",
    "\n",
    "#### Original Content from PDF:\n",
    "```python\n",
    "def compute_state_value(state):  \n",
    "    if state == terminal_state: \n",
    "        return 0 \n",
    " \n",
    "    action = policy[state]  \n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]  \n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4f372f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values for FrozenLake:\n",
      "V(0)=-3.00 | V(1)=8.00 | V(2)=9.00 | V(3)=-2.00\n",
      "V(4)=-4.00 | V(5)=10.00 | V(6)=-1.00 | V(7)=-2.00\n",
      "V(8)=10.00 | V(9)=0.00 | V(10)=0.00 | V(11)=0.00\n",
      "V(12)=0.00 | V(13)=0.00 | V(14)=0.00 | V(15)=0.00\n"
     ]
    }
   ],
   "source": [
    "def compute_state_value_recursive(state, policy, env, gamma=1.0, memo=None, visited=None, max_depth=100):\n",
    "    \"\"\"\n",
    "    Compute state value using recursive Bellman equation\n",
    "    with memoization to handle cycles and recursion depth limit\n",
    "    \"\"\"\n",
    "    if memo is None:\n",
    "        memo = {}\n",
    "        \n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    # Base case: terminal state or max recursion depth reached\n",
    "    if state == 15 or max_depth <= 0:  # terminal state or max depth\n",
    "        return 0\n",
    "\n",
    "    # Avoid cycles/infinite loops\n",
    "    if state in visited:\n",
    "        # Return 0 or memoized value to break cycle\n",
    "        return memo.get(state, 0)\n",
    "\n",
    "    # Check memo to avoid recomputation\n",
    "    if state in memo:\n",
    "        return memo[state]\n",
    "\n",
    "    visited.add(state)\n",
    "\n",
    "    # Get action from policy\n",
    "    action = policy.get(state, 0)  # default action can be 0 if not in policy\n",
    "\n",
    "    # For stochastic environments, compute expected value\n",
    "    expected_value = 0\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "\n",
    "    for prob, next_state, reward, is_terminal in transitions:\n",
    "        if is_terminal:\n",
    "            value_contribution = reward\n",
    "        else:\n",
    "            value_contribution = reward + gamma * compute_state_value_recursive(\n",
    "                next_state, policy, env, gamma, memo, visited, max_depth - 1)\n",
    "        expected_value += prob * value_contribution\n",
    "\n",
    "    visited.remove(state)\n",
    "    memo[state] = expected_value\n",
    "    return expected_value\n",
    "\n",
    "\n",
    "# Example usage (assumes 'env' is the FrozenLake environment and 'frozen_lake_policy' is defined)\n",
    "\n",
    "gamma = 1.0\n",
    "\n",
    "frozen_lake_policy = {\n",
    "    0: 1, 1: 2, 2: 1, 3: 1,    # First row\n",
    "    4: 1, 5: 1, 6: 1, 7: 1,    # Second row  \n",
    "    8: 2, 9: 1, 10: 1, 11: 1,  # Third row\n",
    "    12: 2, 13: 2, 14: 2        # Fourth row (excluding terminal)\n",
    "}\n",
    "\n",
    "V = {}\n",
    "for state in range(env.observation_space.n):\n",
    "    V[state] = compute_state_value_recursive(state, frozen_lake_policy, env, gamma)\n",
    "\n",
    "print(\"State Values for FrozenLake:\")\n",
    "for i in range(4):\n",
    "    row = [f\"V({j})={V.get(j, 0):.2f}\" for j in range(i*4, (i+1)*4)]\n",
    "    print(\" | \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394207e",
   "metadata": {},
   "source": [
    "- **Code Explanation:**\n",
    "    - **Memoization**: Prevents infinite recursion and improves efficiency\n",
    "    - **Stochastic Handling**: Computes expected value over all possible transitions\n",
    "    - **Terminal Handling**: Properly handles terminal states with zero continuation value\n",
    "    - **Expected Value**: Weighted sum of outcomes by their probabilities\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc38f8",
   "metadata": {},
   "source": [
    "#### **Bellman Equation (Recursive Form)**\n",
    "\n",
    "- **Primary Concept:**\n",
    "  - $V(s) = r_{s+1} + \\gamma V(s+1)$\n",
    "    - Where:\n",
    "      - $r_{s+1}$ ‚Üí Reward immediately after leaving state $s$\n",
    "      - $\\gamma V(s+1)$ ‚Üí Discounted value of the next state, turning an infinite sum into a **recursive relationship**\n",
    "\n",
    "- **State-Value Function Under Policy $\\pi$:**\n",
    "  - $V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P_{ss'}^a \\left[ R_{ss'}^a + \\gamma V^{\\pi}(s') \\right]$\n",
    "  - For deterministic policies:\n",
    "    - $V^{\\pi}(s) = \\sum_{s'} P_{ss'}^{\\pi(s)} \\left[ R_{ss'}^{\\pi(s)} + \\gamma V^{\\pi}(s') \\right]$\n",
    "    - Where:\n",
    "      - $\\pi(s) $ = Action selected by deterministic policy in state $s$\n",
    "      - $P_{ss'}^{\\pi(s)}$ = Transition probability under policy action\n",
    "      - $R_{ss'}^{\\pi(s)}$ = Expected reward for the transition\n",
    "      - $\\gamma$ = Discount factor\n",
    "\n",
    "\n",
    "\n",
    "### 7.1 Mathematical Foundation of the Bellman Equation\n",
    "\n",
    "- **Primary Concept:**\n",
    "  - The **Bellman Equation** is the cornerstone of dynamic programming in reinforcement learning, providing a recursive relationship between the value of a state and the values of its successor states.\n",
    "\n",
    "- **Complete Mathematical Formulation:**\n",
    "\n",
    "  - **For State-Value Functions:**\n",
    "    - $V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V^{\\pi}(s') \\right]$\n",
    "\n",
    "  - **For Deterministic Policies (simplified):**\n",
    "    - $V^{\\pi}(s) = \\sum_{s'} P(s'|s,\\pi(s)) \\left[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s') \\right]$\n",
    "\n",
    "  - **For Deterministic Environments (further simplified):**\n",
    "    - $V^{\\pi}(s) = R(s,\\pi(s)) + \\gamma V^{\\pi}(s')$\n",
    "\n",
    "- **Where:**\n",
    "  - $V^{\\pi}(s)$ = Value of state $s$ under policy $\\pi$\n",
    "  - $R(s,a,s')$ = Immediate reward for the transition from $s$ to $s'$ using action $$a$\n",
    "  - $\\gamma$ = Discount factor (0 ‚â§ $\\gamma$ ‚â§ 1)\n",
    "  - $P(s'|s,a)$ = Transition probability of moving to state $s'$ from $s$ by action $a$\n",
    "  - $s'$ = Next state after taking action $a $ in state $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5c19c",
   "metadata": {},
   "source": [
    "### 7.2 Intuitive Understanding\n",
    "\n",
    "#### Why the Bellman Equation Works:\n",
    "\n",
    "**Recursive Structure**: \n",
    "- The value of being in a state equals the immediate reward plus the (discounted) value of where you end up\n",
    "- This creates a system of equations that can be solved for optimal values\n",
    "\n",
    "**Optimality Principle**:\n",
    "- Optimal solutions have the property that remaining decisions are optimal for the subproblem starting from the current state\n",
    "- This enables breaking complex problems into simpler subproblems\n",
    "\n",
    "**Dynamic Programming Foundation**:\n",
    "- Enables bottom-up solution construction\n",
    "- Avoids recomputing overlapping subproblems\n",
    "- Guarantees convergence to optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3fdfb",
   "metadata": {},
   "source": [
    "### 7.3 Bellman Equation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bb78760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 30 iterations\n",
      "State Values computed using Bellman Equation:\n",
      " 0.015 |  0.015 |  0.034 |  0.015\n",
      " 0.021 |  0.000 |  0.066 |  0.000\n",
      " 0.054 |  0.159 |  0.219 |  0.000\n",
      " 0.000 |  0.313 |  0.570 |  0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bellman_equation_single_state(state, policy, env, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Apply Bellman equation to compute value of a single state\n",
    "    \"\"\"\n",
    "    if state >= env.observation_space.n - 1:  # Terminal state\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    expected_value = 0\n",
    "    \n",
    "    # Get all possible transitions for this state-action pair\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    \n",
    "    for prob, next_state, reward, is_terminal in transitions:\n",
    "        if is_terminal:\n",
    "            # Terminal transition: only immediate reward\n",
    "            value_contribution = reward\n",
    "        else:\n",
    "            # Non-terminal: immediate reward + discounted future value\n",
    "            value_contribution = reward + gamma * V[next_state]\n",
    "        \n",
    "        expected_value += prob * value_contribution\n",
    "    \n",
    "    return expected_value\n",
    "\n",
    "def bellman_update_all_states(policy, env, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Apply Bellman equation to update all state values\n",
    "    \"\"\"\n",
    "    new_V = V.copy()\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        if state < len(policy):  # Non-terminal state\n",
    "            new_V[state] = bellman_equation_single_state(state, policy, env, V, gamma)\n",
    "        else:\n",
    "            new_V[state] = 0  # Terminal state\n",
    "    \n",
    "    return new_V\n",
    "\n",
    "def iterative_policy_evaluation(policy, env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Iteratively apply Bellman equation until convergence\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_V = bellman_update_all_states(policy, env, V, gamma)\n",
    "        \n",
    "        # Check for convergence\n",
    "        delta = np.max(np.abs(new_V - V))\n",
    "        V = new_V\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "# Example usage with FrozenLake\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "# Simple policy: always go right when possible, otherwise down\n",
    "simple_policy = {\n",
    "    0: 2, 1: 2, 2: 2, 3: 1,     # Row 1: right, right, right, down\n",
    "    4: 2, 5: 1, 6: 1, 7: 1,     # Row 2: right, down, down, down  \n",
    "    8: 2, 9: 2, 10: 1, 11: 1,   # Row 3: right, right, down, down\n",
    "    12: 2, 13: 2, 14: 2         # Row 4: right, right, right\n",
    "}\n",
    "\n",
    "# Compute state values using Bellman equation\n",
    "V_bellman = iterative_policy_evaluation(simple_policy, env, gamma=0.9)\n",
    "\n",
    "print(\"State Values computed using Bellman Equation:\")\n",
    "for i in range(4):\n",
    "    row = [f\"{V_bellman[i*4 + j]:.3f}\" for j in range(4)]\n",
    "    print(\" | \".join(f\"{val:>6}\" for val in row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a4aa8",
   "metadata": {},
   "source": [
    "#### Code Explanation:\n",
    "- **Single State Update**: Applies Bellman equation to compute new value for one state\n",
    "- **Batch Update**: Updates all states using current value estimates\n",
    "- **Iterative Process**: Repeats until values converge (change less than threshold)\n",
    "- **Convergence Check**: Monitors maximum change in values across all states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dfb76b",
   "metadata": {},
   "source": [
    "### 7.4 Bellman Optimality Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f026c0",
   "metadata": {},
   "source": [
    "#### Mathematical Formulation:\n",
    "- **For Optimal State Values**:\n",
    "  - $V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$\n",
    "\n",
    "- **For Optimal Action Values**:\n",
    "  - $Q^*(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')]$\n",
    "\n",
    "#### Key Differences from Policy-Specific Bellman Equation:\n",
    "- **Optimization**: Takes maximum over actions instead of following fixed policy\n",
    "- **Optimal Values**: Represents best possible performance from each state\n",
    "- **Policy Independence**: Defines optimal values regardless of current policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b703e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged after 78 iterations\n",
      "\n",
      "Optimal State Values:\n",
      " 0.069 |  0.061 |  0.074 |  0.056\n",
      " 0.092 |  0.000 |  0.112 |  0.000\n",
      " 0.145 |  0.247 |  0.300 |  0.000\n",
      " 0.000 |  0.380 |  0.639 |  0.000\n"
     ]
    }
   ],
   "source": [
    "def bellman_optimality_operator(V, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Apply Bellman optimality operator to find optimal values\n",
    "    \"\"\"\n",
    "    new_V = np.zeros_like(V)\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal state\n",
    "        action_values = []\n",
    "        \n",
    "        for action in range(env.action_space.n):\n",
    "            action_value = 0\n",
    "            transitions = env.unwrapped.P[state][action]\n",
    "            \n",
    "            for prob, next_state, reward, is_terminal in transitions:\n",
    "                if is_terminal:\n",
    "                    action_value += prob * reward\n",
    "                else:\n",
    "                    action_value += prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "            action_values.append(action_value)\n",
    "        \n",
    "        # Take maximum over all actions\n",
    "        new_V[state] = max(action_values)\n",
    "    \n",
    "    return new_V\n",
    "\n",
    "def value_iteration(env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Find optimal value function using value iteration\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_V = bellman_optimality_operator(V, env, gamma)\n",
    "        \n",
    "        delta = np.max(np.abs(new_V - V))\n",
    "        V = new_V\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "# Find optimal values\n",
    "V_optimal = value_iteration(env, gamma=0.9)\n",
    "print(\"\\nOptimal State Values:\")\n",
    "for i in range(4):\n",
    "    row = [f\"{V_optimal[i*4 + j]:.3f}\" for j in range(4)]\n",
    "    print(\" | \".join(f\"{val:>6}\" for val in row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856273c2",
   "metadata": {},
   "source": [
    "### 7.5 Properties and Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4185e96a",
   "metadata": {},
   "source": [
    "#### **Convergence Guarantees:**\n",
    "\n",
    "- **Contraction Mapping**: Bellman operator is a contraction when Œ≥ < 1\n",
    "    - Guaranteed convergence to unique fixed point\n",
    "    - Convergence rate depends on discount factor\n",
    "\n",
    "- **Fixed Point**: Optimal value function satisfies $V^* = T^*V^*$ where $T^*$ is Bellman optimality operator\n",
    "    - Solution to system of Bellman optimality equations\n",
    "    - Represents true optimal values\n",
    "\n",
    "#### Computational Complexity:\n",
    "- **Time Complexity**: $O(|S|¬≤|A|)$ per iteration\n",
    "    - Must examine all state-action pairs\n",
    "    - Transition probabilities determine exact complexity\n",
    "\n",
    "- **Space Complexity**: $O(|S|)$\n",
    "    - Store value function for all states\n",
    "    - Additional space for transition probabilities\n",
    "\n",
    "- **Convergence Rate**: Geometric with rate Œ≥\n",
    "    - Faster convergence with smaller discount factors\n",
    "    - Trade-off between solution quality and computation time\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b528bc8",
   "metadata": {},
   "source": [
    "### 1) **what $Q^\\pi(s,a)$ means**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\mathbb{E}_\\pi\\big[G_t \\mid S_t=s, A_t=a\\big]\n",
    "  $$\n",
    "* Symbols:\n",
    "\n",
    "  * $G_t$ = total return from time $t$: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots$\n",
    "  * $\\mathbb{E}_\\pi[\\cdot]$ = expectation when actions after time $t$ are chosen according to policy $\\pi$.\n",
    "  * Condition $\\mid S_t=s, A_t=a$ means: **we start in state $s$ and take action $a$ now**; randomness afterward is from environment transitions and the policy.\n",
    "* Intuition: average (over possible randomness) of **immediate reward + discounted future rewards** after taking $a$ in $s$.\n",
    "* Tiny numeric example:\n",
    "\n",
    "  * Suppose $\\gamma=0.9$, immediate reward $r=2$, and expected future return $V^\\pi(s')=5$.\n",
    "  * Then $Q^\\pi(s,a)=2 + 0.9\\times 5 = 6.5$.\n",
    "\n",
    "### 2) **Break the return into immediate + future (derivation)**\n",
    "\n",
    "* Start from definition:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\mathbb{E}\\big[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\big]\n",
    "  $$\n",
    "* Because expectation is linear:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\mathbb{E}[R_{t+1}\\mid s,a] \\;+\\; \\gamma \\,\\mathbb{E}[G_{t+1}\\mid s,a]\n",
    "  $$\n",
    "* Interpretations:\n",
    "\n",
    "  * $\\mathbb{E}[R_{t+1}\\mid s,a]$ = expected immediate reward after doing $a$ in $s$.\n",
    "  * $\\mathbb{E}[G_{t+1}\\mid s,a]$ = expected future return from the next time step onward.\n",
    "\n",
    "### 3) **Deterministic one-step Bellman form**\n",
    "\n",
    "* Formula (deterministic next state $s'$ and deterministic immediate reward $r_a$):\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = r_a + \\gamma V^\\pi(s')\n",
    "  $$\n",
    "* Why this follows:\n",
    "\n",
    "  * If taking $a$ from $s$ always lands in $s'$ and gives reward $r_a$, then $\\mathbb{E}[R_{t+1}\\mid s,a]=r_a$ and $\\mathbb{E}[G_{t+1}\\mid s,a]=V^\\pi(s')$.\n",
    "* Intuition: **immediate reward** + **discounted value of the known next state**.\n",
    "\n",
    "### 4) **Stochastic transitions ‚Äî expectation over next states**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\sum_{s'} P(s'\\mid s,a)\\,\\big[ R(s,a,s') + \\gamma V^\\pi(s')\\big]\n",
    "  $$\n",
    "* Explanation:\n",
    "\n",
    "  * If taking $a$ can lead to several possible next states $s'$, each with probability $P(s'|s,a)$, you average the immediate reward + future value for each possible $s'$.\n",
    "* Numeric example (showing every arithmetic step):\n",
    "\n",
    "  * Let two next states $s_1,s_2$ with probabilities $0.7$ and $0.3$.\n",
    "  * Rewards: $R(s,a,s_1)=1$, $R(s,a,s_2)=2$.\n",
    "  * Values: $V^\\pi(s_1)=3$, $V^\\pi(s_2)=4$.\n",
    "  * $\\gamma=0.9$.\n",
    "  * Compute for $s_1$: inner = $1 + 0.9\\times 3 = 2.7$.\n",
    "    * Multiply by prob: $0.7\\times 3.7 = 2.59$.\n",
    "  \n",
    "  * Compute for $s_2$: inner = $2 + 0.9\\times 4 = 5.6$.\n",
    "    * Multiply by prob: $0.3\\times 5.6 = 1.68$.\n",
    "    * So $Q^\\pi(s,a) = 2.59 + 1.68 = 4.27$.\n",
    "\n",
    "### 5) **Bellman expectation in terms of $Q$ (no $V$ needed)**\n",
    "\n",
    "* Start: $V^\\pi(s') = \\sum_{a'} \\pi(a'\\mid s')\\,Q^\\pi(s',a')$.\n",
    "* Substitute into previous expression:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\sum_{s'} P(s'\\mid s,a)\\Big[ R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'\\mid s')\\,Q^\\pi(s',a')\\Big].\n",
    "  $$\n",
    "* Why useful:\n",
    "\n",
    "  * This is a **self-consistent equation** for $Q^\\pi$ only. Solve it (analytically or iteratively) to find the Q-values of a policy.\n",
    "* Iteration form (policy evaluation):\n",
    "\n",
    "  $$\n",
    "  Q_{k+1}(s,a) \\leftarrow \\sum_{s'} P(s'\\mid s,a)\\Big[R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'\\mid s') Q_k(s',a')\\Big].\n",
    "  $$\n",
    "\n",
    "  * Keep updating $Q$ until it converges.\n",
    "\n",
    "### 6) **Relationship $V^\\pi$ ‚Üî $Q^\\pi$**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  V^\\pi(s) = \\sum_a \\pi(a\\mid s)\\,Q^\\pi(s,a)\n",
    "  $$\n",
    "* Meaning: state-value = **expected Q-value when actions are sampled from $\\pi$**.\n",
    "* Quick numeric example:\n",
    "\n",
    "  * Suppose two actions with probabilities $0.6$ and $0.4$.\n",
    "  * $Q(s,a_1)=10,\\; Q(s,a_2)=2$.\n",
    "  * Then $V(s)=0.6\\times 10 + 0.4\\times 2 = 6.8$.\n",
    "\n",
    "### 7) **Bellman *optimality* equation (why the $\\max$ appears)**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  Q^*(s,a) = \\sum_{s'} P(s'\\mid s,a)\\Big[ R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')\\Big]\n",
    "  $$\n",
    "* Explanation:\n",
    "\n",
    "  * $Q^*$ assumes **after taking action $a$ now we will act optimally thereafter**.\n",
    "  * So the future value from $s'$ is the **maximum** Q-value over all actions available in $s'$: $V^*(s')=\\max_{a'}Q^*(s',a')$.\n",
    "* Small numeric illustration:\n",
    "\n",
    "  * Suppose deterministic next state $s'$, $R=1$, $\\gamma=0.9$, and $\\max_{a'}Q^*(s',a')=9$.\n",
    "  * Then $Q^*(s,a)=1 + 0.9\\times 9 = 9.1$.\n",
    "\n",
    "* The optimal policy uses:\n",
    "\n",
    "  $$\n",
    "  \\pi^*(s) = \\arg\\max_a Q^*(s,a).\n",
    "  $$\n",
    "\n",
    "  * Pick the action that yields the largest $Q^*$ in that state.\n",
    "\n",
    "### 8) **Sample-based updates (how algorithms use these formulas)**\n",
    "\n",
    "* **Q-Learning (off-policy)** one-step sample update (common in practice):\n",
    "\n",
    "  $$\n",
    "  Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[ r + \\gamma\\max_{a'}Q(s',a') - Q(s,a)\\big]\n",
    "  $$\n",
    "\n",
    "  * Use when you sample a transition $(s,a,r,s')$.\n",
    "  * Intuition: move $Q(s,a)$ toward the sample target $r + \\gamma\\max_{a'}Q(s',a')$.\n",
    "* **SARSA (on-policy)**:\n",
    "\n",
    "  $$\n",
    "  Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[ r + \\gamma Q(s',a') - Q(s,a)\\big]\n",
    "  $$\n",
    "\n",
    "  * Here $a'$ is the actual next action chosen by current policy; used when learning while following that policy.\n",
    "\n",
    "### 9) **Terminal-state conventions**\n",
    "\n",
    "* Common options:\n",
    "\n",
    "  * Set $Q(\\text{terminal},a)=0$ for all $a$.\n",
    "  * Or skip/ignore those states in updates.\n",
    "* Both are fine if applied consistently.\n",
    "\n",
    "### 10) **Quick intuitive checklist** üß≠\n",
    "\n",
    "* If you *know* the next state exactly ‚Üí use $Q = r + \\gamma V(\\text{next})$.\n",
    "* If next state is random ‚Üí average over next states with $P(s'|s,a)$.\n",
    "* If you have a policy $\\pi$ and want to compute its values ‚Üí use the Bellman expectation (in terms of $Q$ or $V$).\n",
    "* If you want the best possible behavior ‚Üí replace expected future actions by $\\max$ (Bellman optimality), then act greedily w\\.r.t. $Q^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507135ed",
   "metadata": {},
   "source": [
    "### Complete Q-Value Computation\n",
    "\n",
    "```python\n",
    "def compute_q_value(state, action):  \n",
    "    if state == terminal_state: \n",
    "        return None \n",
    " \n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0] \n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "289a2d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values:\n",
      "{(0, 0): np.float64(0.0), (0, 1): np.float64(0.0), (0, 2): np.float64(0.0), (0, 3): np.float64(0.0), (1, 0): np.float64(0.0), (1, 1): np.float64(0.0), (1, 2): np.float64(0.0), (1, 3): np.float64(0.0), (2, 0): np.float64(0.0), (2, 1): np.float64(0.0), (2, 2): np.float64(0.0), (2, 3): np.float64(0.0), (3, 0): np.float64(0.0), (3, 1): np.float64(0.0), (3, 2): np.float64(0.0), (3, 3): np.float64(0.0), (4, 0): np.float64(0.0), (4, 1): np.float64(0.0), (4, 2): np.float64(0.0), (4, 3): np.float64(0.0), (5, 0): np.float64(0.0), (5, 1): np.float64(0.0), (5, 2): np.float64(0.0), (5, 3): np.float64(0.0), (6, 0): np.float64(0.0), (6, 1): np.float64(0.0), (6, 2): np.float64(0.0), (6, 3): np.float64(0.0), (7, 0): np.float64(0.0), (7, 1): np.float64(0.0), (7, 2): np.float64(0.0), (7, 3): np.float64(0.0), (8, 0): np.float64(0.0), (8, 1): np.float64(0.0), (8, 2): np.float64(0.0), (8, 3): np.float64(0.0), (9, 0): np.float64(0.0), (9, 1): np.float64(0.0), (9, 2): np.float64(0.0), (9, 3): np.float64(0.0), (10, 0): np.float64(0.0), (10, 1): np.float64(0.0), (10, 2): np.float64(0.0), (10, 3): np.float64(0.0), (11, 0): np.float64(0.0), (11, 1): np.float64(0.0), (11, 2): np.float64(0.0), (11, 3): np.float64(0.0), (12, 0): np.float64(0.0), (12, 1): np.float64(0.0), (12, 2): np.float64(0.0), (12, 3): np.float64(0.0), (13, 0): np.float64(0.0), (13, 1): np.float64(0.0), (13, 2): np.float64(0.0), (13, 3): np.float64(0.0), (14, 0): np.float64(0.0), (14, 1): np.float64(0.0), (14, 2): np.float64(0.0), (14, 3): np.float64(1.9), (15, 0): 0, (15, 1): 0, (15, 2): 0, (15, 3): 0}\n",
      "\n",
      "Improved Policy:\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 3}\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 12, Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Import Required Libraries\n",
    "# ============================================\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. Initialize Environment & Parameters\n",
    "# ============================================\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "num_states = env.observation_space.n     # Number of states in FrozenLake\n",
    "num_actions = env.action_space.n         # Number of possible actions\n",
    "terminal_state = 15                      # Goal state index in FrozenLake\n",
    "gamma = 0.9                              # Discount factor\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. Initialize Value Function\n",
    "# ============================================\n",
    "# Value function stores the \"goodness\" of each state\n",
    "V = np.zeros(num_states)     # Start with all states = 0\n",
    "V[terminal_state] = 1.0      # Goal state is assigned value 1\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Q-Value Computation Function\n",
    "# ============================================\n",
    "def compute_q_value(state, action, V):\n",
    "    \"\"\"\n",
    "    Compute the Q-value for a given (state, action) pair.\n",
    "\n",
    "    Parameters:\n",
    "        state  (int): Current state\n",
    "        action (int): Action taken in this state\n",
    "        V      (array): Current value function\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated Q-value\n",
    "    \"\"\"\n",
    "    # Terminal state has no future rewards\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    # env.unwrapped.P[state][action] gives the transition dynamics:\n",
    "    # [(probability, next_state, reward, done), ...]\n",
    "    probability, next_state, reward, done = env.unwrapped.P[state][action][0]\n",
    "    \n",
    "    # Bellman expectation equation\n",
    "    return reward + gamma * V[next_state]\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. Compute Q-values for All State-Action Pairs\n",
    "# ============================================\n",
    "Q = {\n",
    "    (state, action): compute_q_value(state, action, V)\n",
    "    for state in range(num_states)\n",
    "    for action in range(num_actions)\n",
    "}\n",
    "\n",
    "print(\"Q-values:\")\n",
    "print(Q)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. Greedy Policy Improvement\n",
    "# ============================================\n",
    "def improve_policy(Q, num_states, num_actions):\n",
    "    \"\"\"\n",
    "    Improve policy using greedy selection:\n",
    "    For each state, pick the action with the maximum Q-value.\n",
    "    \"\"\"\n",
    "    improved_policy = {}\n",
    "    \n",
    "    for state in range(num_states - 1):  # Exclude terminal state\n",
    "        max_action = max(range(num_actions), key=lambda action: Q[(state, action)])\n",
    "        improved_policy[state] = max_action\n",
    "    \n",
    "    return improved_policy\n",
    "\n",
    "\n",
    "# Generate improved policy\n",
    "policy = improve_policy(Q, num_states, num_actions)\n",
    "\n",
    "print(\"\\nImproved Policy:\")\n",
    "print(policy)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. Test the Improved Policy\n",
    "# ============================================\n",
    "state, _ = env.reset()   # Reset environment to initial state\n",
    "terminated = False\n",
    "\n",
    "while not terminated:\n",
    "    action = policy[state]  # Select action according to policy\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"State: {state}, Reward: {reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "620349ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for State 4:\n",
      "Q(4, Left) = 2.667\n",
      "Q(4, Down) = 2.333\n",
      "Q(4, Right) = 0.333\n",
      "Q(4, Up) = 2.667\n"
     ]
    }
   ],
   "source": [
    "def compute_q_value_deterministic(state, action, env, V, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Q-value for deterministic environment\n",
    "    \"\"\"\n",
    "    if state >= env.observation_space.n - 1:  # Terminal state\n",
    "        return 0\n",
    "    \n",
    "    # For deterministic transition, take first (and only) outcome\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    prob, next_state, reward, is_terminal = transitions[0]\n",
    "    \n",
    "    if is_terminal:\n",
    "        return reward\n",
    "    else:\n",
    "        return reward + gamma * V[next_state]\n",
    "\n",
    "def compute_q_value_stochastic(state, action, env, V, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Q-value for stochastic environment (expected value)\n",
    "    \"\"\"\n",
    "    if state >= env.observation_space.n - 1:  # Terminal state\n",
    "        return 0\n",
    "    \n",
    "    expected_q_value = 0\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    \n",
    "    for prob, next_state, reward, is_terminal in transitions:\n",
    "        if is_terminal:\n",
    "            contribution = reward\n",
    "        else:\n",
    "            contribution = reward + gamma * V[next_state]\n",
    "        \n",
    "        expected_q_value += prob * contribution\n",
    "    \n",
    "    return expected_q_value\n",
    "\n",
    "# Example: Computing all Q-values for a specific state\n",
    "def compute_all_q_values_for_state(state, env, V, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Q-values for all actions in a given state\n",
    "    \"\"\"\n",
    "    q_values = {}\n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    \n",
    "    for action in range(env.action_space.n):\n",
    "        q_val = compute_q_value_stochastic(state, action, env, V, gamma)\n",
    "        q_values[action] = q_val\n",
    "        print(f\"Q({state}, {action_names[action]}) = {q_val:.3f}\")\n",
    "    \n",
    "    return q_values\n",
    "\n",
    "# Example state values (from previous computation)\n",
    "example_V = {0: 1,\n",
    "            1: 8,\n",
    "            2: 9, \n",
    "            3: 2, \n",
    "            4: 7, \n",
    "            5: 10, \n",
    "            6: 3, \n",
    "            7: 5, \n",
    "            8: 0}\n",
    "\n",
    "# Compute Q-values for state 4\n",
    "print(\"Q-values for State 4:\")\n",
    "\n",
    "q_values_state_4 = compute_all_q_values_for_state(4, env, example_V, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c29dcf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete Q-Value Table:\n",
      "============================================================\n",
      "State  Left     Down     Right    Up      \n",
      "------------------------------------------------------------\n",
      "0      0.015    0.015    0.015    0.014   \n",
      "1      0.009    0.015    0.015    0.019   \n",
      "2      0.034    0.029    0.034    0.019   \n",
      "3      0.015    0.015    0.009    0.019   \n",
      "4      0.027    0.022    0.021    0.011   \n",
      "5      0.000    0.000    0.000    0.000   \n",
      "6      0.076    0.066    0.076    0.010   \n",
      "7      0.000    0.000    0.000    0.000   \n",
      "8      0.022    0.064    0.054    0.070   \n",
      "9      0.110    0.176    0.159    0.082   \n",
      "10     0.239    0.219    0.191    0.068   \n",
      "11     0.000    0.000    0.000    0.000   \n",
      "12     0.000    0.000    0.000    0.000   \n",
      "13     0.142    0.265    0.313    0.219   \n",
      "14     0.330    0.598    0.570    0.493   \n"
     ]
    }
   ],
   "source": [
    "def compute_all_q_values(policy, env, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Compute Q-values for all state-action pairs\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        for action in range(env.action_space.n):\n",
    "            if state >= env.observation_space.n - 1:  # Terminal state\n",
    "                Q[(state, action)] = None\n",
    "            else:\n",
    "                Q[(state, action)] = compute_q_value_stochastic(state, action, env, V, gamma)\n",
    "    return Q\n",
    "\n",
    "def display_q_table(Q, env):\n",
    "    \"\"\"\n",
    "    Display Q-values in a readable table format\n",
    "    \"\"\"\n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    \n",
    "    print(\"\\nComplete Q-Value Table:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'State':<6} {'Left':<8} {'Down':<8} {'Right':<8} {'Up':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for state in range(env.observation_space.n - 1):  # Exclude terminal\n",
    "        row = [f\"{state:<6}\"]\n",
    "        for action in range(env.action_space.n):\n",
    "            q_val = Q[(state, action)]\n",
    "            if q_val is not None:\n",
    "                row.append(f\"{q_val:<8.3f}\")\n",
    "            else:\n",
    "                row.append(f\"{'N/A':<8}\")\n",
    "        print(\" \".join(row))\n",
    "\n",
    "# Compute complete Q-table\n",
    "complete_Q = compute_all_q_values(simple_policy, env, V_bellman, gamma=0.9)\n",
    "display_q_table(complete_Q, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c2d48",
   "metadata": {},
   "source": [
    "### 8.5 Quick Comparison: $V^\\pi$ Vs $Q^\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af1987",
   "metadata": {},
   "source": [
    "| Function         | Input                 | Output         | Question it answers                                         | Intuition                                                                                              | Example                                                                                         |\n",
    "| ---------------- | --------------------- | -------------- | ----------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------- |\n",
    "| **$V^\\pi(s)$**   | State $s$             | Number (value) | ‚ÄúHow good is this state under policy $\\pi$?‚Äù                | Expected long-term return starting **from state $s$** and following policy $\\pi$.                      | In chess: value of a board position assuming you continue playing according to strategy $\\pi$.  |\n",
    "| **$Q^\\pi(s,a)$** | State $s$, Action $a$ | Number (value) | ‚ÄúHow good is this action in this state under policy $\\pi$?‚Äù | Expected long-term return starting **from state $s$**, taking action $a$, then following policy $\\pi$. | In chess: value of moving a knight to a specific square (action) given the current board state. |\n",
    "\n",
    "\n",
    "\n",
    "### üîë Key Differences\n",
    "\n",
    "1. **Scope**\n",
    "\n",
    "   * $V^\\pi(s)$ looks at **how desirable a state is overall**.\n",
    "   * $Q^\\pi(s,a)$ looks at **how desirable a specific action is in that state**.\n",
    "\n",
    "2. **Decision-making granularity**\n",
    "\n",
    "   * $V^\\pi(s)$ is coarse-grained (state-level).\n",
    "   * $Q^\\pi(s,a)$ is fine-grained (state-action level, more detailed).\n",
    "\n",
    "3. **Relation**\n",
    "\n",
    "   * $V^\\pi(s) = \\sum_a \\pi(a|s)\\, Q^\\pi(s,a)$\n",
    "     (state value is the expected action value under the policy).\n",
    "\n",
    "4. **Use cases**\n",
    "\n",
    "   * $V^\\pi(s)$: Used in **policy evaluation** (how good is my strategy overall?).\n",
    "   * $Q^\\pi(s,a)$: Used in **policy improvement** and **control** (which action should I pick?).\n",
    "\n",
    "\n",
    "\n",
    "### ‚≠ê Intuitive Summary\n",
    "\n",
    "* **$V$** = *‚ÄúValue of being in a place (state).‚Äù*\n",
    "* **$Q$** = *‚ÄúQuality of making a move (action) in that place.‚Äù*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
