{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c04416",
   "metadata": {},
   "source": [
    "# **‚≠êModel Based Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b4d75",
   "metadata": {},
   "source": [
    "## **‚úÖ1. Markov Decision Processes (MDPs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155590b4",
   "metadata": {},
   "source": [
    "### 1.1 What is an MDP?\n",
    "\n",
    "**Definition:** A Markov Decision Process (MDP) is a mathematical framework used to model decision-making in environments where outcomes are partly random and partly under the control of a decision-maker (agent). It's foundational in reinforcement learning and dynamic programming.\n",
    "\n",
    "**Purpose:** MDPs provide a formal way to describe environments where outcomes are partly random and partly under the control of a decision maker (agent).\n",
    "\n",
    "**Key Components of an MDP:**\n",
    "- **States (S):** All possible situations the agent can be in\n",
    "- **Actions (A):** All possible moves the agent can make\n",
    "- **Transition Probabilities (P):** Likelihood of moving from one state to another given an action\n",
    "- **Rewards (R):** Immediate feedback received after taking an action\n",
    "- **Discount Factor (Œ≥):** Weight given to future rewards (0 ‚â§ Œ≥ ‚â§ 1)\n",
    "\n",
    "### 1.2 The Markov Property\n",
    "\n",
    "**Definition:** The Markov Property states that the future state depends only on the `current state` and `action`, **NOT** on the entire history of past states and actions.\n",
    "\n",
    "- **Mathematical Expression:**\n",
    "$$P(S_{t+1} = s' | S_t = s, A_t = a, S_{t-1}, A_{t-1}, ..., S_0, A_0) = P(S_{t+1} = s' | S_t = s, A_t = a)$$\n",
    "\n",
    "- **Intuitive Explanation:** The current state contains all the information needed to make optimal decisions about the future.\n",
    "\n",
    "- **Goal of an Agent in MDP**\n",
    "  - The agent‚Äôs objective is to find a policy `ùúã(ùëé‚à£ùë†)` that maximizes the expected cumulative reward over time. This is often formalized using:\n",
    "      - **Value functions**: Estimate how good it is to be in a state or take an action.\n",
    "      - **Policy iteration** and **value iteration**: Algorithms to compute optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcb48e",
   "metadata": {},
   "source": [
    "### **Frozen Lake**\n",
    "\n",
    "**Environment Description:** An agent must navigate across a frozen lake to reach a goal while avoiding holes.\n",
    "\n",
    "**Components:**\n",
    "- **States:** 16 positions (4√ó4 grid) numbered 0-15\n",
    "- **Actions:** 4 possible moves (0: left, 1: down, 2: right, 3: up)\n",
    "- **Terminal States:** Goal state (rewards +1) and hole states (episode ends) ~ 6\n",
    "- **Transition Probabilities:** Actions don't always lead to expected outcomes due to slippery ice\n",
    "\n",
    "![frozen-lake-png](_img\\frozen-lake.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bea402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n",
      "Number of actions: 4\n",
      "Number of states: 16\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "# Check state and action spaces\n",
    "print(env.action_space)          \n",
    "print(env.observation_space)    \n",
    "\n",
    "print(\"Number of actions:\", env.action_space.n)      \n",
    "print(\"Number of states:\", env.observation_space.n)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96144e0",
   "metadata": {},
   "source": [
    "### 1.4 Transition Probabilities and Rewards\n",
    "\n",
    "**Accessing Transition Information:**\n",
    "```python\n",
    "# env.unwrapped.P[state][action] returns:\n",
    "# [(probability_1, next_state_1, reward_1, is_terminal_1),\n",
    "#  (probability_2, next_state_2, reward_2, is_terminal_2), ...]\n",
    "\n",
    "state = 6\n",
    "action = 0  # left\n",
    "print(env.unwrapped.P[state][action])\n",
    "# Output: [(0.333, 2, 0.0, False), (0.333, 5, 0.0, True), (0.333, 10, 0.0, False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aaa227",
   "metadata": {},
   "source": [
    "### **CliffWalking**\n",
    "\n",
    "- The Cliff Walking environment involves an agent crossing a grid world from start to goal while avoiding falling off a cliff.\n",
    "- If the player moves to a cliff location it returns to the start location.\n",
    "- The player makes moves until they reach the goal, which ends the episode.\n",
    "- Your task is to explore the state and action spaces of this environment.\n",
    "\n",
    "![cliff-walking-gif](_img\\cliff_walking.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a46672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "Number of states: 48\n",
      "[(1.0, np.int64(23), -1, False)]\n",
      "Action: 0 | Probability: 1.0, Next State: 23, Reward: -1, Done: False\n",
      "[(1.0, np.int64(35), -1, False)]\n",
      "Action: 1 | Probability: 1.0, Next State: 35, Reward: -1, Done: False\n",
      "[(1.0, np.int64(47), -1, True)]\n",
      "Action: 2 | Probability: 1.0, Next State: 47, Reward: -1, Done: True\n",
      "[(1.0, np.int64(34), -1, False)]\n",
      "Action: 3 | Probability: 1.0, Next State: 34, Reward: -1, Done: False\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym   \n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Environment Setup\n",
    "# ==============================\n",
    "# Create the CliffWalking environment.\n",
    "# \"CliffWalking-v1\" is a classic control problem from reinforcement learning.\n",
    "# \"render_mode='rgb_array'\" means the environment won't open a window;\n",
    "# instead, it keeps the visual output as an image array (useful for debugging or rendering later).\n",
    "env = gym.make('CliffWalking-v1', render_mode='rgb_array')\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Action and State Spaces\n",
    "# ==============================\n",
    "# Number of possible actions the agent can take (Up, Right, Down, Left = 4).\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Number of possible states in the gridworld (4 rows √ó 12 columns = 48).\n",
    "num_states = env.observation_space.n\n",
    "\n",
    "print(\"Number of actions:\", num_actions)\n",
    "print(\"Number of states:\", num_states)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Exploring Transitions\n",
    "# ==============================\n",
    "# Each state has a set of transitions, depending on the chosen action.\n",
    "# Let's pick a specific state (for example, 35) and explore what happens when we try different actions.\n",
    "state = 35\n",
    "\n",
    "# Loop through all possible actions from this state\n",
    "for action in range(num_actions):\n",
    "    # The environment has an internal dictionary \"P\" that stores transitions.\n",
    "    # P[state][action] gives a list of possible outcomes when taking `action` in `state`.\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    print(transitions)\n",
    "\n",
    "    # Each transition has the format: (probability, next_state, reward, done)\n",
    "    # -> probability: chance of this outcome (usually 1.0 for deterministic envs like CliffWalking)\n",
    "    # -> next_state: the state you land in after the action\n",
    "    # -> reward: the reward received for this action\n",
    "    # -> done: whether the episode ends after this transition\n",
    "    for transition in transitions:\n",
    "        probability, next_state, reward, done = transition\n",
    "        print(f\"Action: {action} | Probability: {probability}, Next State: {next_state}, Reward: {reward}, Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c01e753",
   "metadata": {},
   "source": [
    "## **‚úÖ2. Policies and State-Value Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c740b4",
   "metadata": {},
   "source": [
    "### **‚ú®2.1 Policies**\n",
    "\n",
    "**Definition:** A policy $\\pi$ is a strategy that defines which action to take in each state to maximize the expected cumulative reward (return).\n",
    "\n",
    "**Types:**\n",
    "- **Deterministic Policy:** Always chooses the same action for a given state\n",
    "- **Stochastic Policy:** Chooses actions according to a probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e187a4",
   "metadata": {},
   "source": [
    "### **‚ú®2.2 State-Value Functions**\n",
    "\n",
    "**Definition:**\n",
    "The **state-value function** $V(s)$ estimates how good it is to be in a given state $s$.\n",
    "It represents the **expected return (sum of discounted future rewards)** when starting in state $s$ and following a given policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Mathematical Expression (Expanded Form):**\n",
    "\n",
    "$$\n",
    "V(s) = r_{s+1} + \\gamma r_{s+2} + \\gamma^2 r_{s+3} + \\cdots + \\gamma^{n-1} r_{s+n}\n",
    "$$\n",
    "\n",
    "‚úÖ **Interpretation:**\n",
    "\n",
    "* $r_{s+1}$: Immediate reward after leaving state $s$\n",
    "* $\\gamma r_{s+2}$: Next reward, discounted by factor $\\gamma$\n",
    "* $\\gamma^2 r_{s+3}$: Reward two steps later, discounted further\n",
    "* $\\cdots$ Continues infinitely\n",
    "* $\\gamma \\in [0,1]$: Discount factor that balances **present vs. future rewards**\n",
    "\n",
    "üìå **When to use:**\n",
    "\n",
    "* **Conceptual / Theoretical explanation** of value functions.\n",
    "* When introducing RL to beginners ‚Üí easy to show ‚Äúwhy future rewards are discounted.‚Äù\n",
    "* To **manually calculate returns** in very short episodes (e.g., toy problems like a 3-step grid world).\n",
    "* Useful in **Monte Carlo methods**, where we sample entire episodes and directly compute the return.\n",
    "* Not practical for real-world problems because we can‚Äôt compute infinite sums.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Bellman Equation (Recursive Form):**\n",
    "\n",
    "$$\n",
    "V(s) = r_{s+1} + \\gamma V(s+1)\n",
    "$$\n",
    "\n",
    "‚úÖ **Interpretation:**\n",
    "\n",
    "* $r_{s+1}$: Reward right after leaving state $s$\n",
    "* $\\gamma V(s+1)$: The discounted *value of the next state*\n",
    "* Turns the infinite sum into a **recursive relationship**\n",
    "\n",
    "üìå **When to use:**\n",
    "\n",
    "* **Dynamic Programming (DP):**\n",
    "  * `Value Iteration`\n",
    "  * `Policy Iteration`\n",
    "  * `Policy Evaluation`\n",
    "* Great for **deterministic environments**, where the next state is known with certainty.\n",
    "* Useful in **algorithm derivations**, since recursion makes equations easier to solve.\n",
    "* Helps in **bootstrapping methods** like Temporal-Difference (TD) learning, where we approximate returns by one-step lookahead instead of waiting for the whole episode.\n",
    "* Key in proving convergence of RL algorithms.\n",
    "\n",
    "üí° Example: In **Gridworld**, if moving right always gives +1 reward, then\n",
    "\n",
    "$$\n",
    "V(s) = 1 + \\gamma V(s')\n",
    "$$\n",
    "\n",
    "is easier to compute recursively instead of expanding all rewards.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. General Bellman Equation with Policy (Full Form):**\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\Big[ R(s,a,s') + \\gamma V^\\pi(s') \\Big]\n",
    "$$\n",
    "\n",
    "‚úÖ **Interpretation:**\n",
    "\n",
    "* $\\pi(a|s)$: Policy ‚Üí probability of taking action $a$ in state $s$\n",
    "* $P(s'|s,a)$: Transition probability ‚Üí chance of landing in state $s'$\n",
    "* $R(s,a,s')$: Expected reward for going from $s$ to $s'$ with action $a$\n",
    "* $V^\\pi(s')$: Value of the next state under policy $\\pi$\n",
    "* Captures **stochasticity in both actions and environment dynamics**\n",
    "\n",
    "üìå **When to use:**\n",
    "\n",
    "* In **real-world MDPs** where:\n",
    "\n",
    "  * Multiple actions are possible\n",
    "  * Transitions are probabilistic (not deterministic)\n",
    "* Central to **Reinforcement Learning algorithms**:\n",
    "\n",
    "  * **Policy Evaluation** (compute value of a given policy)\n",
    "  * **Policy Iteration** (improve policies step by step)\n",
    "  * **Actor-Critic methods** (where the critic estimates $V^\\pi$)\n",
    "* Needed when using **model-based RL**, since it explicitly uses transition probabilities $P(s'|s,a)$.\n",
    "* Forms the foundation for **policy optimization methods** like Policy Gradient, A2C, PPO (where value functions are approximated).\n",
    "\n",
    "üí° Example: In a **robot navigation task**, if the robot in state $s$ has a 70% chance to move forward and a 30% chance to slip sideways, this equation correctly handles those probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "‚ú® **Usage:**\n",
    "\n",
    "* **Expanded formula** ‚Üí Good for intuition, teaching, and small toy problems. Used in Monte Carlo methods.\n",
    "* **Recursive Bellman formula** ‚Üí Practical for computation, DP, and TD-learning. Efficient because it uses recursion instead of full sum.\n",
    "* **General Bellman with policy** ‚Üí Realistic MDPs with stochastic transitions and multiple actions. Core of almost all RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bbb28e",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create the environment\n",
    "env = gym.make('MyGridWorld', render_mode='rgb_array')\n",
    "state, info = env.reset()\n",
    "\n",
    "# Example Policy (Grid World)\n",
    "# 0: left, 1: down, 2: right, 3: up\n",
    "\n",
    "policy = {\n",
    "    0: 1,  # In state 0, go down\n",
    "    1: 2,  # In state 1, go right\n",
    "    2: 1,  # In state 2, go down\n",
    "    3: 1,  # In state 3, go down\n",
    "    4: 3,  # In state 4, go up\n",
    "    5: 1,  # In state 5, go down\n",
    "    6: 2,  # In state 6, go right\n",
    "    7: 3   # In state 7, go up\n",
    "}\n",
    "\n",
    "# Policy Execution\n",
    "terminated = False\n",
    "while not terminated:\n",
    "  # Select action based on policy \n",
    "  action = policy[state]\n",
    "  state, reward, terminated, truncated, info = env.step(action)\n",
    "  # Render the environment\n",
    "  render()\n",
    "\n",
    "# State-Value Functions\n",
    "def compute_state_value(state, policy):\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state, policy)\n",
    "\n",
    "# Compute all state values\n",
    "gamma = 1\n",
    "terminal_state = 8\n",
    "state_values = {state: compute_state_value(state, policy) for state in range(num_states)}\n",
    "print(state_values)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4467d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CUSTOM 3x3 GRIDWORLD WITH POLICY AND STATE-VALUES ===\n",
      "\n",
      "Custom 3x3 GridWorld Layout:\n",
      "S    \n",
      "  M M\n",
      "    D\n",
      "\n",
      "State-values: {0: 1, 1: 8, 2: 9, 3: 2, 4: 7, 5: 10, 6: 3, 7: 5, 8: 0}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CUSTOM 3x3 GRIDWORLD WITH POLICY AND STATE-VALUES ===\")\n",
    "print()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Custom 3x3 GridWorld Env\n",
    "# -------------------------------\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"ansi\"]}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shape = (3, 3)\n",
    "        self.observation_space = spaces.Discrete(9)\n",
    "        self.action_space = spaces.Discrete(4)  # 0:left,1:down,2:right,3:up\n",
    "        \n",
    "        # Define rewards\n",
    "        self.terminal_state = 8\n",
    "        self.rewards = {8: 10, 4: -2, 7: -2}\n",
    "        \n",
    "        # Precompute P like Gym\n",
    "        self.P = {s: {a: [] for a in range(4)} for s in range(9)}\n",
    "        for s in range(9):\n",
    "            for a in range(4):\n",
    "                ns = self._move(s, a)\n",
    "                r = self.rewards.get(ns, -1)\n",
    "                done = ns == self.terminal_state\n",
    "                self.P[s][a] = [(1.0, ns, r, done)]  # deterministic\n",
    "                \n",
    "    def _move(self, state, action):\n",
    "        if state == self.terminal_state:\n",
    "            return state\n",
    "        row, col = state // 3, state % 3\n",
    "        if action == 0:    # left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 1:  # down\n",
    "            row = min(2, row + 1)\n",
    "        elif action == 2:  # right\n",
    "            col = min(2, col + 1)\n",
    "        elif action == 3:  # up\n",
    "            row = max(0, row - 1)\n",
    "        return row * 3 + col\n",
    "    \n",
    "    def render(self, mode=\"ansi\"):\n",
    "        grid = np.full(self.shape, \" \")\n",
    "        grid[0,0] = \"S\"  # start\n",
    "        grid[2,2] = \"D\"  # diamond\n",
    "        grid[1,1] = grid[1,2] = \"M\"  # mountains\n",
    "        return \"\\n\".join([\" \".join(row) for row in grid])\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnv()\n",
    "num_states = env.observation_space.n\n",
    "gamma = 1\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define a deterministic policy\n",
    "# -------------------------------\n",
    "policy = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3,  # up\n",
    "    8: 0   # terminal\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Compute state values\n",
    "# -------------------------------\n",
    "def compute_state_value(state):\n",
    "    if state == env.terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "\n",
    "V = {s: compute_state_value(s) for s in range(num_states)}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Display results\n",
    "# -------------------------------\n",
    "print(\"Custom 3x3 GridWorld Layout:\")\n",
    "print(env.render())\n",
    "print()\n",
    "print(\"State-values:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c209f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy 1: {0: 'down', 1: 'right', 2: 'down', 3: 'down', 4: 'up', 5: 'down', 6: 'right', 7: 'up'}\n",
      "Policy 2: {0: 'right', 1: 'right', 2: 'down', 3: 'right', 4: 'right', 5: 'down', 6: 'right', 7: 'right'}\n",
      "\n",
      "2. STATE-VALUE FUNCTIONS\n",
      "========================\n",
      "V(s) = Expected return starting from state s following policy œÄ\n",
      "\n",
      "Computing state values...\n",
      "\n",
      "RESULTS:\n",
      "========\n",
      "State-values for Policy 1: {0: 1.0, 1: 8.0, 2: 9.0, 3: 2.0, 4: 7.0, 5: 10.0, 6: 3.0, 7: 5.0, 8: 0}\n",
      "State-values for Policy 2: {0: 7.0, 1: 8.0, 2: 9.0, 3: 7.0, 4: 9.0, 5: 10.0, 6: 8.0, 7: 10.0, 8: 0}\n",
      "\n",
      "EXAMPLE CALCULATION (Policy 1, State 2):\n",
      "========================================\n",
      "State 2 ‚Üí Action down ‚Üí State 5\n",
      "Reward: -1\n",
      "V(2) = -1 + 1.0 √ó V(5) = -1 + 1.0 √ó 10.0 = 9.0\n",
      "\n",
      "POLICY COMPARISON:\n",
      "==================\n",
      "Total value (Policy 1): 45.0\n",
      "Total value (Policy 2): 68.0\n",
      "Better policy: Policy 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# 1. POLICIES\n",
    "# ==========================\n",
    "\n",
    "# Actions: 0: left, 1: down, 2: right, 3: up\n",
    "policy1 = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3   # up\n",
    "}\n",
    "\n",
    "policy2 = {\n",
    "    0: 2,  # right\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 2,  # right\n",
    "    4: 2,  # right\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 2   # right\n",
    "}\n",
    "\n",
    "action_names = {0: 'left', 1: 'down', 2: 'right', 3: 'up'}\n",
    "\n",
    "print(\"Policy 1:\", {s: action_names[a] for s, a in policy1.items()})\n",
    "print(\"Policy 2:\", {s: action_names[a] for s, a in policy2.items()})\n",
    "print()\n",
    "\n",
    "# ==========================\n",
    "# 2. ENVIRONMENT MODEL (P)\n",
    "# ==========================\n",
    "gamma = 1.0          # Discount factor\n",
    "num_states = 9       # 3x3 grid\n",
    "terminal_state = 8   # Diamond\n",
    "\n",
    "# Build transition table: P[state][action] = [(prob, next_state, reward, done)]\n",
    "P = {s: {a: [] for a in range(4)} for s in range(num_states)}\n",
    "\n",
    "def move(state, action):\n",
    "    \"\"\"Return next state after taking an action.\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return state\n",
    "    \n",
    "    row, col = state // 3, state % 3\n",
    "    if action == 0:    # left\n",
    "        col = max(0, col - 1)\n",
    "    elif action == 1:  # down\n",
    "        row = min(2, row + 1)\n",
    "    elif action == 2:  # right\n",
    "        col = min(2, col + 1)\n",
    "    elif action == 3:  # up\n",
    "        row = max(0, row - 1)\n",
    "    return row * 3 + col\n",
    "\n",
    "def reward(next_state):\n",
    "    \"\"\"Return reward for landing in next_state.\"\"\"\n",
    "    if next_state == 8:   # Diamond\n",
    "        return 10\n",
    "    elif next_state in [4, 7]:  # Mountains\n",
    "        return -2\n",
    "    else:  # All other states\n",
    "        return -1\n",
    "\n",
    "# Fill transition table\n",
    "for s in range(num_states):\n",
    "    for a in range(4):\n",
    "        ns = move(s, a)\n",
    "        r = reward(ns)\n",
    "        done = (ns == terminal_state)\n",
    "        P[s][a] = [(1.0, ns, r, done)]   # deterministic env\n",
    "\n",
    "# ==========================\n",
    "# 3. STATE-VALUE FUNCTIONS\n",
    "# ==========================\n",
    "print(\"2. STATE-VALUE FUNCTIONS\")\n",
    "print(\"========================\")\n",
    "print(\"V(s) = Expected return starting from state s following policy œÄ\")\n",
    "print()\n",
    "\n",
    "def compute_state_value(state, policy):\n",
    "    \"\"\"Bellman expectation with env.P\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    transitions = P[state][action]\n",
    "    \n",
    "    value = 0\n",
    "    for prob, next_state, reward, _ in transitions:\n",
    "        value += prob * (reward + gamma * compute_state_value(next_state, policy))\n",
    "    return value\n",
    "\n",
    "# Calculate state values for both policies\n",
    "print(\"Computing state values...\")\n",
    "print()\n",
    "\n",
    "V1 = {s: compute_state_value(s, policy1) for s in range(num_states)}\n",
    "V2 = {s: compute_state_value(s, policy2) for s in range(num_states)}\n",
    "\n",
    "# ==========================\n",
    "# 4. RESULTS\n",
    "# ==========================\n",
    "print(\"RESULTS:\")\n",
    "print(\"========\")\n",
    "print(\"State-values for Policy 1:\", V1)\n",
    "print(\"State-values for Policy 2:\", V2)\n",
    "print()\n",
    "\n",
    "# Example calculation walkthrough\n",
    "print(\"EXAMPLE CALCULATION (Policy 1, State 2):\")\n",
    "print(\"========================================\")\n",
    "state = 2\n",
    "action = policy1[state]\n",
    "prob, next_state, reward, _ = P[state][action][0]\n",
    "print(f\"State 2 ‚Üí Action {action_names[action]} ‚Üí State {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"V(2) = {reward} + {gamma} √ó V({next_state}) = {reward} + {gamma} √ó {V1[next_state]} = {V1[2]}\")\n",
    "print()\n",
    "\n",
    "# Compare policies\n",
    "print(\"POLICY COMPARISON:\")\n",
    "print(\"==================\")\n",
    "total1 = sum(V1[s] for s in range(8))  # exclude terminal\n",
    "total2 = sum(V2[s] for s in range(8))\n",
    "print(f\"Total value (Policy 1): {total1}\")\n",
    "print(f\"Total value (Policy 2): {total2}\")\n",
    "print(f\"Better policy: Policy {'2' if total2 > total1 else '1'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa7797",
   "metadata": {},
   "source": [
    "## **‚úÖ3. Action-Value Functions (Q-Values)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7d2fe",
   "metadata": {},
   "source": [
    "### **‚ú®3.1 Action-Value (Q) Function**\n",
    "\n",
    "**Definition:**\n",
    "The **action-value function** $Q^\\pi(s,a)$ is the **expected return** when you **start in state $s$**, **take action $a$**, and then **follow policy $\\pi$** thereafter:\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ G_t \\;\\middle|\\; S_t = s,\\; A_t = a \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Bellman forms (from simplest ‚Üí most general)**\n",
    "\n",
    "1. **Deterministic one-step (matches the slide):**\n",
    "   If taking $a$ from $s$ deterministically yields next state $s'$ and reward $r_a$:\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = r_a + \\gamma\\,V^\\pi(s')\n",
    "$$\n",
    "\n",
    "‚û°Ô∏è **Interpretation:** \"Immediate reward now + discounted value of the next state.\"\n",
    "\n",
    "---\n",
    "\n",
    "2. **Expectation over next states (using $V^\\pi$):**\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\sum_{s'} P(s' \\mid s,a) \\Big[ R(s,a,s') + \\gamma\\,V^\\pi(s') \\Big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "3. **Bellman expectation in terms of $Q^\\pi$ only (no $V$ needed):**\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\sum_{s'} P(s' \\mid s,a) \\Big[ R(s,a,s') + \\gamma \\sum_{a'} \\pi(a' \\mid s')\\,Q^\\pi(s',a') \\Big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Relationship with state values**\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\sum_a \\pi(a \\mid s)\\,Q^\\pi(s,a)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Optimality (how Q drives control)**\n",
    "\n",
    "The **optimal action-value function** satisfies the Bellman optimality equation:\n",
    "\n",
    "$$\n",
    "Q^*(s,a) = \\sum_{s'} P(s' \\mid s,a) \\Big[ R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a') \\Big]\n",
    "$$\n",
    "\n",
    "And the **optimal policy** is:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a Q^*(s,a)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "üìå **When to use $Q$:**\n",
    "\n",
    "* **Model-free control** (Q-Learning, SARSA, DQN) to pick actions directly\n",
    "* **Action selection** without needing the model once $Q$ is learned\n",
    "* **Greedy/improvement steps** in DP or policy iteration\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ú®3.2 Computing Q-Values**\n",
    "\n",
    "**Core recipe (deterministic, mirrors the slide):**\n",
    "\n",
    "1. From $(s,a)$ get the **immediate reward** $r_a$ and **next state** $s'$.\n",
    "2. Combine with the **discounted value of $s'$:**\n",
    "\n",
    "$$\n",
    "Q(s,a) = r_a + \\gamma\\,V(s')\n",
    "$$\n",
    "\n",
    "*(If $s'$ is terminal, use $V(s') = 0$.)*\n",
    "\n",
    "---\n",
    "\n",
    "**Stochastic transitions:**\n",
    "If $(s,a)$ can lead to multiple $s'_i$ with probabilities $p_i$ and rewards $r_i$:\n",
    "\n",
    "$$\n",
    "Q(s,a) = \\sum_i p_i \\Big[ r_i + \\gamma\\,V(s'_i) \\Big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Directly with $Q$ (no $V$ table):**\n",
    "Iterate the Bellman expectation:\n",
    "\n",
    "$$\n",
    "Q_{k+1}(s,a) \\;\\leftarrow\\; \\sum_{s'} P(s' \\mid s,a)\\Big[ R(s,a,s') + \\gamma \\sum_{a'} \\pi(a' \\mid s')\\,Q_k(s',a') \\Big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Tiny code sketch (Gym-style envs):**\n",
    "\n",
    "```python\n",
    "def compute_q_value(env, s, a, V, gamma):\n",
    "    # env.unwrapped.P[s][a] -> list of (p, next_state, r, done)\n",
    "    outcomes = env.unwrapped.P[s][a]\n",
    "    return sum(\n",
    "        p * (r + (0 if done else gamma * V[next_state]))\n",
    "        for p, next_state, r, done in outcomes\n",
    "    )\n",
    "```\n",
    "\n",
    "*(For deterministic MDPs, this collapses to the single outcome, exactly like the slide.)*\n",
    "\n",
    "---\n",
    "\n",
    "**Example (Gridworld, Œ≥=1):**\n",
    "\n",
    "* $Q(4,\\text{down}) = -2 + 1 \\times 5 = 3$\n",
    "* $Q(4,\\text{left}) = -1 + 1 \\times 2 = 1$\n",
    "* $Q(4,\\text{up}) = -1 + 1 \\times 8 = 7$\n",
    "* $Q(4,\\text{right}) = -1 + 1 \\times 10 = 9$\n",
    "\n",
    "---\n",
    "\n",
    "**Terminal-state note:**\n",
    "Common conventions:\n",
    "\n",
    "* Set $Q(s_{\\text{terminal}},a) = 0$ for all $a$\n",
    "* Or skip computing it (return `None`), as shown in the slide\n",
    "\n",
    "Both work if applied consistently.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cheat-Sheet üìù**\n",
    "\n",
    "* **Intuition:**\n",
    "  ‚ÄúImmediate reward of action $a$ in state $s$ + discounted desirability of where that action leads.‚Äù\n",
    "* **From $V$ to $Q$:**\n",
    "  $Q(s,a) = r + \\gamma V(\\text{next})$\n",
    "* **From $Q$ to $V$:**\n",
    "  $V(s) = \\mathbb{E}_{a \\sim \\pi}[Q(s,a)]$\n",
    "* **Control:**\n",
    "  Act greedily w\\.r.t. $Q$; learn $Q$ via **Q-Learning, SARSA, DQN**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7fee05",
   "metadata": {},
   "source": [
    "**Implementation:**\n",
    "```python\n",
    "def compute_q_value(state, action, V):\n",
    "    if state == terminal_state:\n",
    "        return None\n",
    "    \n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * V[next_state]\n",
    "\n",
    "# Compute all Q-values\n",
    "Q = {(state, action): compute_q_value(state, action, V)\n",
    "     for state in range(num_states)\n",
    "     for action in range(num_actions)}\n",
    "```\n",
    "\n",
    "### 3.3 Policy Improvement Using Q-Values\n",
    "\n",
    "**Greedy Policy Improvement:** Select the action with the highest Q-value for each state.\n",
    "\n",
    "```python\n",
    "def improve_policy(Q, num_states, num_actions):\n",
    "    improved_policy = {}\n",
    "    \n",
    "    for state in range(num_states - 1):  # Exclude terminal state\n",
    "        # Find action with maximum Q-value\n",
    "        max_action = max(range(num_actions), \n",
    "                        key=lambda action: Q[(state, action)])\n",
    "        improved_policy[state] = max_action\n",
    "    \n",
    "    return improved_policy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20129335",
   "metadata": {},
   "source": [
    "## **‚úÖ4. Policy Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3dac54",
   "metadata": {},
   "source": [
    "### 4.1 Algorithm Overview\n",
    "\n",
    "**Definition:** Policy Iteration is an algorithm that finds the optimal policy by alternating between policy evaluation and policy improvement until convergence.\n",
    "\n",
    "**Steps:**\n",
    "1. **Initialize:** Start with an arbitrary policy œÄ‚ÇÄ\n",
    "2. **Policy Evaluation:** Compute V^œÄ for current policy\n",
    "3. **Policy Improvement:** Create new policy œÄ' by acting greedily with respect to V^œÄ\n",
    "4. **Check Convergence:** If œÄ' = œÄ, stop. Otherwise, set œÄ = œÄ' and go to step 2\n",
    "\n",
    "### 4.2 Implementation\n",
    "\n",
    "```python\n",
    "def policy_evaluation(policy, threshold=0.001):\n",
    "    \"\"\"Evaluate a given policy until convergence.\"\"\"\n",
    "    V = {state: 0 for state in range(num_states)}\n",
    "    \n",
    "    while True:\n",
    "        new_V = {state: 0 for state in range(num_states)}\n",
    "        \n",
    "        for state in range(num_states - 1):\n",
    "            if state != terminal_state:\n",
    "                action = policy[state]\n",
    "                _, next_state, reward, _ = env.P[state][action][0]\n",
    "                new_V[state] = reward + gamma * V[next_state]\n",
    "        \n",
    "        # Check convergence\n",
    "        if all(abs(new_V[s] - V[s]) < threshold for s in V):\n",
    "            break\n",
    "        V = new_V\n",
    "    \n",
    "    return V\n",
    "\n",
    "def policy_improvement(V):\n",
    "    \"\"\"Improve policy based on current value function.\"\"\"\n",
    "    improved_policy = {}\n",
    "    \n",
    "    for state in range(num_states - 1):\n",
    "        Q_values = []\n",
    "        for action in range(num_actions):\n",
    "            _, next_state, reward, _ = env.P[state][action][0]\n",
    "            q_val = reward + gamma * V[next_state]\n",
    "            Q_values.append(q_val)\n",
    "        \n",
    "        # Select best action\n",
    "        max_action = max(range(num_actions), key=lambda a: Q_values[a])\n",
    "        improved_policy[state] = max_action\n",
    "    \n",
    "    return improved_policy\n",
    "\n",
    "def policy_iteration():\n",
    "    \"\"\"Complete policy iteration algorithm.\"\"\"\n",
    "    # Initialize with arbitrary policy\n",
    "    policy = {0: 1, 1: 2, 2: 1, 3: 1, 4: 3, 5: 1, 6: 2, 7: 3}\n",
    "    \n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        V = policy_evaluation(policy)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        improved_policy = policy_improvement(V)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if improved_policy == policy:\n",
    "            break\n",
    "            \n",
    "        policy = improved_policy\n",
    "    \n",
    "    return policy, V\n",
    "```\n",
    "\n",
    "**Time Complexity:** O(|S|¬≤|A|) per iteration, where |S| is number of states and |A| is number of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126acaab",
   "metadata": {},
   "source": [
    "## **‚úÖ5. Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a0f74",
   "metadata": {},
   "source": [
    "### 5.1 Algorithm Overview\n",
    "\n",
    "**Definition:** Value Iteration combines policy evaluation and improvement in a single step. It directly computes the optimal value function and derives the policy from it.\n",
    "\n",
    "**Key Insight:** Instead of fully evaluating a policy, perform only one sweep of policy evaluation followed by policy improvement.\n",
    "\n",
    "**Bellman Optimality Equation:**\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + Œ≥V^*(s')]$$\n",
    "\n",
    "### 5.2 Algorithm Steps\n",
    "\n",
    "1. **Initialize:** V(s) = 0 for all states\n",
    "2. **Value Update:** For each state, compute the maximum expected value over all actions\n",
    "3. **Policy Extraction:** Choose actions that achieve the maximum value\n",
    "4. **Convergence Check:** Stop when value changes are below threshold\n",
    "\n",
    "### 5.3 Implementation\n",
    "\n",
    "```python\n",
    "def value_iteration(threshold=0.001):\n",
    "    \"\"\"Value iteration algorithm.\"\"\"\n",
    "    # Initialize\n",
    "    V = {state: 0 for state in range(num_states)}\n",
    "    policy = {state: 0 for state in range(num_states - 1)}\n",
    "    \n",
    "    while True:\n",
    "        new_V = {state: 0 for state in range(num_states)}\n",
    "        \n",
    "        for state in range(num_states - 1):\n",
    "            if state != terminal_state:\n",
    "                # Compute Q-values for all actions\n",
    "                Q_values = []\n",
    "                for action in range(num_actions):\n",
    "                    _, next_state, reward, _ = env.P[state][action][0]\n",
    "                    q_val = reward + gamma * V[next_state]\n",
    "                    Q_values.append(q_val)\n",
    "                \n",
    "                # Take maximum\n",
    "                max_q_value = max(Q_values)\n",
    "                max_action = max(range(num_actions), key=lambda a: Q_values[a])\n",
    "                \n",
    "                new_V[state] = max_q_value\n",
    "                policy[state] = max_action\n",
    "        \n",
    "        # Check convergence\n",
    "        if all(abs(new_V[state] - V[state]) < threshold for state in V):\n",
    "            break\n",
    "            \n",
    "        V = new_V\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "def get_max_action_and_value(state, V):\n",
    "    \"\"\"Helper function to get optimal action and value for a state.\"\"\"\n",
    "    Q_values = []\n",
    "    for action in range(num_actions):\n",
    "        _, next_state, reward, _ = env.P[state][action][0]\n",
    "        q_val = reward + gamma * V[next_state]\n",
    "        Q_values.append(q_val)\n",
    "    \n",
    "    max_action = max(range(num_actions), key=lambda a: Q_values[a])\n",
    "    max_q_value = Q_values[max_action]\n",
    "    \n",
    "    return max_action, max_q_value\n",
    "```\n",
    "\n",
    "**Time Complexity:** O(|S|¬≤|A|) per iteration, typically converges faster than Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0c597",
   "metadata": {},
   "source": [
    "## **‚úÖ6. Comparison: Policy Iteration vs Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e30c1",
   "metadata": {},
   "source": [
    "| Aspect | Policy Iteration | Value Iteration |\n",
    "|--------|------------------|-----------------|\n",
    "| **Approach** | Separate evaluation and improvement | Combined evaluation and improvement |\n",
    "| **Convergence** | Finite number of iterations | Asymptotic convergence |\n",
    "| **Per Iteration Cost** | Higher (full policy evaluation) | Lower (single value update) |\n",
    "| **Total Iterations** | Fewer iterations needed | More iterations needed |\n",
    "| **Memory** | Stores explicit policy | Policy derived from values |\n",
    "| **Practical Use** | Better for small state spaces | Better for large state spaces |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df943b8",
   "metadata": {},
   "source": [
    "## **‚úÖ7. Key Formulas Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d02d7e",
   "metadata": {},
   "source": [
    "### 7.1 Value Functions\n",
    "- **State Value:** $V^œÄ(s) = \\mathbb{E}[G_t | S_t = s, œÄ]$\n",
    "- **Action Value:** $Q^œÄ(s,a) = \\mathbb{E}[G_t | S_t = s, A_t = a, œÄ]$\n",
    "- **Return:** $G_t = \\sum_{k=0}^{\\infty} Œ≥^k R_{t+k+1}$\n",
    "\n",
    "### 7.2 Bellman Equations\n",
    "- **State Value Bellman:** $V^œÄ(s) = \\sum_a œÄ(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + Œ≥V^œÄ(s')]$\n",
    "- **Action Value Bellman:** $Q^œÄ(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + Œ≥\\sum_{a'} œÄ(a'|s')Q^œÄ(s',a')]$\n",
    "- **Optimality Bellman:** $V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + Œ≥V^*(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7772b0",
   "metadata": {},
   "source": [
    "## **‚úÖ8. Practical Tips and Best Practices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5513d6",
   "metadata": {},
   "source": [
    "### 8.1 Implementation Considerations\n",
    "- **Convergence Threshold:** Choose appropriate threshold (e.g., 0.001) based on precision needs\n",
    "- **Discount Factor:** Œ≥ close to 1 emphasizes future rewards; closer to 0 emphasizes immediate rewards\n",
    "- **Terminal States:** Always handle terminal states separately (return 0 or fixed value)\n",
    "\n",
    "### 8.2 Common Pitfalls\n",
    "- **Infinite Loops:** Ensure proper convergence checks in iterative algorithms\n",
    "- **State Indexing:** Be careful with state numbering and terminal state handling\n",
    "- **Action Spaces:** Verify action space size matches expected number of actions\n",
    "\n",
    "### 8.3 Extensions and Advanced Topics\n",
    "- **Stochastic Policies:** Extend to probabilistic action selection\n",
    "- **Function Approximation:** Use neural networks for large state spaces\n",
    "- **Model-Free Methods:** Q-Learning and SARSA for unknown environments\n",
    "- **Policy Gradient Methods:** Direct policy optimization without value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb044bc",
   "metadata": {},
   "source": [
    "## **‚úÖ9. Code Examples Repository**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bab618",
   "metadata": {},
   "source": [
    "### 9.1 Complete Grid World Example\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class GridWorldMDP:\n",
    "    def __init__(self, env_name='FrozenLake-v1'):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.num_states = self.env.observation_space.n\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.gamma = 1.0\n",
    "        self.terminal_state = self.num_states - 1\n",
    "    \n",
    "    def policy_iteration(self, threshold=0.001):\n",
    "        \"\"\"Complete policy iteration implementation.\"\"\"\n",
    "        policy = {i: 0 for i in range(self.num_states - 1)}\n",
    "        \n",
    "        while True:\n",
    "            V = self.policy_evaluation(policy, threshold)\n",
    "            new_policy = self.policy_improvement(V)\n",
    "            \n",
    "            if new_policy == policy:\n",
    "                break\n",
    "            policy = new_policy\n",
    "        \n",
    "        return policy, V\n",
    "    \n",
    "    def value_iteration(self, threshold=0.001):\n",
    "        \"\"\"Complete value iteration implementation.\"\"\"\n",
    "        V = {s: 0 for s in range(self.num_states)}\n",
    "        \n",
    "        while True:\n",
    "            new_V = V.copy()\n",
    "            for state in range(self.num_states - 1):\n",
    "                if state != self.terminal_state:\n",
    "                    values = []\n",
    "                    for action in range(self.num_actions):\n",
    "                        transitions = self.env.unwrapped.P[state][action]\n",
    "                        value = sum(prob * (reward + self.gamma * V[next_state])\n",
    "                                  for prob, next_state, reward, _ in transitions)\n",
    "                        values.append(value)\n",
    "                    new_V[state] = max(values)\n",
    "            \n",
    "            if all(abs(new_V[s] - V[s]) < threshold for s in V):\n",
    "                break\n",
    "            V = new_V\n",
    "        \n",
    "        # Extract policy\n",
    "        policy = {}\n",
    "        for state in range(self.num_states - 1):\n",
    "            if state != self.terminal_state:\n",
    "                values = []\n",
    "                for action in range(self.num_actions):\n",
    "                    transitions = self.env.unwrapped.P[state][action]\n",
    "                    value = sum(prob * (reward + self.gamma * V[next_state])\n",
    "                              for prob, next_state, reward, _ in transitions)\n",
    "                    values.append(value)\n",
    "                policy[state] = np.argmax(values)\n",
    "        \n",
    "        return policy, V\n",
    "\n",
    "# Usage\n",
    "mdp = GridWorldMDP()\n",
    "optimal_policy, optimal_values = mdp.value_iteration()\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"Optimal Values:\", optimal_values)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
