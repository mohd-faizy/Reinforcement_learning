{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c04416",
   "metadata": {},
   "source": [
    "# **‚≠êModel Based Learning‚≠ê**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dccedf",
   "metadata": {},
   "source": [
    "# Table of Contents - Model Based Learning\n",
    "\n",
    "- [1. Markov Decision Processes (MDPs)](#1-markov-decision-processes-mdps)\n",
    "  - [1.1 What is an MDP?](#11-what-is-an-mdp)\n",
    "  - [1.2 The Markov Property](#12-the-markov-property)\n",
    "  - [1.3 Frozen Lake Environment](#13-frozen-lake-environment)\n",
    "  - [1.4 CliffWalking Environment](#14-cliffwalking-environment)\n",
    "  \n",
    "- [2. State-value and Action-Value Functions](#2-state-value-and-action-value-functions)\n",
    "  - [2.1 State-Value Functions (V^œÄ)](#21-state-value-functions-$V^œÄ$)\n",
    "  - [2.2 Action-Value Functions (Q^œÄ)](#22-action-value-functions-$Q^œÄ$)\n",
    "  \n",
    "- [3. Policy & Value Iteration](#3-policy--value-iteration)\n",
    "  - [3.1 Overview](#31-overview)\n",
    "  - [3.2 Policy Iteration](#32-policy-iteration)\n",
    "  - [3.3 Value Iteration](#33-value-iteration)\n",
    "  - [3.4 Comparison: Policy vs Value Iteration](#34-comparison-policy-vs-value-iteration)\n",
    "  \n",
    "- [4. Summary](#4-summary)\n",
    "  - [4.1 Value Functions](#41-value-functions)\n",
    "  - [4.2 Bellman Equations](#42-bellman-equations)\n",
    "  - [4.3 Practical Tips and Best Practices](#43-practical-tips-and-best-practices)\n",
    "  - [4.4 Code Examples Repository](#44-code-examples-repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b4d75",
   "metadata": {},
   "source": [
    "# **‚úÖ1. Markov Decision Processes (MDPs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6165d",
   "metadata": {},
   "source": [
    "## 1.1 What is an MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ef0c2",
   "metadata": {},
   "source": [
    ">**Definition:** A Markov Decision Process (MDP) is a mathematical framework used to model decision-making in environments where outcomes are partly random and partly under the control of a decision-maker (agent). It's foundational in reinforcement learning and dynamic programming.\n",
    "\n",
    "**Purpose:** MDPs provide a formal way to describe environments where outcomes are partly random and partly under the control of a decision maker (agent).\n",
    "\n",
    "**Key Components of an MDP:**\n",
    "- **States (S):** All possible situations the agent can be in\n",
    "- **Actions (A):** All possible moves the agent can make\n",
    "- **Transition Probabilities (P):** Likelihood of moving from one state to another given an action\n",
    "- **Rewards (R):** Immediate feedback received after taking an action\n",
    "- **Discount Factor (Œ≥):** Weight given to future rewards (0 ‚â§ Œ≥ ‚â§ 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ba460",
   "metadata": {},
   "source": [
    "## 1.2 The Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c1c33",
   "metadata": {},
   "source": [
    "**Definition:** The Markov Property states that the future state depends only on the `current state` and `action`, **NOT** on the entire history of past states and actions.\n",
    "\n",
    "- **Mathematical Expression:**\n",
    "$$P(S_{t+1} = s' | S_t = s, A_t = a, S_{t-1}, A_{t-1}, ..., S_0, A_0) = P(S_{t+1} = s' | S_t = s, A_t = a)$$\n",
    "\n",
    "- **Intuitive Explanation:** The current state contains all the information needed to make optimal decisions about the future.\n",
    "\n",
    "- **Goal of an Agent in MDP**\n",
    "  - The agent‚Äôs objective is to find a policy `ùúã(ùëé‚à£ùë†)` that maximizes the expected cumulative reward over time. This is often formalized using:\n",
    "      - **Value functions**: Estimate how good it is to be in a state or take an action.\n",
    "      - **Policy iteration** and **value iteration**: Algorithms to compute optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e200e",
   "metadata": {},
   "source": [
    "## 1.3 Frozen Lake Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d7171",
   "metadata": {},
   "source": [
    "Imagine a 4x4 grid that represents a frozen lake. There are three types of tiles:\n",
    "\n",
    "1.  **`S`** : The starting point (safe).\n",
    "2.  **`F`** : Frozen surface (safe, you can walk on it).\n",
    "3.  **`H`** : A hole in the ice (dangerous, you fall in and the episode ends).\n",
    "4.  **`G`** : The goal (where you receive a reward).\n",
    "\n",
    "A simple layout might look like this:\n",
    "```\n",
    "S  F  F  F\n",
    "F  H  F  H\n",
    "F  F  F  H\n",
    "H  F  F  G\n",
    "```\n",
    "You are an agent (a person trying to cross the lake). Your goal is to find a path from `S` to `G` without falling into a hole `H`.\n",
    "\n",
    "\n",
    "### Mapping the Problem to a Markov Decision Process (MDP)\n",
    "\n",
    "An MDP is defined by a 5-tuple `(S, A, P, R, Œ≥)`:\n",
    "*   **S**: Set of states\n",
    "*   **A**: Set of actions\n",
    "*   **P**: Transition probabilities `P(s' | s, a)`\n",
    "*   **R**: Reward function `R(s, a, s')`\n",
    "*   **Œ≥**: Discount factor (between 0 and 1)\n",
    "\n",
    "Let's break down the Frozen Lake problem into these components.\n",
    "\n",
    "#### 1. `States (S)`\n",
    "Each tile (cell) in the grid is a state. We can represent them by their coordinates.\n",
    "*   **S**: `{(0,0), (0,1), (0,2), (0,3), (1,0), ..., (3,3)}`\n",
    "*   The **terminal states** are the holes `H` and the goal `G`. Once you enter one, the episode is over. For example, `(1,1)` is a hole and `(3,3)` is the goal.\n",
    "\n",
    "#### 2. `Actions (A)`\n",
    "The actions are the possible moves the agent can take from any state (if the move is possible).\n",
    "*   **A**: `{UP, DOWN, LEFT, RIGHT}`\n",
    "\n",
    "#### 3. `Transition Probabilities (P)`\n",
    "This is the core of the \"Markov\" property. The outcome of an action is **stochastic** (non-deterministic). This mimics the slippery nature of ice.\n",
    "\n",
    "*   **Intended Action**: 33.3% chance\n",
    "*   **Slipping Left**: 33.3% chance\n",
    "*   **Slipping Right**: 33.3% chance\n",
    "\n",
    "**Example:** From state `(0,1)` (a frozen tile), if the agent intends to go `DOWN`:\n",
    "*   With ~33% probability, it successfully moves `DOWN` to `(1,1)`.\n",
    "*   With ~33% probability, it slips and moves `LEFT` to `(0,0)`.\n",
    "*   With ~33% probability, it slips and moves `RIGHT` to `(0,2)`.\n",
    "\n",
    "If a move would take the agent into a wall (e.g., moving `LEFT` from `(0,0)`), the agent simply stays in its current state. The probability mass for that invalid move is added to the probability of remaining in the current state.\n",
    "\n",
    "#### 4. `Reward Function (R)`\n",
    "The reward defines the goal of the agent. We give a reward only when the agent reaches a meaningful state.\n",
    "\n",
    "*   **Reaching the Goal (G):** `R = +1`\n",
    "*   **Falling into a Hole (H):** `R = 0` (Some versions use a small negative reward like `-1` to penalize failure)\n",
    "*   **Stepping on any other Frozen (F) tile:** `R = 0`\n",
    "\n",
    "The agent gets this reward *upon entering* the new state `s'`.\n",
    "\n",
    "**Example:**\n",
    "*   `R((2,3), DOWN, (3,3)) = +1` (Moving into the goal from above)\n",
    "*   `R((1,0), RIGHT, (1,1)) = 0` (Moving into a hole)\n",
    "*   `R((0,0), RIGHT, (0,1)) = 0` (Moving onto a frozen tile)\n",
    "\n",
    "#### 5. `Discount Factor (Œ≥)`\n",
    "This determines how much the agent cares about `future rewards` vs. `immediate rewards`.\n",
    "*   Let's choose **Œ≥ = 0.9** for this example. This means the agent strongly prefers reaching the goal quickly but still values eventually reaching it over never reaching it at all.\n",
    "\n",
    "\n",
    "\n",
    "### **How an Episode Unrolls**\n",
    "\n",
    "Let's simulate a few steps of a possible episode:\n",
    "\n",
    "1.  **Time t=0**: State `s‚ÇÄ = (0,0)` (Start).\n",
    "2.  **Action a‚ÇÄ**: The agent chooses `RIGHT` (intending to go to `(0,1)`).\n",
    "3.  **Transition**: Due to slippiness, it actually slips and moves `DOWN` to `s‚ÇÅ = (1,0)`. Reward `r‚ÇÄ = 0`.\n",
    "4.  **Time t=1**: State `s‚ÇÅ = (1,0)`.\n",
    "5.  **Action a‚ÇÅ**: The agent chooses `RIGHT` again (intending to go to `(1,1)` - a hole!).\n",
    "6.  **Transition**: It successfully executes the action and moves into the hole at `s‚ÇÇ = (1,1)`. This is a terminal state.\n",
    "7.  **Reward r‚ÇÅ**: The agent receives `r‚ÇÅ = 0` for entering a hole. The episode ends. The total return for this episode is `0`.\n",
    "\n",
    "**A successful episode** would involve the agent navigating the slippery ice, potentially getting lucky with slips, and eventually landing on `(3,3)` to collect a reward of `+1`.\n",
    "\n",
    "### The \"Solution\" to the MDP\n",
    "\n",
    "The goal of solving an MDP is to find a **policy (œÄ)**, which is a strategy that tells the agent what action to take in every state (`œÄ(s) -> a`).\n",
    "\n",
    "The optimal policy `œÄ*` is the one that maximizes the expected cumulative discounted reward (the **return**). In this case, it's the policy that has the highest chance of getting the agent to the goal without falling in a hole.\n",
    "\n",
    "For a small grid like this, we can compute this optimal policy using algorithms like **Value Iteration** or **Policy Iteration**. The result would be a map showing the best action for every tile:\n",
    "\n",
    "\n",
    "### **Frozen Lake**\n",
    "\n",
    "**Environment Description:** An agent must navigate across a frozen lake to reach a goal while avoiding holes.\n",
    "\n",
    "**Components:**\n",
    "- **States:** 16 positions (4√ó4 grid) numbered 0-15\n",
    "- **Actions:** 4 possible moves (0: left, 1: down, 2: right, 3: up)\n",
    "- **Terminal States:** Goal state (rewards +1) and hole states (episode ends) ~ 6\n",
    "- **Transition Probabilities:** Actions don't always lead to expected outcomes due to slippery ice\n",
    "\n",
    "![frozen-lake-png](_img\\frozen-lake.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bea402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n",
      "Number of actions: 4\n",
      "Number of states: 16\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "# Check state and action spaces\n",
    "print(env.action_space)          \n",
    "print(env.observation_space)    \n",
    "\n",
    "print(\"Number of actions:\", env.action_space.n)      \n",
    "print(\"Number of states:\", env.observation_space.n)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96144e0",
   "metadata": {},
   "source": [
    "**Transition Probabilities and Rewards**\n",
    "\n",
    "**Accessing Transition Information:**\n",
    "```python\n",
    "# env.unwrapped.P[state][action] returns:\n",
    "# [(probability_1, next_state_1, reward_1, is_terminal_1),\n",
    "#  (probability_2, next_state_2, reward_2, is_terminal_2), ...]\n",
    "\n",
    "state = 6\n",
    "action = 0  # left\n",
    "print(env.unwrapped.P[state][action])\n",
    "# Output: [(0.333, 2, 0.0, False), (0.333, 5, 0.0, True), (0.333, 10, 0.0, False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e44c9",
   "metadata": {},
   "source": [
    "## 1.4 CliffWalking Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aaa227",
   "metadata": {},
   "source": [
    "- The Cliff Walking environment involves an agent crossing a grid world from start to goal while avoiding falling off a cliff.\n",
    "- If the player moves to a cliff location it returns to the start location.\n",
    "- The player makes moves until they reach the goal, which ends the episode.\n",
    "- Your task is to explore the state and action spaces of this environment.\n",
    "\n",
    "![cliff-walking-gif](_img\\cliff_walking.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a46672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "Number of states: 48\n",
      "[(1.0, np.int64(23), -1, False)]\n",
      "Action: 0 | Probability: 1.0, Next State: 23, Reward: -1, Done: False\n",
      "[(1.0, np.int64(35), -1, False)]\n",
      "Action: 1 | Probability: 1.0, Next State: 35, Reward: -1, Done: False\n",
      "[(1.0, np.int64(47), -1, True)]\n",
      "Action: 2 | Probability: 1.0, Next State: 47, Reward: -1, Done: True\n",
      "[(1.0, np.int64(34), -1, False)]\n",
      "Action: 3 | Probability: 1.0, Next State: 34, Reward: -1, Done: False\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym   \n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Environment Setup\n",
    "# ==============================\n",
    "# Create the CliffWalking environment.\n",
    "# \"CliffWalking-v1\" is a classic control problem from reinforcement learning.\n",
    "# \"render_mode='rgb_array'\" means the environment won't open a window;\n",
    "# instead, it keeps the visual output as an image array (useful for debugging or rendering later).\n",
    "env = gym.make('CliffWalking-v1', render_mode='rgb_array')\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Action and State Spaces\n",
    "# ==============================\n",
    "# Number of possible actions the agent can take (Up, Right, Down, Left = 4).\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Number of possible states in the gridworld (4 rows √ó 12 columns = 48).\n",
    "num_states = env.observation_space.n\n",
    "\n",
    "print(\"Number of actions:\", num_actions)\n",
    "print(\"Number of states:\", num_states)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Exploring Transitions\n",
    "# ==============================\n",
    "# Each state has a set of transitions, depending on the chosen action.\n",
    "# Let's pick a specific state (for example, 35) and explore what happens when we try different actions.\n",
    "state = 35\n",
    "\n",
    "# Loop through all possible actions from this state\n",
    "for action in range(num_actions):\n",
    "    # The environment has an internal dictionary \"P\" that stores transitions.\n",
    "    # P[state][action] gives a list of possible outcomes when taking `action` in `state`.\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    print(transitions)\n",
    "\n",
    "    # Each transition has the format: (probability, next_state, reward, done)\n",
    "    # -> probability: chance of this outcome (usually 1.0 for deterministic envs like CliffWalking)\n",
    "    # -> next_state: the state you land in after the action\n",
    "    # -> reward: the reward received for this action\n",
    "    # -> done: whether the episode ends after this transition\n",
    "    for transition in transitions:\n",
    "        probability, next_state, reward, done = transition\n",
    "        print(f\"Action: {action} | Probability: {probability}, Next State: {next_state}, Reward: {reward}, Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828e878",
   "metadata": {},
   "source": [
    "# **‚úÖ2. State-value and Action-Value Funtion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c01e753",
   "metadata": {},
   "source": [
    "## 2.1 State-Value Functions ($V^\\pi$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6222c36",
   "metadata": {},
   "source": [
    "### üîπ **State-Value Function: $V^\\pi(s)$**\n",
    "\n",
    "* **Definition:**\n",
    "  $V^\\pi(s)$ is the *expected total reward (return)* you‚Äôll get if you **start in state $s$** and then **keep following policy $\\pi$**.\n",
    "\n",
    "* **Think of it as:**\n",
    "  ‚ÄúHow good is it to just **be in this state**, assuming I behave according to policy $\\pi$ from now on?‚Äù\n",
    "\n",
    "* **Use case:**\n",
    "\n",
    "  * Helps evaluate *how desirable a state is overall*.\n",
    "  * Important in **policy evaluation** (checking how good a given policy is).\n",
    "\n",
    "* **Example:**\n",
    "  Imagine a gridworld where:\n",
    "\n",
    "  * From state $s$, following policy $\\pi$ usually leads you safely to the goal with +10 reward.\n",
    "  * But sometimes you bump into walls and lose -1.\n",
    "  * The average of all these future outcomes = **state value** $V^\\pi(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c740b4",
   "metadata": {},
   "source": [
    "### **‚ú®1. Policies**\n",
    "\n",
    "**Definition:** A policy $\\pi$ is a strategy that defines which action to take in each state to maximize the expected cumulative reward (return).\n",
    "\n",
    "**Types:**\n",
    "- **Deterministic Policy:** Always chooses the same action for a given state\n",
    "- **Stochastic Policy:** Chooses actions according to a probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e187a4",
   "metadata": {},
   "source": [
    "### **‚ú®2. State-Value Functions**\n",
    "\n",
    "**Definition:**\n",
    "The **state-value function** $V(s)$ estimates how good it is to be in a given state $s$.\n",
    "It represents the **expected return (sum of discounted future rewards)** when starting in state $s$ and following a given policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "#### **a. Mathematical Expression (Expanded Form):**\n",
    "\n",
    "$$\n",
    "V(s) = r_{s+1} + \\gamma r_{s+2} + \\gamma^2 r_{s+3} + \\cdots + \\gamma^{n-1} r_{s+n}\n",
    "$$\n",
    "\n",
    "‚úÖ **Interpretation:**\n",
    "\n",
    "* $r_{s+1}$: Immediate reward after leaving state $s$\n",
    "* $\\gamma r_{s+2}$: Next reward, discounted by factor $\\gamma$\n",
    "* $\\gamma^2 r_{s+3}$: Reward two steps later, discounted further\n",
    "* $\\cdots$ Continues infinitely\n",
    "* $\\gamma \\in [0,1]$: Discount factor that balances **present vs. future rewards**\n",
    "\n",
    "üìå **When to use:**\n",
    "\n",
    "* **Conceptual / Theoretical explanation** of value functions.\n",
    "* When introducing RL to beginners ‚Üí easy to show ‚Äúwhy future rewards are discounted.‚Äù\n",
    "* To **manually calculate returns** in very short episodes (e.g., toy problems like a 3-step grid world).\n",
    "* Useful in **Monte Carlo methods**, where we sample entire episodes and directly compute the return.\n",
    "* Not practical for real-world problems because we can‚Äôt compute infinite sums.\n",
    "\n",
    "---\n",
    "\n",
    "#### **b. Bellman Equation (Recursive Form):**\n",
    "\n",
    "$$\n",
    "V(s) = r_{s+1} + \\gamma V(s+1)\n",
    "$$\n",
    "\n",
    "‚úÖ **Interpretation:**\n",
    "\n",
    "* $r_{s+1}$: Reward right after leaving state $s$\n",
    "* $\\gamma V(s+1)$: The discounted *value of the next state*\n",
    "* Turns the infinite sum into a **recursive relationship**\n",
    "\n",
    "üìå **When to use:**\n",
    "\n",
    "* **Dynamic Programming (DP):**\n",
    "  * `Value Iteration`\n",
    "  * `Policy Iteration`\n",
    "  * `Policy Evaluation`\n",
    "* Great for **deterministic environments**, where the next state is known with certainty.\n",
    "* Useful in **algorithm derivations**, since recursion makes equations easier to solve.\n",
    "* Helps in **bootstrapping methods** like Temporal-Difference (TD) learning, where we approximate returns by one-step lookahead instead of waiting for the whole episode.\n",
    "* Key in proving convergence of RL algorithms.\n",
    "\n",
    "üí° Example: In **Gridworld**, if moving right always gives +1 reward, then\n",
    "\n",
    "$$\n",
    "V(s) = 1 + \\gamma V(s')\n",
    "$$\n",
    "\n",
    "is easier to compute recursively instead of expanding all rewards.\n",
    "\n",
    "---\n",
    "\n",
    "#### **c. General Bellman Equation with Policy (Full Form):**\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\Big[ R(s,a,s') + \\gamma V^\\pi(s') \\Big]\n",
    "$$\n",
    "\n",
    "‚úÖ **Interpretation:**\n",
    "\n",
    "* $\\pi(a|s)$: Policy ‚Üí probability of taking action $a$ in state $s$\n",
    "* $P(s'|s,a)$: Transition probability ‚Üí chance of landing in state $s'$\n",
    "* $R(s,a,s')$: Expected reward for going from $s$ to $s'$ with action $a$\n",
    "* $V^\\pi(s')$: Value of the next state under policy $\\pi$\n",
    "* Captures **stochasticity in both actions and environment dynamics**\n",
    "\n",
    "üìå **When to use:**\n",
    "\n",
    "* In **real-world MDPs** where:\n",
    "\n",
    "  * Multiple actions are possible\n",
    "  * Transitions are probabilistic (not deterministic)\n",
    "* Central to **Reinforcement Learning algorithms**:\n",
    "\n",
    "  * **Policy Evaluation** (compute value of a given policy)\n",
    "  * **Policy Iteration** (improve policies step by step)\n",
    "  * **Actor-Critic methods** (where the critic estimates $V^\\pi$)\n",
    "* Needed when using **model-based RL**, since it explicitly uses transition probabilities $P(s'|s,a)$.\n",
    "* Forms the foundation for **policy optimization methods** like Policy Gradient, A2C, PPO (where value functions are approximated).\n",
    "\n",
    "üí° Example: In a **robot navigation task**, if the robot in state $s$ has a 70% chance to move forward and a 30% chance to slip sideways, this equation correctly handles those probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "‚ú® **Usage:**\n",
    "\n",
    "* **Expanded formula** ‚Üí Good for intuition, teaching, and small toy problems. Used in Monte Carlo methods.\n",
    "* **Recursive Bellman formula** ‚Üí Practical for computation, DP, and TD-learning. Efficient because it uses recursion instead of full sum.\n",
    "* **General Bellman with policy** ‚Üí Realistic MDPs with stochastic transitions and multiple actions. Core of almost all RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bbb28e",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create the environment\n",
    "env = gym.make('MyGridWorld', render_mode='rgb_array')\n",
    "state, info = env.reset()\n",
    "\n",
    "# Example Policy (Grid World)\n",
    "# 0: left, 1: down, 2: right, 3: up\n",
    "\n",
    "policy = {\n",
    "    0: 1,  # In state 0, go down\n",
    "    1: 2,  # In state 1, go right\n",
    "    2: 1,  # In state 2, go down\n",
    "    3: 1,  # In state 3, go down\n",
    "    4: 3,  # In state 4, go up\n",
    "    5: 1,  # In state 5, go down\n",
    "    6: 2,  # In state 6, go right\n",
    "    7: 3   # In state 7, go up\n",
    "}\n",
    "\n",
    "# Policy Execution\n",
    "terminated = False\n",
    "while not terminated:\n",
    "  # Select action based on policy \n",
    "  action = policy[state]\n",
    "  state, reward, terminated, truncated, info = env.step(action)\n",
    "  # Render the environment\n",
    "  render()\n",
    "\n",
    "# State-Value Functions\n",
    "def compute_state_value(state, policy):\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state, policy)\n",
    "\n",
    "# Compute all state values\n",
    "gamma = 1\n",
    "terminal_state = 8\n",
    "state_values = {state: compute_state_value(state, policy) for state in range(num_states)}\n",
    "print(state_values)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4467d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CUSTOM 3x3 GRIDWORLD WITH POLICY AND STATE-VALUES ===\n",
      "\n",
      "Custom 3x3 GridWorld Layout:\n",
      "S    \n",
      "  M M\n",
      "    D\n",
      "\n",
      "State-values: {0: 1, 1: 8, 2: 9, 3: 2, 4: 7, 5: 10, 6: 3, 7: 5, 8: 0}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CUSTOM 3x3 GRIDWORLD WITH POLICY AND STATE-VALUES ===\")\n",
    "print()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Custom 3x3 GridWorld Env\n",
    "# -------------------------------\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"ansi\"]}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shape = (3, 3)\n",
    "        self.observation_space = spaces.Discrete(9)\n",
    "        self.action_space = spaces.Discrete(4)  # 0:left,1:down,2:right,3:up\n",
    "        \n",
    "        # Define rewards\n",
    "        self.terminal_state = 8\n",
    "        self.rewards = {8: 10, 4: -2, 7: -2}\n",
    "        \n",
    "        # Precompute P like Gym\n",
    "        self.P = {s: {a: [] for a in range(4)} for s in range(9)}\n",
    "        for s in range(9):\n",
    "            for a in range(4):\n",
    "                ns = self._move(s, a)\n",
    "                r = self.rewards.get(ns, -1)\n",
    "                done = ns == self.terminal_state\n",
    "                self.P[s][a] = [(1.0, ns, r, done)]  # deterministic\n",
    "                \n",
    "    def _move(self, state, action):\n",
    "        if state == self.terminal_state:\n",
    "            return state\n",
    "        row, col = state // 3, state % 3\n",
    "        if action == 0:    # left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 1:  # down\n",
    "            row = min(2, row + 1)\n",
    "        elif action == 2:  # right\n",
    "            col = min(2, col + 1)\n",
    "        elif action == 3:  # up\n",
    "            row = max(0, row - 1)\n",
    "        return row * 3 + col\n",
    "    \n",
    "    def render(self, mode=\"ansi\"):\n",
    "        grid = np.full(self.shape, \" \")\n",
    "        grid[0,0] = \"S\"  # start\n",
    "        grid[2,2] = \"D\"  # diamond\n",
    "        grid[1,1] = grid[1,2] = \"M\"  # mountains\n",
    "        return \"\\n\".join([\" \".join(row) for row in grid])\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnv()\n",
    "num_states = env.observation_space.n\n",
    "gamma = 1\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define a deterministic policy\n",
    "# -------------------------------\n",
    "policy = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3,  # up\n",
    "    8: 0   # terminal\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Compute state values\n",
    "# -------------------------------\n",
    "def compute_state_value(state):\n",
    "    if state == env.terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "\n",
    "V = {s: compute_state_value(s) for s in range(num_states)}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Display results\n",
    "# -------------------------------\n",
    "print(\"Custom 3x3 GridWorld Layout:\")\n",
    "print(env.render())\n",
    "print()\n",
    "print(\"State-values:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c209f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy 1: {0: 'down', 1: 'right', 2: 'down', 3: 'down', 4: 'up', 5: 'down', 6: 'right', 7: 'up'}\n",
      "Policy 2: {0: 'right', 1: 'right', 2: 'down', 3: 'right', 4: 'right', 5: 'down', 6: 'right', 7: 'right'}\n",
      "\n",
      "2. STATE-VALUE FUNCTIONS\n",
      "========================\n",
      "V(s) = Expected return starting from state s following policy œÄ\n",
      "\n",
      "Computing state values...\n",
      "\n",
      "RESULTS:\n",
      "========\n",
      "State-values for Policy 1: {0: 1.0, 1: 8.0, 2: 9.0, 3: 2.0, 4: 7.0, 5: 10.0, 6: 3.0, 7: 5.0, 8: 0}\n",
      "State-values for Policy 2: {0: 7.0, 1: 8.0, 2: 9.0, 3: 7.0, 4: 9.0, 5: 10.0, 6: 8.0, 7: 10.0, 8: 0}\n",
      "\n",
      "EXAMPLE CALCULATION (Policy 1, State 2):\n",
      "========================================\n",
      "State 2 ‚Üí Action down ‚Üí State 5\n",
      "Reward: -1\n",
      "V(2) = -1 + 1.0 √ó V(5) = -1 + 1.0 √ó 10.0 = 9.0\n",
      "\n",
      "POLICY COMPARISON:\n",
      "==================\n",
      "Total value (Policy 1): 45.0\n",
      "Total value (Policy 2): 68.0\n",
      "Better policy: Policy 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# 1. POLICIES\n",
    "# ==========================\n",
    "\n",
    "# Actions: 0: left, 1: down, 2: right, 3: up\n",
    "policy1 = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3   # up\n",
    "}\n",
    "\n",
    "policy2 = {\n",
    "    0: 2,  # right\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 2,  # right\n",
    "    4: 2,  # right\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 2   # right\n",
    "}\n",
    "\n",
    "action_names = {0: 'left', 1: 'down', 2: 'right', 3: 'up'}\n",
    "\n",
    "print(\"Policy 1:\", {s: action_names[a] for s, a in policy1.items()})\n",
    "print(\"Policy 2:\", {s: action_names[a] for s, a in policy2.items()})\n",
    "print()\n",
    "\n",
    "# ==========================\n",
    "# 2. ENVIRONMENT MODEL (P)\n",
    "# ==========================\n",
    "gamma = 1.0          # Discount factor\n",
    "num_states = 9       # 3x3 grid\n",
    "terminal_state = 8   # Diamond\n",
    "\n",
    "# Build transition table: P[state][action] = [(prob, next_state, reward, done)]\n",
    "P = {s: {a: [] for a in range(4)} for s in range(num_states)}\n",
    "\n",
    "def move(state, action):\n",
    "    \"\"\"Return next state after taking an action.\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return state\n",
    "    \n",
    "    row, col = state // 3, state % 3\n",
    "    if action == 0:    # left\n",
    "        col = max(0, col - 1)\n",
    "    elif action == 1:  # down\n",
    "        row = min(2, row + 1)\n",
    "    elif action == 2:  # right\n",
    "        col = min(2, col + 1)\n",
    "    elif action == 3:  # up\n",
    "        row = max(0, row - 1)\n",
    "    return row * 3 + col\n",
    "\n",
    "def reward(next_state):\n",
    "    \"\"\"Return reward for landing in next_state.\"\"\"\n",
    "    if next_state == 8:   # Diamond\n",
    "        return 10\n",
    "    elif next_state in [4, 7]:  # Mountains\n",
    "        return -2\n",
    "    else:  # All other states\n",
    "        return -1\n",
    "\n",
    "# Fill transition table\n",
    "for s in range(num_states):\n",
    "    for a in range(4):\n",
    "        ns = move(s, a)\n",
    "        r = reward(ns)\n",
    "        done = (ns == terminal_state)\n",
    "        P[s][a] = [(1.0, ns, r, done)]   # deterministic env\n",
    "\n",
    "# ==========================\n",
    "# 3. STATE-VALUE FUNCTIONS\n",
    "# ==========================\n",
    "print(\"2. STATE-VALUE FUNCTIONS\")\n",
    "print(\"========================\")\n",
    "print(\"V(s) = Expected return starting from state s following policy œÄ\")\n",
    "print()\n",
    "\n",
    "def compute_state_value(state, policy):\n",
    "    \"\"\"Bellman expectation with env.P\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    transitions = P[state][action]\n",
    "    \n",
    "    value = 0\n",
    "    for prob, next_state, reward, _ in transitions:\n",
    "        value += prob * (reward + gamma * compute_state_value(next_state, policy))\n",
    "    return value\n",
    "\n",
    "# Calculate state values for both policies\n",
    "print(\"Computing state values...\")\n",
    "print()\n",
    "\n",
    "V1 = {s: compute_state_value(s, policy1) for s in range(num_states)}\n",
    "V2 = {s: compute_state_value(s, policy2) for s in range(num_states)}\n",
    "\n",
    "# ==========================\n",
    "# 4. RESULTS\n",
    "# ==========================\n",
    "print(\"RESULTS:\")\n",
    "print(\"========\")\n",
    "print(\"State-values for Policy 1:\", V1)\n",
    "print(\"State-values for Policy 2:\", V2)\n",
    "print()\n",
    "\n",
    "# Example calculation walkthrough\n",
    "print(\"EXAMPLE CALCULATION (Policy 1, State 2):\")\n",
    "print(\"========================================\")\n",
    "state = 2\n",
    "action = policy1[state]\n",
    "prob, next_state, reward, _ = P[state][action][0]\n",
    "print(f\"State 2 ‚Üí Action {action_names[action]} ‚Üí State {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"V(2) = {reward} + {gamma} √ó V({next_state}) = {reward} + {gamma} √ó {V1[next_state]} = {V1[2]}\")\n",
    "print()\n",
    "\n",
    "# Compare policies\n",
    "print(\"POLICY COMPARISON:\")\n",
    "print(\"==================\")\n",
    "total1 = sum(V1[s] for s in range(8))  # exclude terminal\n",
    "total2 = sum(V2[s] for s in range(8))\n",
    "print(f\"Total value (Policy 1): {total1}\")\n",
    "print(f\"Total value (Policy 2): {total2}\")\n",
    "print(f\"Better policy: Policy {'2' if total2 > total1 else '1'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa7797",
   "metadata": {},
   "source": [
    "## 2.2 Action-Value Functions ($Q^\\pi$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c2cbb",
   "metadata": {},
   "source": [
    "### üîπ **Action-Value Function: $Q^\\pi(s,a)$**\n",
    "\n",
    "* **Definition:**\n",
    "  $Q^\\pi(s,a)$ is the *expected total reward (return)* if you **start in state $s$**, **take action $a$ immediately**, and then **keep following policy $\\pi$**.\n",
    "\n",
    "* **Think of it as:**\n",
    "  ‚ÄúHow good is it to **choose this action right now** in this state, assuming I‚Äôll behave according to $\\pi$ afterward?‚Äù\n",
    "\n",
    "* **Use case:**\n",
    "\n",
    "  * More detailed than $V^\\pi$, because it separates **which action** you take in a state.\n",
    "  * Needed for **control** (improving or choosing better policies).\n",
    "\n",
    "* **Example:**\n",
    "  Same gridworld:\n",
    "\n",
    "  * In state $s$, you could move **up** (+5 reward) or **down** (-1 penalty).\n",
    "  * $V^\\pi(s)$ would average over what policy $\\pi$ usually does.\n",
    "  * $Q^\\pi(s,\\text{up})=+5$, $Q^\\pi(s,\\text{down})=-1$.\n",
    "  * So $Q$ tells you *which action is better in this state*, while $V$ just says *the overall goodness of the state under your policy*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7d2fe",
   "metadata": {},
   "source": [
    "### 1) **what $Q^\\pi(s,a)$ means**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\mathbb{E}_\\pi\\big[G_t \\mid S_t=s, A_t=a\\big]\n",
    "  $$\n",
    "* Symbols:\n",
    "\n",
    "  * $G_t$ = total return from time $t$: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots$\n",
    "  * $\\mathbb{E}_\\pi[\\cdot]$ = expectation when actions after time $t$ are chosen according to policy $\\pi$.\n",
    "  * Condition $\\mid S_t=s, A_t=a$ means: **we start in state $s$ and take action $a$ now**; randomness afterward is from environment transitions and the policy.\n",
    "* Intuition: average (over possible randomness) of **immediate reward + discounted future rewards** after taking $a$ in $s$.\n",
    "* Tiny numeric example:\n",
    "\n",
    "  * Suppose $\\gamma=0.9$, immediate reward $r=2$, and expected future return $V^\\pi(s')=5$.\n",
    "  * Then $Q^\\pi(s,a)=2 + 0.9\\times 5 = 6.5$.\n",
    "\n",
    "### 2) **Break the return into immediate + future (derivation)**\n",
    "\n",
    "* Start from definition:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\mathbb{E}\\big[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s, A_t=a\\big]\n",
    "  $$\n",
    "* Because expectation is linear:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\mathbb{E}[R_{t+1}\\mid s,a] \\;+\\; \\gamma \\,\\mathbb{E}[G_{t+1}\\mid s,a]\n",
    "  $$\n",
    "* Interpretations:\n",
    "\n",
    "  * $\\mathbb{E}[R_{t+1}\\mid s,a]$ = expected immediate reward after doing $a$ in $s$.\n",
    "  * $\\mathbb{E}[G_{t+1}\\mid s,a]$ = expected future return from the next time step onward.\n",
    "\n",
    "### 3) **Deterministic one-step Bellman form**\n",
    "\n",
    "* Formula (deterministic next state $s'$ and deterministic immediate reward $r_a$):\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = r_a + \\gamma V^\\pi(s')\n",
    "  $$\n",
    "* Why this follows:\n",
    "\n",
    "  * If taking $a$ from $s$ always lands in $s'$ and gives reward $r_a$, then $\\mathbb{E}[R_{t+1}\\mid s,a]=r_a$ and $\\mathbb{E}[G_{t+1}\\mid s,a]=V^\\pi(s')$.\n",
    "* Intuition: **immediate reward** + **discounted value of the known next state**.\n",
    "\n",
    "### 4) **Stochastic transitions ‚Äî expectation over next states**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\sum_{s'} P(s'\\mid s,a)\\,\\big[ R(s,a,s') + \\gamma V^\\pi(s')\\big]\n",
    "  $$\n",
    "* Explanation:\n",
    "\n",
    "  * If taking $a$ can lead to several possible next states $s'$, each with probability $P(s'|s,a)$, you average the immediate reward + future value for each possible $s'$.\n",
    "* Numeric example (showing every arithmetic step):\n",
    "\n",
    "  * Let two next states $s_1,s_2$ with probabilities $0.7$ and $0.3$.\n",
    "  * Rewards: $R(s,a,s_1)=1$, $R(s,a,s_2)=2$.\n",
    "  * Values: $V^\\pi(s_1)=3$, $V^\\pi(s_2)=4$.\n",
    "  * $\\gamma=0.9$.\n",
    "  * Compute for $s_1$: inner = $1 + 0.9\\times 3 = 2.7$.\n",
    "    * Multiply by prob: $0.7\\times 3.7 = 2.59$.\n",
    "  \n",
    "  * Compute for $s_2$: inner = $2 + 0.9\\times 4 = 5.6$.\n",
    "    * Multiply by prob: $0.3\\times 5.6 = 1.68$.\n",
    "    * So $Q^\\pi(s,a) = 2.59 + 1.68 = 4.27$.\n",
    "\n",
    "### 5) **Bellman expectation in terms of $Q$ (no $V$ needed)**\n",
    "\n",
    "* Start: $V^\\pi(s') = \\sum_{a'} \\pi(a'\\mid s')\\,Q^\\pi(s',a')$.\n",
    "* Substitute into previous expression:\n",
    "\n",
    "  $$\n",
    "  Q^\\pi(s,a) = \\sum_{s'} P(s'\\mid s,a)\\Big[ R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'\\mid s')\\,Q^\\pi(s',a')\\Big].\n",
    "  $$\n",
    "* Why useful:\n",
    "\n",
    "  * This is a **self-consistent equation** for $Q^\\pi$ only. Solve it (analytically or iteratively) to find the Q-values of a policy.\n",
    "* Iteration form (policy evaluation):\n",
    "\n",
    "  $$\n",
    "  Q_{k+1}(s,a) \\leftarrow \\sum_{s'} P(s'\\mid s,a)\\Big[R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'\\mid s') Q_k(s',a')\\Big].\n",
    "  $$\n",
    "\n",
    "  * Keep updating $Q$ until it converges.\n",
    "\n",
    "### 6) **Relationship $V^\\pi$ ‚Üî $Q^\\pi$**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  V^\\pi(s) = \\sum_a \\pi(a\\mid s)\\,Q^\\pi(s,a)\n",
    "  $$\n",
    "* Meaning: state-value = **expected Q-value when actions are sampled from $\\pi$**.\n",
    "* Quick numeric example:\n",
    "\n",
    "  * Suppose two actions with probabilities $0.6$ and $0.4$.\n",
    "  * $Q(s,a_1)=10,\\; Q(s,a_2)=2$.\n",
    "  * Then $V(s)=0.6\\times 10 + 0.4\\times 2 = 6.8$.\n",
    "\n",
    "### 7) **Bellman *optimality* equation (why the $\\max$ appears)**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  Q^*(s,a) = \\sum_{s'} P(s'\\mid s,a)\\Big[ R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')\\Big]\n",
    "  $$\n",
    "* Explanation:\n",
    "\n",
    "  * $Q^*$ assumes **after taking action $a$ now we will act optimally thereafter**.\n",
    "  * So the future value from $s'$ is the **maximum** Q-value over all actions available in $s'$: $V^*(s')=\\max_{a'}Q^*(s',a')$.\n",
    "* Small numeric illustration:\n",
    "\n",
    "  * Suppose deterministic next state $s'$, $R=1$, $\\gamma=0.9$, and $\\max_{a'}Q^*(s',a')=9$.\n",
    "  * Then $Q^*(s,a)=1 + 0.9\\times 9 = 9.1$.\n",
    "\n",
    "* The optimal policy uses:\n",
    "\n",
    "  $$\n",
    "  \\pi^*(s) = \\arg\\max_a Q^*(s,a).\n",
    "  $$\n",
    "\n",
    "  * Pick the action that yields the largest $Q^*$ in that state.\n",
    "\n",
    "### 8) **Sample-based updates (how algorithms use these formulas)**\n",
    "\n",
    "* **Q-Learning (off-policy)** one-step sample update (common in practice):\n",
    "\n",
    "  $$\n",
    "  Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[ r + \\gamma\\max_{a'}Q(s',a') - Q(s,a)\\big]\n",
    "  $$\n",
    "\n",
    "  * Use when you sample a transition $(s,a,r,s')$.\n",
    "  * Intuition: move $Q(s,a)$ toward the sample target $r + \\gamma\\max_{a'}Q(s',a')$.\n",
    "* **SARSA (on-policy)**:\n",
    "\n",
    "  $$\n",
    "  Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[ r + \\gamma Q(s',a') - Q(s,a)\\big]\n",
    "  $$\n",
    "\n",
    "  * Here $a'$ is the actual next action chosen by current policy; used when learning while following that policy.\n",
    "\n",
    "### 9) **Terminal-state conventions**\n",
    "\n",
    "* Common options:\n",
    "\n",
    "  * Set $Q(\\text{terminal},a)=0$ for all $a$.\n",
    "  * Or skip/ignore those states in updates.\n",
    "* Both are fine if applied consistently.\n",
    "\n",
    "### 10) **Quick intuitive checklist** üß≠\n",
    "\n",
    "* If you *know* the next state exactly ‚Üí use $Q = r + \\gamma V(\\text{next})$.\n",
    "* If next state is random ‚Üí average over next states with $P(s'|s,a)$.\n",
    "* If you have a policy $\\pi$ and want to compute its values ‚Üí use the Bellman expectation (in terms of $Q$ or $V$).\n",
    "* If you want the best possible behavior ‚Üí replace expected future actions by $\\max$ (Bellman optimality), then act greedily w\\.r.t. $Q^*$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c479569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values:\n",
      "{(0, 0): np.float64(0.0), (0, 1): np.float64(0.0), (0, 2): np.float64(0.0), (0, 3): np.float64(0.0), (1, 0): np.float64(0.0), (1, 1): np.float64(0.0), (1, 2): np.float64(0.0), (1, 3): np.float64(0.0), (2, 0): np.float64(0.0), (2, 1): np.float64(0.0), (2, 2): np.float64(0.0), (2, 3): np.float64(0.0), (3, 0): np.float64(0.0), (3, 1): np.float64(0.0), (3, 2): np.float64(0.0), (3, 3): np.float64(0.0), (4, 0): np.float64(0.0), (4, 1): np.float64(0.0), (4, 2): np.float64(0.0), (4, 3): np.float64(0.0), (5, 0): np.float64(0.0), (5, 1): np.float64(0.0), (5, 2): np.float64(0.0), (5, 3): np.float64(0.0), (6, 0): np.float64(0.0), (6, 1): np.float64(0.0), (6, 2): np.float64(0.0), (6, 3): np.float64(0.0), (7, 0): np.float64(0.0), (7, 1): np.float64(0.0), (7, 2): np.float64(0.0), (7, 3): np.float64(0.0), (8, 0): np.float64(0.0), (8, 1): np.float64(0.0), (8, 2): np.float64(0.0), (8, 3): np.float64(0.0), (9, 0): np.float64(0.0), (9, 1): np.float64(0.0), (9, 2): np.float64(0.0), (9, 3): np.float64(0.0), (10, 0): np.float64(0.0), (10, 1): np.float64(0.0), (10, 2): np.float64(0.0), (10, 3): np.float64(0.0), (11, 0): np.float64(0.0), (11, 1): np.float64(0.0), (11, 2): np.float64(0.0), (11, 3): np.float64(0.0), (12, 0): np.float64(0.0), (12, 1): np.float64(0.0), (12, 2): np.float64(0.0), (12, 3): np.float64(0.0), (13, 0): np.float64(0.0), (13, 1): np.float64(0.0), (13, 2): np.float64(0.0), (13, 3): np.float64(0.0), (14, 0): np.float64(0.0), (14, 1): np.float64(0.0), (14, 2): np.float64(0.0), (14, 3): np.float64(1.9), (15, 0): 0, (15, 1): 0, (15, 2): 0, (15, 3): 0}\n",
      "\n",
      "Improved Policy:\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 3}\n",
      "State: 0, Reward: 0.0\n",
      "State: 0, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 4, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 8, Reward: 0.0\n",
      "State: 12, Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Import Required Libraries\n",
    "# ============================================\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. Initialize Environment & Parameters\n",
    "# ============================================\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "num_states = env.observation_space.n     # Number of states in FrozenLake\n",
    "num_actions = env.action_space.n         # Number of possible actions\n",
    "terminal_state = 15                      # Goal state index in FrozenLake\n",
    "gamma = 0.9                              # Discount factor\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. Initialize Value Function\n",
    "# ============================================\n",
    "# Value function stores the \"goodness\" of each state\n",
    "V = np.zeros(num_states)     # Start with all states = 0\n",
    "V[terminal_state] = 1.0      # Goal state is assigned value 1\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Q-Value Computation Function\n",
    "# ============================================\n",
    "def compute_q_value(state, action, V):\n",
    "    \"\"\"\n",
    "    Compute the Q-value for a given (state, action) pair.\n",
    "\n",
    "    Parameters:\n",
    "        state  (int): Current state\n",
    "        action (int): Action taken in this state\n",
    "        V      (array): Current value function\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated Q-value\n",
    "    \"\"\"\n",
    "    # Terminal state has no future rewards\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    # env.unwrapped.P[state][action] gives the transition dynamics:\n",
    "    # [(probability, next_state, reward, done), ...]\n",
    "    probability, next_state, reward, done = env.unwrapped.P[state][action][0]\n",
    "    \n",
    "    # Bellman expectation equation\n",
    "    return reward + gamma * V[next_state]\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. Compute Q-values for All State-Action Pairs\n",
    "# ============================================\n",
    "Q = {\n",
    "    (state, action): compute_q_value(state, action, V)\n",
    "    for state in range(num_states)\n",
    "    for action in range(num_actions)\n",
    "}\n",
    "\n",
    "print(\"Q-values:\")\n",
    "print(Q)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. Greedy Policy Improvement\n",
    "# ============================================\n",
    "def improve_policy(Q, num_states, num_actions):\n",
    "    \"\"\"\n",
    "    Improve policy using greedy selection:\n",
    "    For each state, pick the action with the maximum Q-value.\n",
    "    \"\"\"\n",
    "    improved_policy = {}\n",
    "    \n",
    "    for state in range(num_states - 1):  # Exclude terminal state\n",
    "        max_action = max(range(num_actions), key=lambda action: Q[(state, action)])\n",
    "        improved_policy[state] = max_action\n",
    "    \n",
    "    return improved_policy\n",
    "\n",
    "\n",
    "# Generate improved policy\n",
    "policy = improve_policy(Q, num_states, num_actions)\n",
    "\n",
    "print(\"\\nImproved Policy:\")\n",
    "print(policy)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. Test the Improved Policy\n",
    "# ============================================\n",
    "state, _ = env.reset()   # Reset environment to initial state\n",
    "terminated = False\n",
    "\n",
    "while not terminated:\n",
    "    action = policy[state]  # Select action according to policy\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"State: {state}, Reward: {reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1e5c4",
   "metadata": {},
   "source": [
    "### üîé Why are most Q-values `0.0`?\n",
    "\n",
    "* we **initialized the value function `V` with all zeros**, except for the terminal state (`V[15] = 1.0`).\n",
    "\n",
    "* Then we computed Q-values as:\n",
    "\n",
    "  $$\n",
    "  Q(s,a) = r + \\gamma V(s')\n",
    "  $$\n",
    "\n",
    "* Since **most transitions in FrozenLake lead to states with `V=0`**, their Q-values become `0.0`.\n",
    "* The **only exception is state 14 ‚Üí action 3 (right)**, which leads directly to the goal (`state 15`). That‚Äôs why `Q[(14,3)] = 1.9` (`reward=1 + Œ≥*1.0`).\n",
    "\n",
    "\n",
    "### üîé Why does the policy look like this?\n",
    "\n",
    "```\n",
    "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 3}\n",
    "```\n",
    "\n",
    "* Greedy policy improvement picks the **action with the highest Q-value**.\n",
    "* Since all Q-values are `0.0` except for state `14 ‚Üí action 3`, the policy defaults to choosing **action 0** (usually \"Left\") everywhere.\n",
    "* At state `14`, it correctly chooses action `3` (Right ‚Üí goal).\n",
    "\n",
    "So the policy is trivial: \"always go left, except when at state 14, go right.\"\n",
    "\n",
    "\n",
    "### üîé Why does the agent loop between states (0 ‚Üí 4 ‚Üí 8 ‚Üí 4 ‚Üí 8 ‚Ä¶)?\n",
    "\n",
    "* FrozenLake is **slippery** (`is_slippery=True`), meaning the agent doesn‚Äôt always move in the intended direction.\n",
    "* With a bad policy (always choosing \"left\"), the agent just bounces around between states (like 0 ‚Üí 4 ‚Üí 8 ‚Üí 12).\n",
    "* It **never learns a good path** because we only computed **one-step Q-values** from an untrained `V`.\n",
    "\n",
    "\n",
    "> ### ‚≠êTo actually learn a meaningful policy, we need **policy iteration** or **value iteration** (looping updates to `V` and `Q` until convergence).‚≠ê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147fd7a",
   "metadata": {},
   "source": [
    "## 2.3 Quick Comparison: $V^\\pi$ Vs $Q^\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff7db4",
   "metadata": {},
   "source": [
    "| Function         | Input                 | Output         | Question it answers                                         | Intuition                                                                                              | Example                                                                                         |\n",
    "| ---------------- | --------------------- | -------------- | ----------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------- |\n",
    "| **$V^\\pi(s)$**   | State $s$             | Number (value) | ‚ÄúHow good is this state under policy $\\pi$?‚Äù                | Expected long-term return starting **from state $s$** and following policy $\\pi$.                      | In chess: value of a board position assuming you continue playing according to strategy $\\pi$.  |\n",
    "| **$Q^\\pi(s,a)$** | State $s$, Action $a$ | Number (value) | ‚ÄúHow good is this action in this state under policy $\\pi$?‚Äù | Expected long-term return starting **from state $s$**, taking action $a$, then following policy $\\pi$. | In chess: value of moving a knight to a specific square (action) given the current board state. |\n",
    "\n",
    "\n",
    "\n",
    "### üîë Key Differences\n",
    "\n",
    "1. **Scope**\n",
    "\n",
    "   * $V^\\pi(s)$ looks at **how desirable a state is overall**.\n",
    "   * $Q^\\pi(s,a)$ looks at **how desirable a specific action is in that state**.\n",
    "\n",
    "2. **Decision-making granularity**\n",
    "\n",
    "   * $V^\\pi(s)$ is coarse-grained (state-level).\n",
    "   * $Q^\\pi(s,a)$ is fine-grained (state-action level, more detailed).\n",
    "\n",
    "3. **Relation**\n",
    "\n",
    "   * $V^\\pi(s) = \\sum_a \\pi(a|s)\\, Q^\\pi(s,a)$\n",
    "     (state value is the expected action value under the policy).\n",
    "\n",
    "4. **Use cases**\n",
    "\n",
    "   * $V^\\pi(s)$: Used in **policy evaluation** (how good is my strategy overall?).\n",
    "   * $Q^\\pi(s,a)$: Used in **policy improvement** and **control** (which action should I pick?).\n",
    "\n",
    "\n",
    "\n",
    "### ‚≠ê Intuitive Summary\n",
    "\n",
    "* **$V$** = *‚ÄúValue of being in a place (state).‚Äù*\n",
    "* **$Q$** = *‚ÄúQuality of making a move (action) in that place.‚Äù*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9013b754",
   "metadata": {},
   "source": [
    "# **‚úÖ3. Policy & Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e5ff3",
   "metadata": {},
   "source": [
    "> ### **Initialize policy** $\\longrightarrow$ **Evaluate policy** $\\longleftrightarrow$ **Improve policy** $\\longrightarrow$ **Optimal policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2bc52",
   "metadata": {},
   "source": [
    "## 3.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86877bfb",
   "metadata": {},
   "source": [
    "* **Goal**: Find the **optimal policy** (œÄ\\*) that maximizes expected return in a Markov Decision Process (MDP).\n",
    "* Two key algorithms:\n",
    "\n",
    "  * **Policy Iteration (PI)** ‚Üí Iterative evaluation + improvement of a policy.\n",
    "  * **Value Iteration (VI)** ‚Üí A faster version that combines evaluation & improvement in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc936a5",
   "metadata": {},
   "source": [
    "## 3.2 Policy Iteration ($PI$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20129335",
   "metadata": {},
   "source": [
    "### üîπ Steps\n",
    "\n",
    "1. **Initialize**\n",
    "\n",
    "   * Start with a random policy œÄ.\n",
    "\n",
    "2. **Policy Evaluation**\n",
    "\n",
    "   * Compute the **state-value function V(s)** under œÄ:\n",
    "\n",
    "     $$\n",
    "     V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} P(s'|s,a)[r + \\gamma V^{\\pi}(s')]\n",
    "     $$\n",
    "   * Iterate until values converge.\n",
    "\n",
    "3. **Policy Improvement**\n",
    "\n",
    "   * For each state, pick the action that maximizes the Q-value:\n",
    "\n",
    "     $$\n",
    "     \\pi'(s) = \\arg\\max_a Q(s,a)\n",
    "     $$\n",
    "   * Update policy.\n",
    "\n",
    "4. **Repeat** evaluation ‚Üí improvement until **policy stabilizes** (no further change).\n",
    "\n",
    "\n",
    "\n",
    "### üîπ **Implementation Outline**\n",
    "\n",
    "* **Functions**:\n",
    "\n",
    "  * `policy_evaluation(policy)` ‚Üí Computes V(s) using `compute_state_value()`.\n",
    "  * `policy_improvement(policy)` ‚Üí Builds new policy by computing Q(s,a) and picking `max_action`.\n",
    "  * `policy_iteration()` ‚Üí Alternates evaluation & improvement until convergence.\n",
    "* **Termination condition**:\n",
    "  Policy does not change between iterations ‚áí œÄ\\* found.\n",
    "\n",
    "\n",
    "\n",
    "### üîπ **Example: Grid World**\n",
    "\n",
    "* Apply PI to a grid environment.\n",
    "* Result: Agent finds **shortest path** to the goal (fewer steps than initial random policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3dac54",
   "metadata": {},
   "source": [
    "‚≠ê**Algorithm Overview**\n",
    "\n",
    "> **Definition:** Policy Iteration is an algorithm that finds the optimal policy by alternating between policy evaluation and policy improvement until convergence.\n",
    "\n",
    "‚≠ê**Steps:**\n",
    "1. **Initialize:** Start with an arbitrary policy $\\pi_o$\n",
    "2. **Policy Evaluation:** Compute $V^\\pi$ for current policy\n",
    "3. **Policy Improvement:** Create new policy œÄ' by acting greedily with respect to $V^\\pi$\n",
    "4. **Check Convergence:** If $\\pi^{'} = \\pi$ , stop. Otherwise, set $\\pi = \\pi^{'}$ and go to **Step-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24b303e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimal Policy (per state):\n",
      "[[0 3 0 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n",
      "\n",
      "‚úÖ Optimal Value Function:\n",
      "[[0.06888673 0.06141154 0.07440786 0.05580526]\n",
      " [0.09185135 0.         0.11220737 0.        ]\n",
      " [0.14543417 0.24749575 0.29961685 0.        ]\n",
      " [0.         0.37993513 0.63901979 0.        ]]\n",
      "\n",
      "‚úÖ Policy with arrows:\n",
      "[['‚Üê' '‚Üë' '‚Üê' '‚Üë']\n",
      " ['‚Üê' '‚Üê' '‚Üê' '‚Üê']\n",
      " ['‚Üë' '‚Üì' '‚Üê' '‚Üê']\n",
      " ['‚Üê' '‚Üí' '‚Üì' '‚Üê']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# -------------------------------\n",
    "# Create GridWorld Environment\n",
    "# -------------------------------\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=None)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Access transition probabilities\n",
    "P = env.unwrapped.P\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "theta = 1e-6   # convergence threshold\n",
    "\n",
    "# -------------------------------\n",
    "# Compute State Value\n",
    "# -------------------------------\n",
    "def compute_state_value(state, policy, V):\n",
    "    \"\"\"Computes the value of a state under the given policy.\"\"\"\n",
    "    action = policy[state]\n",
    "    value = 0.0\n",
    "    for prob, next_state, reward, terminated in P[state][action]:\n",
    "        value += prob * (reward + gamma * V[next_state])\n",
    "    return value\n",
    "\n",
    "# -------------------------------\n",
    "# Policy Evaluation\n",
    "# -------------------------------\n",
    "def policy_evaluation(policy):\n",
    "    V = np.zeros(num_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(num_states):\n",
    "            v = V[state]\n",
    "            V[state] = compute_state_value(state, policy, V)\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# -------------------------------\n",
    "# Compute Q-value\n",
    "# -------------------------------\n",
    "def compute_q_value(state, action, V):\n",
    "    \"\"\"Q(s,a) = sum over next states [ P(s'|s,a) * (R + gamma*V(s')) ]\"\"\"\n",
    "    q = 0.0\n",
    "    for prob, next_state, reward, terminated in P[state][action]:\n",
    "        q += prob * (reward + gamma * V[next_state])\n",
    "    return q\n",
    "\n",
    "# -------------------------------\n",
    "# Policy Improvement\n",
    "# -------------------------------\n",
    "def policy_improvement(V, policy):\n",
    "    stable = True\n",
    "    for state in range(num_states):\n",
    "        old_action = policy[state]\n",
    "        # Choose best action\n",
    "        action_values = [compute_q_value(state, a, V) for a in range(num_actions)]\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[state] = best_action\n",
    "        if old_action != best_action:\n",
    "            stable = False\n",
    "    return policy, stable\n",
    "\n",
    "# -------------------------------\n",
    "# Policy Iteration\n",
    "# -------------------------------\n",
    "def policy_iteration():\n",
    "    # Initialize random policy\n",
    "    policy = np.random.choice(num_actions, size=num_states)\n",
    "    while True:\n",
    "        V = policy_evaluation(policy)\n",
    "        policy, stable = policy_improvement(V, policy)\n",
    "        if stable:\n",
    "            break\n",
    "    return policy, V\n",
    "\n",
    "# -------------------------------\n",
    "# Run Policy Iteration\n",
    "# -------------------------------\n",
    "optimal_policy, optimal_V = policy_iteration()\n",
    "\n",
    "# Pretty print\n",
    "print(\"‚úÖ Optimal Policy (per state):\")\n",
    "print(optimal_policy.reshape((4, 4)))  # 4x4 Grid\n",
    "print(\"\\n‚úÖ Optimal Value Function:\")\n",
    "print(optimal_V.reshape((4, 4)))\n",
    "\n",
    "# Action mapping for better visualization\n",
    "action_map = {0: '‚Üê', 1: '‚Üì', 2: '‚Üí', 3: '‚Üë'}\n",
    "print(\"\\n‚úÖ Policy with arrows:\")\n",
    "policy_arrows = np.array([action_map[a] for a in optimal_policy]).reshape((4, 4))\n",
    "print(policy_arrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224738b",
   "metadata": {},
   "source": [
    "**Time Complexity:** $O(|S|¬≤|A|)$ per iteration, where |S| is number of states and |A| is number of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d84d6",
   "metadata": {},
   "source": [
    "## 3.3 Value Iteration ($VI$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3935f357",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üîπ **Idea**\n",
    "\n",
    "* Speeds up policy iteration by **combining evaluation & improvement** in a single update.\n",
    "* Instead of fully evaluating a policy each time, we directly update value estimates.\n",
    "\n",
    "### üîπ **Steps**\n",
    "\n",
    "1. **Initialize**\n",
    "\n",
    "   * Set $V(s) = 0$ for all states.\n",
    "\n",
    "2. **Iterative Update**\n",
    "\n",
    "   * For each state:\n",
    "\n",
    "     $$\n",
    "     V(s) \\leftarrow \\max_a \\sum_{s',r} P(s'|s,a)[r + \\gamma V(s')]\n",
    "     $$\n",
    "   * Policy derived as:\n",
    "\n",
    "     $$\n",
    "     \\pi(s) = \\arg\\max_a Q(s,a)\n",
    "     $$\n",
    "\n",
    "3. **Convergence**\n",
    "\n",
    "   * Continue until value updates are below a **threshold (Œµ)**.\n",
    "\n",
    "\n",
    "\n",
    "### üîπ **Implementation Outline**\n",
    "\n",
    "* **Functions**:\n",
    "\n",
    "  * `get_max_action_and_value(state, V)` ‚Üí Returns (max\\_action, max\\_q\\_value).\n",
    "  * `compute_action_value(state, action, V)` ‚Üí Computes Q(s,a) using updated V.\n",
    "* **Loop**:\n",
    "\n",
    "  * Update new state-values (`new_V`).\n",
    "  * Derive improved policy simultaneously.\n",
    "  * Check if `||new_V - V|| < Œµ`, then stop.\n",
    "\n",
    "\n",
    "\n",
    "### üîπ **Key Notes**\n",
    "\n",
    "* Uses **previous iteration V** directly (instead of `compute_state_value`).\n",
    "* Same final result as Policy Iteration.\n",
    "* **Advantage**: Often converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce9e89",
   "metadata": {},
   "source": [
    "### ‚≠êAlgorithm Overview\n",
    "\n",
    "> **Definition:** Value Iteration combines policy evaluation and improvement in a single step. It directly computes the optimal value function and derives the policy from it.\n",
    "\n",
    "**Key Insight:** Instead of fully evaluating a policy, perform only one sweep of policy evaluation followed by policy improvement.\n",
    "\n",
    "**Bellman Optimality Equation:**\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + Œ≥V^*(s')]$$\n",
    "\n",
    "### ‚≠êAlgorithm Steps\n",
    "\n",
    "1. **Initialize:** $V(s)$ = 0 for all states\n",
    "2. **Value Update:** For each state, compute the maximum expected value over all actions\n",
    "3. **Policy Extraction:** Choose actions that achieve the maximum value\n",
    "4. **Convergence Check:** Stop when value changes are below threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9532ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: {0: 0, 1: 3, 2: 0, 3: 3, 4: 0, 5: 0, 6: 0, 7: 0, 8: 3, 9: 1, 10: 0, 11: 0, 12: 0, 13: 2, 14: 1, 15: 0}\n",
      "Optimal Values: {0: 0.06162274283994246, 1: 0.05531399137342944, 2: 0.0699622244159502, 3: 0.05101702913301784, 4: 0.08519461431783229, 5: 0.0, 6: 0.10976851693787404, 7: 0.0, 8: 0.13996615432409334, 9: 0.24373109529624215, 10: 0.2969629949287245, 11: 0.0, 12: 0.0, 13: 0.3771539838433998, 14: 0.6375395830635082, 15: 0}\n",
      "State 5: Best action = 0, Value = 0.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode=None)\n",
    "mdp_env = env.unwrapped  # access the underlying MDP to get .P\n",
    "num_states = mdp_env.observation_space.n\n",
    "num_actions = mdp_env.action_space.n\n",
    "terminal_state = 15  # Goal state in FrozenLake 4x4\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "def value_iteration(threshold=0.001):\n",
    "    \"\"\"Value iteration algorithm.\"\"\"\n",
    "    # Initialize\n",
    "    V = {state: 0 for state in range(num_states)}\n",
    "    policy = {state: 0 for state in range(num_states)}\n",
    "\n",
    "    while True:\n",
    "        new_V = {state: 0 for state in range(num_states)}\n",
    "\n",
    "        for state in range(num_states):\n",
    "            if state == terminal_state:\n",
    "                new_V[state] = 0\n",
    "                continue\n",
    "\n",
    "            # Compute Q-values for all actions\n",
    "            Q_values = []\n",
    "            for action in range(num_actions):\n",
    "                q_val = 0\n",
    "                for prob, next_state, reward, done in mdp_env.P[state][action]:\n",
    "                    q_val += prob * (reward + gamma * V[next_state])\n",
    "                Q_values.append(q_val)\n",
    "\n",
    "            # Take maximum\n",
    "            max_q_value = max(Q_values)\n",
    "            max_action = int(np.argmax(Q_values))\n",
    "\n",
    "            new_V[state] = max_q_value\n",
    "            policy[state] = max_action\n",
    "\n",
    "        # Check convergence\n",
    "        if all(abs(new_V[s] - V[s]) < threshold for s in range(num_states)):\n",
    "            break\n",
    "\n",
    "        V = new_V\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "def get_max_action_and_value(state, V):\n",
    "    \"\"\"Helper function to get optimal action and value for a state.\"\"\"\n",
    "    Q_values = []\n",
    "    for action in range(num_actions):\n",
    "        q_val = 0\n",
    "        for prob, next_state, reward, done in mdp_env.P[state][action]:\n",
    "            q_val += prob * (reward + gamma * V[next_state])\n",
    "        Q_values.append(q_val)\n",
    "\n",
    "    max_action = int(np.argmax(Q_values))\n",
    "    max_q_value = Q_values[max_action]\n",
    "\n",
    "    return max_action, max_q_value\n",
    "\n",
    "# Test the functions\n",
    "if __name__ == \"__main__\":\n",
    "    optimal_policy, optimal_V = value_iteration()\n",
    "    print(\"Optimal Policy:\", optimal_policy)\n",
    "    print(\"Optimal Values:\", optimal_V)\n",
    "\n",
    "    # Test helper function\n",
    "    test_state = 5\n",
    "    action, value = get_max_action_and_value(test_state, optimal_V)\n",
    "    print(f\"State {test_state}: Best action = {action}, Value = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed731ca1",
   "metadata": {},
   "source": [
    "**Time Complexity:** $O(|S|¬≤|A|)$ per iteration, typically converges faster than Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0c597",
   "metadata": {},
   "source": [
    "## 3.4. Quick comparison: $PI$ Vs $VI$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e30c1",
   "metadata": {},
   "source": [
    "| Aspect                 | **Policy Iteration**                                                                                                                                                                                              | **Value Iteration**                                                                                                                                                                               |\n",
    "| ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Approach**           | Two clear steps: (1) **Policy Evaluation** ‚Äì compute how good the current policy is (using $V^\\pi$), (2) **Policy Improvement** ‚Äì update the policy to be greedy w\\.r.t. those values. These two steps alternate. | Blends evaluation and improvement into **one step** by directly updating the value function toward optimality ($V^*$) using the Bellman optimality equation.                                      |\n",
    "| **Convergence**        | Converges in a **finite number of iterations** (guaranteed to find the optimal policy after some steps).                                                                                                          | Converges **asymptotically**: values get closer to $V^*$ with each update but only *truly* converge after infinite updates. In practice, we stop once changes are very small (below a threshold). |\n",
    "| **Per Iteration Cost** | **High** ‚Äì because policy evaluation usually requires solving or approximating a system of equations (can take many sweeps over all states).                                                                      | **Low** ‚Äì each iteration just does one value update per state (using the Bellman backup). Much cheaper per iteration.                                                                             |\n",
    "| **Total Iterations**   | **Fewer** ‚Äì since each iteration makes a ‚Äúbig jump‚Äù by fully evaluating the policy.                                                                                                                               | **More** ‚Äì since each step is small and incremental. Needs many sweeps to reach near-optimal values.                                                                                              |\n",
    "| **Memory**             | Must store an **explicit policy** (mapping from states to actions) alongside the value function.                                                                                                                  | Policy is **implicitly derived** from the value function (choose the action that maximizes the next-state value).                                                                                 |\n",
    "| **Practical Use**      | Works best for **small or medium-sized state spaces** where exact policy evaluation is feasible.                                                                                                                  | Preferred for **large or complex state spaces**, since it avoids heavy evaluation and still reaches a good approximation.                                                                         |\n",
    "\n",
    "\n",
    "\n",
    "### **Intuition**\n",
    "\n",
    "- **Policy Iteration = ‚ÄúThink hard, act big‚Äù** ‚Üí Each step is expensive but fewer steps are needed.\n",
    "- **Value Iteration = ‚ÄúThink fast, act small‚Äù** ‚Üí Each step is cheap but you need more of them.\n",
    "\n",
    "- **Both PI and VI converge to the same optimal policy.**\n",
    "- Choice depends on efficiency needs:\n",
    "  - PI ‚Üí Conceptually simpler, good for learning.\n",
    "  - VI ‚Üí Faster, practical for large environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df943b8",
   "metadata": {},
   "source": [
    "# **‚úÖ4. Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d02d7e",
   "metadata": {},
   "source": [
    "## 4.1 Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715abc4",
   "metadata": {},
   "source": [
    "- **State Value:** $V^œÄ(s) = \\mathbb{E}[G_t | S_t = s, œÄ]$\n",
    "- **Action Value:** $Q^œÄ(s,a) = \\mathbb{E}[G_t | S_t = s, A_t = a, œÄ]$\n",
    "- **Return:** $G_t = \\sum_{k=0}^{\\infty} Œ≥^k R_{t+k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a6f41",
   "metadata": {},
   "source": [
    "## 4.2 Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01132d3b",
   "metadata": {},
   "source": [
    "- **State Value Bellman:** $V^œÄ(s) = \\sum_a œÄ(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + Œ≥V^œÄ(s')]$\n",
    "- **Action Value Bellman:** $Q^œÄ(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + Œ≥\\sum_{a'} œÄ(a'|s')Q^œÄ(s',a')]$\n",
    "- **Optimality Bellman:** $V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + Œ≥V^*(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7772b0",
   "metadata": {},
   "source": [
    "## 4.3 Practical Tips and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5513d6",
   "metadata": {},
   "source": [
    "### Implementation Considerations\n",
    "- **Convergence Threshold:** Choose appropriate threshold (e.g., 0.001) based on precision needs\n",
    "- **Discount Factor:** Œ≥ close to 1 emphasizes future rewards; closer to 0 emphasizes immediate rewards\n",
    "- **Terminal States:** Always handle terminal states separately (return 0 or fixed value)\n",
    "\n",
    "### Common Pitfalls\n",
    "- **Infinite Loops:** Ensure proper convergence checks in iterative algorithms\n",
    "- **State Indexing:** Be careful with state numbering and terminal state handling\n",
    "- **Action Spaces:** Verify action space size matches expected number of actions\n",
    "\n",
    "### Extensions and Advanced Topics\n",
    "- **Stochastic Policies:** Extend to probabilistic action selection\n",
    "- **Function Approximation:** Use neural networks for large state spaces\n",
    "- **Model-Free Methods:** Q-Learning and SARSA for unknown environments\n",
    "- **Policy Gradient Methods:** Direct policy optimization without value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb044bc",
   "metadata": {},
   "source": [
    "## 4.4 Code Examples Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471978fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class GridWorldMDP:\n",
    "    def __init__(self, env_name='FrozenLake-v1'):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.num_states = self.env.observation_space.n\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.gamma = 1.0\n",
    "        self.terminal_state = self.num_states - 1\n",
    "    \n",
    "    def policy_iteration(self, threshold=0.001):\n",
    "        \"\"\"Complete policy iteration implementation.\"\"\"\n",
    "        policy = {i: 0 for i in range(self.num_states - 1)}\n",
    "        \n",
    "        while True:\n",
    "            V = self.policy_evaluation(policy, threshold)\n",
    "            new_policy = self.policy_improvement(V)\n",
    "            \n",
    "            if new_policy == policy:\n",
    "                break\n",
    "            policy = new_policy\n",
    "        \n",
    "        return policy, V\n",
    "    \n",
    "    def value_iteration(self, threshold=0.001):\n",
    "        \"\"\"Complete value iteration implementation.\"\"\"\n",
    "        V = {s: 0 for s in range(self.num_states)}\n",
    "        \n",
    "        while True:\n",
    "            new_V = V.copy()\n",
    "            for state in range(self.num_states - 1):\n",
    "                if state != self.terminal_state:\n",
    "                    values = []\n",
    "                    for action in range(self.num_actions):\n",
    "                        transitions = self.env.unwrapped.P[state][action]\n",
    "                        value = sum(prob * (reward + self.gamma * V[next_state])\n",
    "                                  for prob, next_state, reward, _ in transitions)\n",
    "                        values.append(value)\n",
    "                    new_V[state] = max(values)\n",
    "            \n",
    "            if all(abs(new_V[s] - V[s]) < threshold for s in V):\n",
    "                break\n",
    "            V = new_V\n",
    "        \n",
    "        # Extract policy\n",
    "        policy = {}\n",
    "        for state in range(self.num_states - 1):\n",
    "            if state != self.terminal_state:\n",
    "                values = []\n",
    "                for action in range(self.num_actions):\n",
    "                    transitions = self.env.unwrapped.P[state][action]\n",
    "                    value = sum(prob * (reward + self.gamma * V[next_state])\n",
    "                              for prob, next_state, reward, _ in transitions)\n",
    "                    values.append(value)\n",
    "                policy[state] = np.argmax(values)\n",
    "        \n",
    "        return policy, V\n",
    "\n",
    "# Usage\n",
    "mdp = GridWorldMDP()\n",
    "optimal_policy, optimal_values = mdp.value_iteration()\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"Optimal Values:\", optimal_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
