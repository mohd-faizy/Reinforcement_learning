{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c04416",
   "metadata": {},
   "source": [
    "# **â­Model Based Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b4d75",
   "metadata": {},
   "source": [
    "## **âœ…1. Markov Decision Processes (MDPs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155590b4",
   "metadata": {},
   "source": [
    "### 1.1 What is an MDP?\n",
    "\n",
    "**Definition:** A Markov Decision Process (MDP) is a mathematical framework used to model decision-making in environments where outcomes are partly random and partly under the control of a decision-maker (agent). It's foundational in reinforcement learning and dynamic programming.\n",
    "\n",
    "**Purpose:** MDPs provide a formal way to describe environments where outcomes are partly random and partly under the control of a decision maker (agent).\n",
    "\n",
    "**Key Components of an MDP:**\n",
    "- **States (S):** All possible situations the agent can be in\n",
    "- **Actions (A):** All possible moves the agent can make\n",
    "- **Transition Probabilities (P):** Likelihood of moving from one state to another given an action\n",
    "- **Rewards (R):** Immediate feedback received after taking an action\n",
    "- **Discount Factor (Î³):** Weight given to future rewards (0 â‰¤ Î³ â‰¤ 1)\n",
    "\n",
    "### 1.2 The Markov Property\n",
    "\n",
    "**Definition:** The Markov Property states that the future state depends only on the `current state` and `action`, **NOT** on the entire history of past states and actions.\n",
    "\n",
    "- **Mathematical Expression:**\n",
    "$$P(S_{t+1} = s' | S_t = s, A_t = a, S_{t-1}, A_{t-1}, ..., S_0, A_0) = P(S_{t+1} = s' | S_t = s, A_t = a)$$\n",
    "\n",
    "- **Intuitive Explanation:** The current state contains all the information needed to make optimal decisions about the future.\n",
    "\n",
    "- **Goal of an Agent in MDP**\n",
    "  - The agentâ€™s objective is to find a policy `ðœ‹(ð‘Žâˆ£ð‘ )` that maximizes the expected cumulative reward over time. This is often formalized using:\n",
    "      - **Value functions**: Estimate how good it is to be in a state or take an action.\n",
    "      - **Policy iteration** and **value iteration**: Algorithms to compute optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcb48e",
   "metadata": {},
   "source": [
    "### **Frozen Lake**\n",
    "\n",
    "**Environment Description:** An agent must navigate across a frozen lake to reach a goal while avoiding holes.\n",
    "\n",
    "**Components:**\n",
    "- **States:** 16 positions (4Ã—4 grid) numbered 0-15\n",
    "- **Actions:** 4 possible moves (0: left, 1: down, 2: right, 3: up)\n",
    "- **Terminal States:** Goal state (rewards +1) and hole states (episode ends) ~ 6\n",
    "- **Transition Probabilities:** Actions don't always lead to expected outcomes due to slippery ice\n",
    "\n",
    "![frozen-lake-png](_img\\frozen-lake.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bea402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n",
      "Number of actions: 4\n",
      "Number of states: 16\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "# Check state and action spaces\n",
    "print(env.action_space)          \n",
    "print(env.observation_space)    \n",
    "\n",
    "print(\"Number of actions:\", env.action_space.n)      \n",
    "print(\"Number of states:\", env.observation_space.n)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96144e0",
   "metadata": {},
   "source": [
    "### 1.4 Transition Probabilities and Rewards\n",
    "\n",
    "**Accessing Transition Information:**\n",
    "```python\n",
    "# env.unwrapped.P[state][action] returns:\n",
    "# [(probability_1, next_state_1, reward_1, is_terminal_1),\n",
    "#  (probability_2, next_state_2, reward_2, is_terminal_2), ...]\n",
    "\n",
    "state = 6\n",
    "action = 0  # left\n",
    "print(env.unwrapped.P[state][action])\n",
    "# Output: [(0.333, 2, 0.0, False), (0.333, 5, 0.0, True), (0.333, 10, 0.0, False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aaa227",
   "metadata": {},
   "source": [
    "### **CliffWalking**\n",
    "\n",
    "- The Cliff Walking environment involves an agent crossing a grid world from start to goal while avoiding falling off a cliff.\n",
    "- If the player moves to a cliff location it returns to the start location.\n",
    "- The player makes moves until they reach the goal, which ends the episode.\n",
    "- Your task is to explore the state and action spaces of this environment.\n",
    "\n",
    "![cliff-walking-gif](_img\\cliff_walking.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a46672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "Number of states: 48\n",
      "[(1.0, np.int64(23), -1, False)]\n",
      "Action: 0 | Probability: 1.0, Next State: 23, Reward: -1, Done: False\n",
      "[(1.0, np.int64(35), -1, False)]\n",
      "Action: 1 | Probability: 1.0, Next State: 35, Reward: -1, Done: False\n",
      "[(1.0, np.int64(47), -1, True)]\n",
      "Action: 2 | Probability: 1.0, Next State: 47, Reward: -1, Done: True\n",
      "[(1.0, np.int64(34), -1, False)]\n",
      "Action: 3 | Probability: 1.0, Next State: 34, Reward: -1, Done: False\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym   \n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Environment Setup\n",
    "# ==============================\n",
    "# Create the CliffWalking environment.\n",
    "# \"CliffWalking-v1\" is a classic control problem from reinforcement learning.\n",
    "# \"render_mode='rgb_array'\" means the environment won't open a window;\n",
    "# instead, it keeps the visual output as an image array (useful for debugging or rendering later).\n",
    "env = gym.make('CliffWalking-v1', render_mode='rgb_array')\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Action and State Spaces\n",
    "# ==============================\n",
    "# Number of possible actions the agent can take (Up, Right, Down, Left = 4).\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Number of possible states in the gridworld (4 rows Ã— 12 columns = 48).\n",
    "num_states = env.observation_space.n\n",
    "\n",
    "print(\"Number of actions:\", num_actions)\n",
    "print(\"Number of states:\", num_states)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Exploring Transitions\n",
    "# ==============================\n",
    "# Each state has a set of transitions, depending on the chosen action.\n",
    "# Let's pick a specific state (for example, 35) and explore what happens when we try different actions.\n",
    "state = 35\n",
    "\n",
    "# Loop through all possible actions from this state\n",
    "for action in range(num_actions):\n",
    "    # The environment has an internal dictionary \"P\" that stores transitions.\n",
    "    # P[state][action] gives a list of possible outcomes when taking `action` in `state`.\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    print(transitions)\n",
    "\n",
    "    # Each transition has the format: (probability, next_state, reward, done)\n",
    "    # -> probability: chance of this outcome (usually 1.0 for deterministic envs like CliffWalking)\n",
    "    # -> next_state: the state you land in after the action\n",
    "    # -> reward: the reward received for this action\n",
    "    # -> done: whether the episode ends after this transition\n",
    "    for transition in transitions:\n",
    "        probability, next_state, reward, done = transition\n",
    "        print(f\"Action: {action} | Probability: {probability}, Next State: {next_state}, Reward: {reward}, Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c01e753",
   "metadata": {},
   "source": [
    "## **âœ…2. Policies and State-Value Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bbb28e",
   "metadata": {},
   "source": [
    "### 2.1 Policies\n",
    "\n",
    "**Definition:** A policy (Ï€) is a strategy that defines which action to take in each state to maximize the expected cumulative reward (return).\n",
    "\n",
    "**Types:**\n",
    "- **Deterministic Policy:** Always chooses the same action for a given state\n",
    "- **Stochastic Policy:** Chooses actions according to a probability distribution\n",
    "\n",
    "**Example Policy (Grid World):**\n",
    "```python\n",
    "# 0: left, 1: down, 2: right, 3: up\n",
    "policy = {\n",
    "    0: 1,  # In state 0, go down\n",
    "    1: 2,  # In state 1, go right\n",
    "    2: 1,  # In state 2, go down\n",
    "    3: 1,  # In state 3, go down\n",
    "    4: 3,  # In state 4, go up\n",
    "    5: 1,  # In state 5, go down\n",
    "    6: 2,  # In state 6, go right\n",
    "    7: 3   # In state 7, go up\n",
    "}\n",
    "```\n",
    "\n",
    "**Policy Execution:**\n",
    "```python\n",
    "state, info = env.reset()\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = policy[state]\n",
    "    state, reward, terminated, _, _ = env.step(action)\n",
    "```\n",
    "\n",
    "### 2.2 State-Value Functions\n",
    "\n",
    "**Definition:** The state-value function $V^\\pi(s)$ represents the expected return (cumulative discounted reward) when starting from state s and following policy $\\pi$.\n",
    "\n",
    "**Mathematical Expression:**\n",
    "$$V^Ï€(s) = \\mathbb{E}[G_t | S_t = s, Ï€]$$\n",
    "\n",
    "Where $G_t = R_{t+1} + Î³R_{t+2} + Î³^2R_{t+3} + ... = \\sum_{k=0}^{\\infty} Î³^k R_{t+k+1}$\n",
    "\n",
    "**Bellman Equation for State Values:**\n",
    "$$V^Ï€(s) = \\sum_a Ï€(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + Î³V^Ï€(s')]$$\n",
    "\n",
    "**Example Calculation:**\n",
    "For a 9-state grid world with discount factor Î³ = 1:\n",
    "- Goal state: V(8) = 0 (terminal state)\n",
    "- State 5 (adjacent to goal): V(5) = -1 + 1 Ã— 10 = 10\n",
    "- State 2: V(2) = -1 + 1 Ã— 10 = 9\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "def compute_state_value(state, policy):\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state, policy)\n",
    "\n",
    "# Compute all state values\n",
    "gamma = 1\n",
    "terminal_state = 8\n",
    "V = {state: compute_state_value(state, policy) \n",
    "     for state in range(num_states)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CUSTOM 3x3 GRIDWORLD WITH POLICY AND STATE-VALUES ===\")\n",
    "print()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Custom 3x3 GridWorld Env\n",
    "# -------------------------------\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"ansi\"]}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shape = (3, 3)\n",
    "        self.observation_space = spaces.Discrete(9)\n",
    "        self.action_space = spaces.Discrete(4)  # 0:left,1:down,2:right,3:up\n",
    "        \n",
    "        # Define rewards\n",
    "        self.terminal_state = 8\n",
    "        self.rewards = {8: 10, 4: -2, 7: -2}\n",
    "        \n",
    "        # Precompute P like Gym\n",
    "        self.P = {s: {a: [] for a in range(4)} for s in range(9)}\n",
    "        for s in range(9):\n",
    "            for a in range(4):\n",
    "                ns = self._move(s, a)\n",
    "                r = self.rewards.get(ns, -1)\n",
    "                done = ns == self.terminal_state\n",
    "                self.P[s][a] = [(1.0, ns, r, done)]  # deterministic\n",
    "                \n",
    "    def _move(self, state, action):\n",
    "        if state == self.terminal_state:\n",
    "            return state\n",
    "        row, col = state // 3, state % 3\n",
    "        if action == 0:    # left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 1:  # down\n",
    "            row = min(2, row + 1)\n",
    "        elif action == 2:  # right\n",
    "            col = min(2, col + 1)\n",
    "        elif action == 3:  # up\n",
    "            row = max(0, row - 1)\n",
    "        return row * 3 + col\n",
    "    \n",
    "    def render(self, mode=\"ansi\"):\n",
    "        grid = np.full(self.shape, \" \")\n",
    "        grid[0,0] = \"S\"  # start\n",
    "        grid[2,2] = \"D\"  # diamond\n",
    "        grid[1,1] = grid[1,2] = \"M\"  # mountains\n",
    "        return \"\\n\".join([\" \".join(row) for row in grid])\n",
    "\n",
    "# Create environment\n",
    "env = GridWorldEnv()\n",
    "num_states = env.observation_space.n\n",
    "gamma = 1\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define a deterministic policy\n",
    "# -------------------------------\n",
    "policy = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3,  # up\n",
    "    8: 0   # terminal\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Compute state values\n",
    "# -------------------------------\n",
    "def compute_state_value(state):\n",
    "    if state == env.terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "\n",
    "V = {s: compute_state_value(s) for s in range(num_states)}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Display results\n",
    "# -------------------------------\n",
    "print(\"Custom 3x3 GridWorld Layout:\")\n",
    "print(env.render())\n",
    "print()\n",
    "print(\"State-values:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c209f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy 1: {0: 'down', 1: 'right', 2: 'down', 3: 'down', 4: 'up', 5: 'down', 6: 'right', 7: 'up'}\n",
      "Policy 2: {0: 'right', 1: 'right', 2: 'down', 3: 'right', 4: 'right', 5: 'down', 6: 'right', 7: 'right'}\n",
      "\n",
      "2. STATE-VALUE FUNCTIONS\n",
      "========================\n",
      "V(s) = Expected return starting from state s following policy Ï€\n",
      "\n",
      "Computing state values...\n",
      "\n",
      "RESULTS:\n",
      "========\n",
      "State-values for Policy 1: {0: 1.0, 1: 8.0, 2: 9.0, 3: 2.0, 4: 7.0, 5: 10.0, 6: 3.0, 7: 5.0, 8: 0}\n",
      "State-values for Policy 2: {0: 7.0, 1: 8.0, 2: 9.0, 3: 7.0, 4: 9.0, 5: 10.0, 6: 8.0, 7: 10.0, 8: 0}\n",
      "\n",
      "EXAMPLE CALCULATION (Policy 1, State 2):\n",
      "========================================\n",
      "State 2 â†’ Action down â†’ State 5\n",
      "Reward: -1\n",
      "V(2) = -1 + 1.0 Ã— V(5) = -1 + 1.0 Ã— 10.0 = 9.0\n",
      "\n",
      "POLICY COMPARISON:\n",
      "==================\n",
      "Total value (Policy 1): 45.0\n",
      "Total value (Policy 2): 68.0\n",
      "Better policy: Policy 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# 1. POLICIES\n",
    "# ==========================\n",
    "\n",
    "# Actions: 0: left, 1: down, 2: right, 3: up\n",
    "policy1 = {\n",
    "    0: 1,  # down\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 1,  # down\n",
    "    4: 3,  # up\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 3   # up\n",
    "}\n",
    "\n",
    "policy2 = {\n",
    "    0: 2,  # right\n",
    "    1: 2,  # right\n",
    "    2: 1,  # down\n",
    "    3: 2,  # right\n",
    "    4: 2,  # right\n",
    "    5: 1,  # down\n",
    "    6: 2,  # right\n",
    "    7: 2   # right\n",
    "}\n",
    "\n",
    "action_names = {0: 'left', 1: 'down', 2: 'right', 3: 'up'}\n",
    "\n",
    "print(\"Policy 1:\", {s: action_names[a] for s, a in policy1.items()})\n",
    "print(\"Policy 2:\", {s: action_names[a] for s, a in policy2.items()})\n",
    "print()\n",
    "\n",
    "# ==========================\n",
    "# 2. ENVIRONMENT MODEL (P)\n",
    "# ==========================\n",
    "gamma = 1.0          # Discount factor\n",
    "num_states = 9       # 3x3 grid\n",
    "terminal_state = 8   # Diamond\n",
    "\n",
    "# Build transition table: P[state][action] = [(prob, next_state, reward, done)]\n",
    "P = {s: {a: [] for a in range(4)} for s in range(num_states)}\n",
    "\n",
    "def move(state, action):\n",
    "    \"\"\"Return next state after taking an action.\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return state\n",
    "    \n",
    "    row, col = state // 3, state % 3\n",
    "    if action == 0:    # left\n",
    "        col = max(0, col - 1)\n",
    "    elif action == 1:  # down\n",
    "        row = min(2, row + 1)\n",
    "    elif action == 2:  # right\n",
    "        col = min(2, col + 1)\n",
    "    elif action == 3:  # up\n",
    "        row = max(0, row - 1)\n",
    "    return row * 3 + col\n",
    "\n",
    "def reward(next_state):\n",
    "    \"\"\"Return reward for landing in next_state.\"\"\"\n",
    "    if next_state == 8:   # Diamond\n",
    "        return 10\n",
    "    elif next_state in [4, 7]:  # Mountains\n",
    "        return -2\n",
    "    else:  # All other states\n",
    "        return -1\n",
    "\n",
    "# Fill transition table\n",
    "for s in range(num_states):\n",
    "    for a in range(4):\n",
    "        ns = move(s, a)\n",
    "        r = reward(ns)\n",
    "        done = (ns == terminal_state)\n",
    "        P[s][a] = [(1.0, ns, r, done)]   # deterministic env\n",
    "\n",
    "# ==========================\n",
    "# 3. STATE-VALUE FUNCTIONS\n",
    "# ==========================\n",
    "print(\"2. STATE-VALUE FUNCTIONS\")\n",
    "print(\"========================\")\n",
    "print(\"V(s) = Expected return starting from state s following policy Ï€\")\n",
    "print()\n",
    "\n",
    "def compute_state_value(state, policy):\n",
    "    \"\"\"Bellman expectation with env.P\"\"\"\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    \n",
    "    action = policy[state]\n",
    "    transitions = P[state][action]\n",
    "    \n",
    "    value = 0\n",
    "    for prob, next_state, reward, _ in transitions:\n",
    "        value += prob * (reward + gamma * compute_state_value(next_state, policy))\n",
    "    return value\n",
    "\n",
    "# Calculate state values for both policies\n",
    "print(\"Computing state values...\")\n",
    "print()\n",
    "\n",
    "V1 = {s: compute_state_value(s, policy1) for s in range(num_states)}\n",
    "V2 = {s: compute_state_value(s, policy2) for s in range(num_states)}\n",
    "\n",
    "# ==========================\n",
    "# 4. RESULTS\n",
    "# ==========================\n",
    "print(\"RESULTS:\")\n",
    "print(\"========\")\n",
    "print(\"State-values for Policy 1:\", V1)\n",
    "print(\"State-values for Policy 2:\", V2)\n",
    "print()\n",
    "\n",
    "# Example calculation walkthrough\n",
    "print(\"EXAMPLE CALCULATION (Policy 1, State 2):\")\n",
    "print(\"========================================\")\n",
    "state = 2\n",
    "action = policy1[state]\n",
    "prob, next_state, reward, _ = P[state][action][0]\n",
    "print(f\"State 2 â†’ Action {action_names[action]} â†’ State {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"V(2) = {reward} + {gamma} Ã— V({next_state}) = {reward} + {gamma} Ã— {V1[next_state]} = {V1[2]}\")\n",
    "print()\n",
    "\n",
    "# Compare policies\n",
    "print(\"POLICY COMPARISON:\")\n",
    "print(\"==================\")\n",
    "total1 = sum(V1[s] for s in range(8))  # exclude terminal\n",
    "total2 = sum(V2[s] for s in range(8))\n",
    "print(f\"Total value (Policy 1): {total1}\")\n",
    "print(f\"Total value (Policy 2): {total2}\")\n",
    "print(f\"Better policy: Policy {'2' if total2 > total1 else '1'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa7797",
   "metadata": {},
   "source": [
    "## **âœ…3. Action-Value Functions (Q-Values)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7d2fe",
   "metadata": {},
   "source": [
    "### 3.1 Q-Value Definition\n",
    "\n",
    "**Definition:** The action-value function Q^Ï€(s,a) represents the expected return when starting from state s, taking action a, and then following policy Ï€.\n",
    "\n",
    "**Mathematical Expression:**\n",
    "$$Q^Ï€(s,a) = \\mathbb{E}[G_t | S_t = s, A_t = a, Ï€]$$\n",
    "\n",
    "**Bellman Equation for Action Values:**\n",
    "$$Q^Ï€(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + Î³V^Ï€(s')]$$\n",
    "\n",
    "**Relationship with State Values:**\n",
    "$$Q^Ï€(s,a) = R(s,a) + Î³ \\sum_{s'} P(s'|s,a)V^Ï€(s')$$\n",
    "\n",
    "### 3.2 Computing Q-Values\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "1. For each state-action pair (s,a)\n",
    "2. Calculate immediate reward\n",
    "3. Add discounted value of next state\n",
    "4. Sum over all possible next states weighted by transition probabilities\n",
    "\n",
    "**Example Calculation (State 4, Grid World):**\n",
    "- Q(4, down) = -2 + 1 Ã— 5 = 3\n",
    "- Q(4, left) = -1 + 1 Ã— 2 = 1  \n",
    "- Q(4, up) = -1 + 1 Ã— 8 = 7\n",
    "- Q(4, right) = -1 + 1 Ã— 10 = 9\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "def compute_q_value(state, action, V):\n",
    "    if state == terminal_state:\n",
    "        return None\n",
    "    \n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * V[next_state]\n",
    "\n",
    "# Compute all Q-values\n",
    "Q = {(state, action): compute_q_value(state, action, V)\n",
    "     for state in range(num_states)\n",
    "     for action in range(num_actions)}\n",
    "```\n",
    "\n",
    "### 3.3 Policy Improvement Using Q-Values\n",
    "\n",
    "**Greedy Policy Improvement:** Select the action with the highest Q-value for each state.\n",
    "\n",
    "```python\n",
    "def improve_policy(Q, num_states, num_actions):\n",
    "    improved_policy = {}\n",
    "    \n",
    "    for state in range(num_states - 1):  # Exclude terminal state\n",
    "        # Find action with maximum Q-value\n",
    "        max_action = max(range(num_actions), \n",
    "                        key=lambda action: Q[(state, action)])\n",
    "        improved_policy[state] = max_action\n",
    "    \n",
    "    return improved_policy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20129335",
   "metadata": {},
   "source": [
    "## **âœ…4. Policy Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3dac54",
   "metadata": {},
   "source": [
    "### 4.1 Algorithm Overview\n",
    "\n",
    "**Definition:** Policy Iteration is an algorithm that finds the optimal policy by alternating between policy evaluation and policy improvement until convergence.\n",
    "\n",
    "**Steps:**\n",
    "1. **Initialize:** Start with an arbitrary policy Ï€â‚€\n",
    "2. **Policy Evaluation:** Compute V^Ï€ for current policy\n",
    "3. **Policy Improvement:** Create new policy Ï€' by acting greedily with respect to V^Ï€\n",
    "4. **Check Convergence:** If Ï€' = Ï€, stop. Otherwise, set Ï€ = Ï€' and go to step 2\n",
    "\n",
    "### 4.2 Implementation\n",
    "\n",
    "```python\n",
    "def policy_evaluation(policy, threshold=0.001):\n",
    "    \"\"\"Evaluate a given policy until convergence.\"\"\"\n",
    "    V = {state: 0 for state in range(num_states)}\n",
    "    \n",
    "    while True:\n",
    "        new_V = {state: 0 for state in range(num_states)}\n",
    "        \n",
    "        for state in range(num_states - 1):\n",
    "            if state != terminal_state:\n",
    "                action = policy[state]\n",
    "                _, next_state, reward, _ = env.P[state][action][0]\n",
    "                new_V[state] = reward + gamma * V[next_state]\n",
    "        \n",
    "        # Check convergence\n",
    "        if all(abs(new_V[s] - V[s]) < threshold for s in V):\n",
    "            break\n",
    "        V = new_V\n",
    "    \n",
    "    return V\n",
    "\n",
    "def policy_improvement(V):\n",
    "    \"\"\"Improve policy based on current value function.\"\"\"\n",
    "    improved_policy = {}\n",
    "    \n",
    "    for state in range(num_states - 1):\n",
    "        Q_values = []\n",
    "        for action in range(num_actions):\n",
    "            _, next_state, reward, _ = env.P[state][action][0]\n",
    "            q_val = reward + gamma * V[next_state]\n",
    "            Q_values.append(q_val)\n",
    "        \n",
    "        # Select best action\n",
    "        max_action = max(range(num_actions), key=lambda a: Q_values[a])\n",
    "        improved_policy[state] = max_action\n",
    "    \n",
    "    return improved_policy\n",
    "\n",
    "def policy_iteration():\n",
    "    \"\"\"Complete policy iteration algorithm.\"\"\"\n",
    "    # Initialize with arbitrary policy\n",
    "    policy = {0: 1, 1: 2, 2: 1, 3: 1, 4: 3, 5: 1, 6: 2, 7: 3}\n",
    "    \n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        V = policy_evaluation(policy)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        improved_policy = policy_improvement(V)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if improved_policy == policy:\n",
    "            break\n",
    "            \n",
    "        policy = improved_policy\n",
    "    \n",
    "    return policy, V\n",
    "```\n",
    "\n",
    "**Time Complexity:** O(|S|Â²|A|) per iteration, where |S| is number of states and |A| is number of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126acaab",
   "metadata": {},
   "source": [
    "## **âœ…5. Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a0f74",
   "metadata": {},
   "source": [
    "### 5.1 Algorithm Overview\n",
    "\n",
    "**Definition:** Value Iteration combines policy evaluation and improvement in a single step. It directly computes the optimal value function and derives the policy from it.\n",
    "\n",
    "**Key Insight:** Instead of fully evaluating a policy, perform only one sweep of policy evaluation followed by policy improvement.\n",
    "\n",
    "**Bellman Optimality Equation:**\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + Î³V^*(s')]$$\n",
    "\n",
    "### 5.2 Algorithm Steps\n",
    "\n",
    "1. **Initialize:** V(s) = 0 for all states\n",
    "2. **Value Update:** For each state, compute the maximum expected value over all actions\n",
    "3. **Policy Extraction:** Choose actions that achieve the maximum value\n",
    "4. **Convergence Check:** Stop when value changes are below threshold\n",
    "\n",
    "### 5.3 Implementation\n",
    "\n",
    "```python\n",
    "def value_iteration(threshold=0.001):\n",
    "    \"\"\"Value iteration algorithm.\"\"\"\n",
    "    # Initialize\n",
    "    V = {state: 0 for state in range(num_states)}\n",
    "    policy = {state: 0 for state in range(num_states - 1)}\n",
    "    \n",
    "    while True:\n",
    "        new_V = {state: 0 for state in range(num_states)}\n",
    "        \n",
    "        for state in range(num_states - 1):\n",
    "            if state != terminal_state:\n",
    "                # Compute Q-values for all actions\n",
    "                Q_values = []\n",
    "                for action in range(num_actions):\n",
    "                    _, next_state, reward, _ = env.P[state][action][0]\n",
    "                    q_val = reward + gamma * V[next_state]\n",
    "                    Q_values.append(q_val)\n",
    "                \n",
    "                # Take maximum\n",
    "                max_q_value = max(Q_values)\n",
    "                max_action = max(range(num_actions), key=lambda a: Q_values[a])\n",
    "                \n",
    "                new_V[state] = max_q_value\n",
    "                policy[state] = max_action\n",
    "        \n",
    "        # Check convergence\n",
    "        if all(abs(new_V[state] - V[state]) < threshold for state in V):\n",
    "            break\n",
    "            \n",
    "        V = new_V\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "def get_max_action_and_value(state, V):\n",
    "    \"\"\"Helper function to get optimal action and value for a state.\"\"\"\n",
    "    Q_values = []\n",
    "    for action in range(num_actions):\n",
    "        _, next_state, reward, _ = env.P[state][action][0]\n",
    "        q_val = reward + gamma * V[next_state]\n",
    "        Q_values.append(q_val)\n",
    "    \n",
    "    max_action = max(range(num_actions), key=lambda a: Q_values[a])\n",
    "    max_q_value = Q_values[max_action]\n",
    "    \n",
    "    return max_action, max_q_value\n",
    "```\n",
    "\n",
    "**Time Complexity:** O(|S|Â²|A|) per iteration, typically converges faster than Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0c597",
   "metadata": {},
   "source": [
    "## **âœ…6. Comparison: Policy Iteration vs Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e30c1",
   "metadata": {},
   "source": [
    "| Aspect | Policy Iteration | Value Iteration |\n",
    "|--------|------------------|-----------------|\n",
    "| **Approach** | Separate evaluation and improvement | Combined evaluation and improvement |\n",
    "| **Convergence** | Finite number of iterations | Asymptotic convergence |\n",
    "| **Per Iteration Cost** | Higher (full policy evaluation) | Lower (single value update) |\n",
    "| **Total Iterations** | Fewer iterations needed | More iterations needed |\n",
    "| **Memory** | Stores explicit policy | Policy derived from values |\n",
    "| **Practical Use** | Better for small state spaces | Better for large state spaces |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df943b8",
   "metadata": {},
   "source": [
    "## **âœ…7. Key Formulas Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d02d7e",
   "metadata": {},
   "source": [
    "### 7.1 Value Functions\n",
    "- **State Value:** $V^Ï€(s) = \\mathbb{E}[G_t | S_t = s, Ï€]$\n",
    "- **Action Value:** $Q^Ï€(s,a) = \\mathbb{E}[G_t | S_t = s, A_t = a, Ï€]$\n",
    "- **Return:** $G_t = \\sum_{k=0}^{\\infty} Î³^k R_{t+k+1}$\n",
    "\n",
    "### 7.2 Bellman Equations\n",
    "- **State Value Bellman:** $V^Ï€(s) = \\sum_a Ï€(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + Î³V^Ï€(s')]$\n",
    "- **Action Value Bellman:** $Q^Ï€(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + Î³\\sum_{a'} Ï€(a'|s')Q^Ï€(s',a')]$\n",
    "- **Optimality Bellman:** $V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + Î³V^*(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7772b0",
   "metadata": {},
   "source": [
    "## **âœ…8. Practical Tips and Best Practices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5513d6",
   "metadata": {},
   "source": [
    "### 8.1 Implementation Considerations\n",
    "- **Convergence Threshold:** Choose appropriate threshold (e.g., 0.001) based on precision needs\n",
    "- **Discount Factor:** Î³ close to 1 emphasizes future rewards; closer to 0 emphasizes immediate rewards\n",
    "- **Terminal States:** Always handle terminal states separately (return 0 or fixed value)\n",
    "\n",
    "### 8.2 Common Pitfalls\n",
    "- **Infinite Loops:** Ensure proper convergence checks in iterative algorithms\n",
    "- **State Indexing:** Be careful with state numbering and terminal state handling\n",
    "- **Action Spaces:** Verify action space size matches expected number of actions\n",
    "\n",
    "### 8.3 Extensions and Advanced Topics\n",
    "- **Stochastic Policies:** Extend to probabilistic action selection\n",
    "- **Function Approximation:** Use neural networks for large state spaces\n",
    "- **Model-Free Methods:** Q-Learning and SARSA for unknown environments\n",
    "- **Policy Gradient Methods:** Direct policy optimization without value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb044bc",
   "metadata": {},
   "source": [
    "## **âœ…9. Code Examples Repository**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bab618",
   "metadata": {},
   "source": [
    "### 9.1 Complete Grid World Example\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class GridWorldMDP:\n",
    "    def __init__(self, env_name='FrozenLake-v1'):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.num_states = self.env.observation_space.n\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.gamma = 1.0\n",
    "        self.terminal_state = self.num_states - 1\n",
    "    \n",
    "    def policy_iteration(self, threshold=0.001):\n",
    "        \"\"\"Complete policy iteration implementation.\"\"\"\n",
    "        policy = {i: 0 for i in range(self.num_states - 1)}\n",
    "        \n",
    "        while True:\n",
    "            V = self.policy_evaluation(policy, threshold)\n",
    "            new_policy = self.policy_improvement(V)\n",
    "            \n",
    "            if new_policy == policy:\n",
    "                break\n",
    "            policy = new_policy\n",
    "        \n",
    "        return policy, V\n",
    "    \n",
    "    def value_iteration(self, threshold=0.001):\n",
    "        \"\"\"Complete value iteration implementation.\"\"\"\n",
    "        V = {s: 0 for s in range(self.num_states)}\n",
    "        \n",
    "        while True:\n",
    "            new_V = V.copy()\n",
    "            for state in range(self.num_states - 1):\n",
    "                if state != self.terminal_state:\n",
    "                    values = []\n",
    "                    for action in range(self.num_actions):\n",
    "                        transitions = self.env.unwrapped.P[state][action]\n",
    "                        value = sum(prob * (reward + self.gamma * V[next_state])\n",
    "                                  for prob, next_state, reward, _ in transitions)\n",
    "                        values.append(value)\n",
    "                    new_V[state] = max(values)\n",
    "            \n",
    "            if all(abs(new_V[s] - V[s]) < threshold for s in V):\n",
    "                break\n",
    "            V = new_V\n",
    "        \n",
    "        # Extract policy\n",
    "        policy = {}\n",
    "        for state in range(self.num_states - 1):\n",
    "            if state != self.terminal_state:\n",
    "                values = []\n",
    "                for action in range(self.num_actions):\n",
    "                    transitions = self.env.unwrapped.P[state][action]\n",
    "                    value = sum(prob * (reward + self.gamma * V[next_state])\n",
    "                              for prob, next_state, reward, _ in transitions)\n",
    "                    values.append(value)\n",
    "                policy[state] = np.argmax(values)\n",
    "        \n",
    "        return policy, V\n",
    "\n",
    "# Usage\n",
    "mdp = GridWorldMDP()\n",
    "optimal_policy, optimal_values = mdp.value_iteration()\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"Optimal Values:\", optimal_values)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
